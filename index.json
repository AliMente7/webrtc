{
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadyToSendDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadyToSendDelegate.html",
    "title": "Delegate PeerConnection.LocalSdpReadyToSendDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.LocalSdpReadyToSendDelegate Delegate for LocalSdpReadytoSend event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void LocalSdpReadyToSendDelegate(string type, string sdp); Parameters Type Name Description String type SDP message type, one of \"offer\", \"answer\", or \"ice\". String sdp Raw SDP message content."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceStateChangedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceStateChangedDelegate.html",
    "title": "Delegate PeerConnection.IceStateChangedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.IceStateChangedDelegate Delegate for the IceStateChanged event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void IceStateChangedDelegate(IceConnectionState newState); Parameters Type Name Description IceConnectionState newState The new ICE connection state."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode.html",
    "title": "Enum PeerConnection.FrameHeightRoundMode | MixedReality-WebRTC Documentation",
    "keywords": "Enum PeerConnection.FrameHeightRoundMode Frame height round mode. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum FrameHeightRoundMode : int Fields Name Description Crop Crop frame height to the nearest multiple of 16. ((height - nearestLowerMultipleOf16) / 2) rows are cropped from the top and (height - nearestLowerMultipleOf16 - croppedRowsTop) rows are cropped from the bottom. None Leave frames unchanged. Pad Pad frame height to the nearest multiple of 16. ((nearestHigherMultipleOf16 - height) / 2) rows are added symmetrically at the top and (nearestHigherMultipleOf16 - height - addedRowsTop) rows are added symmetrically at the bottom. See Also SetFrameHeightRoundMode(PeerConnection.FrameHeightRoundMode)"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelRemovedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelRemovedDelegate.html",
    "title": "Delegate PeerConnection.DataChannelRemovedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.DataChannelRemovedDelegate Delegate for DataChannelRemoved event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void DataChannelRemovedDelegate(DataChannel channel); Parameters Type Name Description DataChannel channel The data channel just removed."
  },
  "api/Microsoft.MixedReality.WebRTC.LocalVideoTrack.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.LocalVideoTrack.html",
    "title": "Class LocalVideoTrack | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalVideoTrack Video track sending to the remote peer video frames originating from a local track source. Inheritance Object LocalVideoTrack Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class LocalVideoTrack : IDisposable Properties | Improve this Doc View Source Enabled Enabled status of the track. If enabled, send local video frames to the remote peer as expected. If disabled, send only black frames instead. Declaration public bool Enabled { get; set; } Property Value Type Description Boolean Remarks Reading the value of this property after the track has been disposed is valid, and returns false . Writing to this property after the track has been disposed throws an exception. | Improve this Doc View Source Name Track name as specified during creation. This property is immutable. Declaration public string Name { get; } Property Value Type Description String | Improve this Doc View Source PeerConnection Peer connection this video track is added to, if any. This is null after the track has been removed from the peer connection. Declaration public PeerConnection PeerConnection { get; } Property Value Type Description PeerConnection | Improve this Doc View Source Source External source for this video track, or null if the source is some internal video capture device. Declaration public ExternalVideoTrackSource Source { get; } Property Value Type Description ExternalVideoTrackSource Methods | Improve this Doc View Source Dispose() Declaration public void Dispose() Events | Improve this Doc View Source Argb32VideoFrameReady Event that occurs when a video frame has been produced by the underlying source and is available. Declaration public event Argb32VideoFrameDelegate Argb32VideoFrameReady Event Type Type Description Argb32VideoFrameDelegate | Improve this Doc View Source I420AVideoFrameReady Event that occurs when a video frame has been produced by the underlying source and is available. Declaration public event I420AVideoFrameDelegate I420AVideoFrameReady Event Type Type Description I420AVideoFrameDelegate"
  },
  "api/Microsoft.MixedReality.WebRTC.IVideoFrameStorage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IVideoFrameStorage.html",
    "title": "Interface IVideoFrameStorage | MixedReality-WebRTC Documentation",
    "keywords": "Interface IVideoFrameStorage Interface for a storage of a single video frame. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface IVideoFrameStorage Properties | Improve this Doc View Source Buffer Raw storage buffer of capacity Capacity . Declaration byte[] Buffer { get; } Property Value Type Description Byte [] | Improve this Doc View Source Capacity Storage capacity, in bytes. Declaration ulong Capacity { get; set; } Property Value Type Description UInt64 | Improve this Doc View Source Height Frame height, in pixels. Declaration uint Height { get; set; } Property Value Type Description UInt32 | Improve this Doc View Source Width Frame width, in pixels. Declaration uint Width { get; set; } Property Value Type Description UInt32"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.html",
    "title": "Namespace Microsoft.MixedReality.WebRTC.Unity | MixedReality-WebRTC Documentation",
    "keywords": "Namespace Microsoft.MixedReality.WebRTC.Unity Classes AudioSource Base class for audio sources plugging into the internal peer connection API to expose a single audio stream to a renderer ( MediaPlayer or custom). AudioStreamStartedEvent Unity event corresponding to a new audio stream being started. AudioStreamStoppedEvent Unity event corresponding to an on-going audio stream being stopped. CaptureCameraAttribute Attribute for a property used by SceneVideoSource to capture the content of a framebuffer, and for which some constraints on stereoscopic rendering options need to be enforced (and errors can be reported in the Editor if they are not followed). CustomVideoSource<T> Abstract base component for a custom video source delivering raw video frames directly to the WebRTC implementation. LocalAudioSource This component represents a local audio source added as an audio track to an existing WebRTC peer connection and sent to the remote peer. The audio track can optionally be rendered locally with a MediaPlayer . LocalVideoSource This component represents a local video source added as a video track to an existing WebRTC peer connection and sent to the remote peer. The video track can optionally be displayed locally with a MediaPlayer . MediaPlayer Play video frames received from a WebRTC video track. NodeDssSignaler Simple signaler for debug and testing. This is based on https://github.com/bengreenier/node-dss and SHOULD NOT BE USED FOR PRODUCTION. PeerConnection High-level wrapper for Unity WebRTC functionalities. This is the API entry point for establishing a connection with a remote peer. RemoteAudioSource This component represents a remote audio source added as an audio track to an existing WebRTC peer connection by a remote peer and received locally. The audio track can optionally be displayed locally with a MediaPlayer . RemoteVideoSource This component represents a remote video source added as a video track to an existing WebRTC peer connection by a remote peer and received locally. The video track can optionally be displayed locally with a MediaPlayer . SceneVideoSource Custom video source capturing the Unity scene content as rendered by a given camera, and sending it as a video track through the selected peer connection. SdpTokenAttribute Attribute for string properties representing an SDP token, which has constraints on the allowed characters it can contain, as defined in the SDP RFC. See https://tools.ietf.org/html/rfc4566#page-43 for details. Signaler Base class for WebRTC signaling implementations in Unity. Signaler.Message VideoSource Base class for video sources plugging into the internal peer connection API to expose a single video stream to a renderer ( MediaPlayer or custom). VideoStreamStartedEvent Unity event corresponding to a new video stream being started. VideoStreamStoppedEvent Unity event corresponding to an on-going video stream being stopped. WebRTCErrorEvent A UnityEvent that represents a WebRTC error event. Structs ConfigurableIceServer Represents an Ice server in a simple way that allows configuration from the unity inspector VideoCaptureConstraints Additional optional constraints applied to the resolution and framerate when selecting a video capture format. Enums IceType Enumeration of the different types of ICE servers. LocalVideoSourceFormatMode Video capture format selection mode for a local video source. Signaler.Message.WireMessageType Possible message types as-serialized on the wire"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.WebRTCErrorEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.WebRTCErrorEvent.html",
    "title": "Class WebRTCErrorEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class WebRTCErrorEvent A UnityEvent that represents a WebRTC error event. Inheritance Object WebRTCErrorEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class WebRTCErrorEvent : UnityEvent<string>"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.html",
    "title": "Class Signaler | MixedReality-WebRTC Documentation",
    "keywords": "Class Signaler Base class for WebRTC signaling implementations in Unity. Inheritance Object Signaler NodeDssSignaler Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class Signaler : MonoBehaviour Fields _nativePeer Native PeerConnection object from the underlying WebRTC C# library, available once the peer has been initialized and the signaler is attached to it. Declaration protected PeerConnection _nativePeer Field Value Type Description PeerConnection Properties PeerConnection The PeerConnection this signaler is attached to, or null if not attached yet to any connection. This is updated automatically by the peer connection once it finished initializing. Declaration public PeerConnection PeerConnection { get; } Property Value Type Description PeerConnection Methods OnIceCandiateReadyToSend(String, Int32, String) Callback fired when an ICE candidate message has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected abstract void OnIceCandiateReadyToSend(string candidate, int sdpMlineIndex, string sdpMid) Parameters Type Name Description String candidate Int32 sdpMlineIndex String sdpMid OnPeerInitialized(PeerConnection) Callback fired from the PeerConnection when it finished initializing, to subscribe to signaling-related events. Declaration public void OnPeerInitialized(PeerConnection peer) Parameters Type Name Description PeerConnection peer The peer connection to attach to OnPeerUninitializing(PeerConnection) Callback fired from the PeerConnection before it starts uninitializing itself and disposing of the underlying implementation object. Declaration public void OnPeerUninitializing(PeerConnection peer) Parameters Type Name Description PeerConnection peer The peer connection about to be deinitialized OnSdpAnswerReadyToSend(String) Callback fired when a local SDP answer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected abstract void OnSdpAnswerReadyToSend(string answer) Parameters Type Name Description String answer The SDP answer message to send. OnSdpOfferReadyToSend(String) Callback fired when a local SDP offer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected abstract void OnSdpOfferReadyToSend(string offer) Parameters Type Name Description String offer The SDP offer message to send. SendMessageAsync(Signaler.Message) Asynchronously send a signaling message to the remote peer. Declaration public abstract Task SendMessageAsync(Signaler.Message message) Parameters Type Name Description Signaler.Message message The signaling message to send to the remote peer. Returns Type Description Task A Task object completed once the message has been sent, but not necessarily delivered. Update() Declaration protected virtual void Update() Events OnConnect Declaration public event Action OnConnect Event Type Type Description Action OnDisconnect Declaration public event Action OnDisconnect Event Type Type Description Action OnFailure Declaration public event Action<Exception> OnFailure Event Type Type Description Action < Exception > OnMessage Declaration public event Action<Signaler.Message> OnMessage Event Type Type Description Action < Signaler.Message >"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.Message.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.Message.html",
    "title": "Class Signaler.Message | MixedReality-WebRTC Documentation",
    "keywords": "Class Signaler.Message Inheritance Object Signaler.Message Inherited Members Object.ToString() Object.Equals(Object) Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetHashCode() Object.GetType() Object.MemberwiseClone() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class Message Fields Data The primary message contents Declaration public string Data Field Value Type Description String IceDataSeparator The data separator needed for proper ICE serialization Declaration public string IceDataSeparator Field Value Type Description String MessageType The message type Declaration public Signaler.Message.WireMessageType MessageType Field Value Type Description Signaler.Message.WireMessageType Methods WireMessageTypeFromString(String) Convert a message type from to Signaler.Message.WireMessageType . Declaration public static Signaler.Message.WireMessageType WireMessageTypeFromString(string stringType) Parameters Type Name Description String stringType The message type as . Returns Type Description Signaler.Message.WireMessageType The message type as a Signaler.Message.WireMessageType object."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.LocalVideoSourceFormatMode.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.LocalVideoSourceFormatMode.html",
    "title": "Enum LocalVideoSourceFormatMode | MixedReality-WebRTC Documentation",
    "keywords": "Enum LocalVideoSourceFormatMode Video capture format selection mode for a local video source. Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public enum LocalVideoSourceFormatMode Fields Name Description Automatic Automatically select a good resolution and framerate based on the runtime detection of the device the application is running on. This currently overwrites the default WebRTC selection only on HoloLens devices. Manual Manually specify a video profile unique ID and/or a kind of video profile to use, and additional optional constraints on the resolution and framerate of that profile."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.LocalVideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.LocalVideoSource.html",
    "title": "Class LocalVideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalVideoSource This component represents a local video source added as a video track to an existing WebRTC peer connection and sent to the remote peer. The video track can optionally be displayed locally with a MediaPlayer . Inheritance Object VideoSource LocalVideoSource Inherited Members VideoSource.FrameQueue VideoSource.VideoStreamStarted VideoSource.VideoStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class LocalVideoSource : VideoSource Fields AutoAddTrack Automatically register as a video track when the peer connection is ready. Declaration public bool AutoAddTrack Field Value Type Description Boolean Remarks If this is false then the user needs to manually call Microsoft.MixedReality.WebRTC.PeerConnection.AddLocalVideoTrackAsync(Microsoft.MixedReality.WebRTC.PeerConnection.VideoCaptureDevice,bool) to add a video track to the peer connection and start sending video data to the remote peer. AutoStartCapture Automatically start local video capture when this component is enabled. Declaration public bool AutoStartCapture Field Value Type Description Boolean Constraints For manual Mode , optional constraints on the resolution and framerate of the capture format. These constraints are additive, meaning a matching format must satisfy all of them at once, in addition of being restricted to the formats supported by the selected video profile or kind of profile. Declaration public VideoCaptureConstraints Constraints Field Value Type Description VideoCaptureConstraints EnableMixedRealityCapture Enable Mixed Reality Capture (MRC) if available on the local device. This option has no effect on devices not supporting MRC, and is silently ignored. Declaration public bool EnableMixedRealityCapture Field Value Type Description Boolean EnableMRCRecordingIndicator Enable the on-screen recording indicator when Mixed Reality Capture (MRC) is available and enabled. This option has no effect on devices not supporting MRC, or if MRC is not enabled. Declaration public bool EnableMRCRecordingIndicator Field Value Type Description Boolean Mode Selection mode for the video capture format. Declaration public LocalVideoSourceFormatMode Mode Field Value Type Description LocalVideoSourceFormatMode PeerConnection Peer connection this local video source will add a video track to. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection PreferredVideoCodec Name of the preferred video codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredVideoCodec Field Value Type Description String TrackName Name of the track. This will be sent in the SDP messages. Declaration public string TrackName Field Value Type Description String VideoProfileId For manual Mode , unique identifier of the video profile to use, or an empty string to leave unconstrained. Declaration public string VideoProfileId Field Value Type Description String VideoProfileKind For manual Mode , kind of video profile to use among a list of predefined ones, or an empty string to leave unconstrained. Declaration public PeerConnection.VideoProfileKind VideoProfileKind Field Value Type Description PeerConnection.VideoProfileKind Properties Track Video track added to the peer connection that this component encapsulates. Declaration public LocalVideoTrack Track { get; } Property Value Type Description LocalVideoTrack Methods Awake() Declaration protected void Awake() OnDestroy() Declaration protected void OnDestroy() OnDisable() Callback when the Unity component is disabled. This is the proper way to disable the video source and get it to stop video capture. Declaration protected void OnDisable() OnEnable() Callback when the Unity component is enabled. This is the proper way to enable the video source and get it to start video capture and enqueue video frames. Declaration protected void OnEnable()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.LocalVideoSourceEditor.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.LocalVideoSourceEditor.html",
    "title": "Class LocalVideoSourceEditor | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalVideoSourceEditor Inspector editor for LocalVideoSource . Allows displaying some error message when Mixed Reality Capture is enabled but XR is not, the later corresponding to a non-exclusive app (2D slate) where MRC is not available. Inheritance Object LocalVideoSourceEditor Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class LocalVideoSourceEditor : UnityEditor.Editor Methods OnInspectorGUI() Override implementation of Editor.OnInspectorGUI to draw the inspector GUI for the currently selected LocalVideoSource . Declaration public override void OnInspectorGUI()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.CaptureCameraDrawer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.CaptureCameraDrawer.html",
    "title": "Class CaptureCameraDrawer | MixedReality-WebRTC Documentation",
    "keywords": "Class CaptureCameraDrawer Property drawer for CaptureCameraAttribute , to report an error to the user if the associated property instance cannot be used for framebuffer capture by SceneVideoSource . Inheritance Object CaptureCameraDrawer Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class CaptureCameraDrawer : PropertyDrawer Methods GetPropertyHeight(SerializedProperty, GUIContent) Declaration public override float GetPropertyHeight(SerializedProperty property, GUIContent label) Parameters Type Name Description SerializedProperty property GUIContent label Returns Type Description Single OnGUI(Rect, SerializedProperty, GUIContent) Declaration public override void OnGUI(Rect position, SerializedProperty property, GUIContent label) Parameters Type Name Description Rect position SerializedProperty property GUIContent label Validate(Camera) Validate that a given instance can be used for framebuffer capture by SceneVideoSource based on the current settings of the Unity Player for the current build platform. Declaration public static void Validate(Camera camera) Parameters Type Name Description Camera camera The camera instance to test the settings of. See Also Validate(Camera)"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.TrackKind.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.TrackKind.html",
    "title": "Enum PeerConnection.TrackKind | MixedReality-WebRTC Documentation",
    "keywords": "Enum PeerConnection.TrackKind Kind of WebRTC track. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum TrackKind : uint Fields Name Description Audio Audio track. Data Data track. Unknown Unknown track kind. Generally not initialized or error. Video Video track."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.LocalVideoTrackSettings.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.LocalVideoTrackSettings.html",
    "title": "Class PeerConnection.LocalVideoTrackSettings | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnection.LocalVideoTrackSettings Settings for adding a local video track. Inheritance Object PeerConnection.LocalVideoTrackSettings Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class LocalVideoTrackSettings : object Fields | Improve this Doc View Source enableMrc Enable Mixed Reality Capture (MRC) on devices supporting the feature. This setting is silently ignored on device not supporting MRC. Declaration public bool enableMrc Field Value Type Description Boolean Remarks This is only supported on UWP. | Improve this Doc View Source enableMrcRecordingIndicator Display the on-screen recording indicator while MRC is enabled. This setting is silently ignored on device not supporting MRC, or if enableMrc is set to false . Declaration public bool enableMrcRecordingIndicator Field Value Type Description Boolean Remarks This is only supported on UWP. | Improve this Doc View Source framerate Optional capture frame rate, in frames per second (FPS). This must be a capture framerate the device supports. Declaration public double? framerate Field Value Type Description Nullable < Double > Remarks This is compared by strict equality, so is best left unspecified or to an exact value retrieved by GetVideoCaptureFormatsAsync(String) . | Improve this Doc View Source height Optional capture resolution height, in pixels. This must be a resolution width the device supports. Declaration public uint? height Field Value Type Description Nullable < UInt32 > | Improve this Doc View Source trackName Name of the track to create, as used for the SDP negotiation. This name needs to comply with the requirements of an SDP token, as described in the SDP RFC https://tools.ietf.org/html/rfc4566#page-43 . In particular the name cannot contain spaces nor double quotes \" . The track name can optionally be empty, in which case the implementation will create a valid random track name. Declaration public string trackName Field Value Type Description String | Improve this Doc View Source videoDevice Optional video capture device to use for capture. Use the default device if not specified. Declaration public VideoCaptureDevice videoDevice Field Value Type Description VideoCaptureDevice | Improve this Doc View Source videoProfileId Optional unique identifier of the video profile to use for capture, if the device supports video profiles, as retrieved by one of: This requires videoDevice to be specified. Declaration public string videoProfileId Field Value Type Description String | Improve this Doc View Source videoProfileKind Optional video profile kind to restrict the list of video profiles to consider. Note that this is not exclusive with videoProfileId , although in practice it is recommended to specify only one or the other. This requires videoDevice to be specified. Declaration public PeerConnection.VideoProfileKind videoProfileKind Field Value Type Description PeerConnection.VideoProfileKind | Improve this Doc View Source width Optional capture resolution width, in pixels. This must be a resolution width the device supports. Declaration public uint? width Field Value Type Description Nullable < UInt32 >"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceGatheringStateChangedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceGatheringStateChangedDelegate.html",
    "title": "Delegate PeerConnection.IceGatheringStateChangedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.IceGatheringStateChangedDelegate Delegate for the IceGatheringStateChanged event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void IceGatheringStateChangedDelegate(IceGatheringState newState); Parameters Type Name Description IceGatheringState newState The new ICE gathering state."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceCandidateReadytoSendDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.IceCandidateReadytoSendDelegate.html",
    "title": "Delegate PeerConnection.IceCandidateReadytoSendDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.IceCandidateReadytoSendDelegate Delegate for the IceCandidateReadytoSend event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void IceCandidateReadytoSendDelegate(string candidate, int sdpMlineindex, string sdpMid); Parameters Type Name Description String candidate Raw SDP message describing the ICE candidate. Int32 sdpMlineindex Index of the m= line. String sdpMid Media identifier"
  },
  "api/Microsoft.MixedReality.WebRTC.IceGatheringState.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IceGatheringState.html",
    "title": "Enum IceGatheringState | MixedReality-WebRTC Documentation",
    "keywords": "Enum IceGatheringState State of an ICE gathering process. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum IceGatheringState : int Remarks See RTPIceGatheringState from the WebRTC 1.0 standard. Fields Name Description Complete The gathering process is complete. At least one ICE transport was active, and all transports finished gathering ICE candidates. Gathering The gathering process started. At least one ICE transport is active and gathering some ICE candidates. New There is no ICE transport, or none of them started gathering ICE candidates. See Also https://www.w3.org/TR/webrtc/#rtcicegatheringstate-enum"
  },
  "api/Microsoft.MixedReality.WebRTC.IceConnectionState.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IceConnectionState.html",
    "title": "Enum IceConnectionState | MixedReality-WebRTC Documentation",
    "keywords": "Enum IceConnectionState State of an ICE connection. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum IceConnectionState : int Remarks Due to the underlying implementation, this is currently a mix of the RTPIceGatheringState and the RTPPeerConnectionState from the WebRTC 1.0 standard. Fields Name Description Checking ICE connection received an offer, but transports are not writable yet. Closed The peer connection was closed entirely. Completed ICE connection finished establishing. Connected Transports are writable. Disconnected ICE connection is disconnected, there is no more writable transport. Failed Failed establishing an ICE connection. New Newly created ICE connection. This is the starting state. See Also https://www.w3.org/TR/webrtc/#rtcicegatheringstate-enum https://www.w3.org/TR/webrtc/#rtcpeerconnectionstate-enum"
  },
  "api/Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.html",
    "title": "Class ExternalVideoTrackSource | MixedReality-WebRTC Documentation",
    "keywords": "Class ExternalVideoTrackSource Video source for WebRTC video tracks based on a custom source of video frames managed by the user and external to the WebRTC implementation. This class is used to inject into the WebRTC engine a video track whose frames are produced by a user-managed source the WebRTC engine knows nothing about, like programmatically generated frames, including frames not strictly of video origin like a 3D rendered scene, or frames coming from a specific capture device not supported natively by WebRTC. This class serves as an adapter for such video frame sources. Inheritance Object ExternalVideoTrackSource Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class ExternalVideoTrackSource : IDisposable Fields | Improve this Doc View Source _frameRequestCallbackArgsHandle GC handle to frame request callback args keeping the delegate alive while the callback is registered with the native implementation. Declaration protected IntPtr _frameRequestCallbackArgsHandle Field Value Type Description IntPtr Properties | Improve this Doc View Source PeerConnection Once the external video track source is attached to some video track(s), this returns the peer connection the video track(s) are part of. Otherwise this returns null . Declaration public PeerConnection PeerConnection { get; } Property Value Type Description PeerConnection Methods | Improve this Doc View Source CompleteFrameRequest(UInt32, Int64, Argb32VideoFrame) Complete the current request by providing a video frame for it. This must be used if the video track source was created with CreateFromArgb32Callback(Argb32VideoFrameRequestDelegate) . Declaration public void CompleteFrameRequest(uint requestId, long timestampMs, in Argb32VideoFrame frame) Parameters Type Name Description UInt32 requestId The original request ID. Int64 timestampMs The video frame timestamp. Argb32VideoFrame frame The video frame used to complete the request. | Improve this Doc View Source CompleteFrameRequest(UInt32, Int64, I420AVideoFrame) Complete the current request by providing a video frame for it. This must be used if the video track source was created with CreateFromI420ACallback(I420AVideoFrameRequestDelegate) . Declaration public void CompleteFrameRequest(uint requestId, long timestampMs, in I420AVideoFrame frame) Parameters Type Name Description UInt32 requestId The original request ID. Int64 timestampMs The video frame timestamp. I420AVideoFrame frame The video frame used to complete the request. | Improve this Doc View Source CreateFromArgb32Callback(Argb32VideoFrameRequestDelegate) Create a new external video track source from a given user callback providing ARGB32-encoded frames. Declaration public static ExternalVideoTrackSource CreateFromArgb32Callback(Argb32VideoFrameRequestDelegate frameCallback) Parameters Type Name Description Argb32VideoFrameRequestDelegate frameCallback The callback that will be used to request frames for tracks. Returns Type Description ExternalVideoTrackSource The newly created track source. | Improve this Doc View Source CreateFromI420ACallback(I420AVideoFrameRequestDelegate) Create a new external video track source from a given user callback providing I420A-encoded frames. Declaration public static ExternalVideoTrackSource CreateFromI420ACallback(I420AVideoFrameRequestDelegate frameCallback) Parameters Type Name Description I420AVideoFrameRequestDelegate frameCallback The callback that will be used to request frames for tracks. Returns Type Description ExternalVideoTrackSource The newly created track source. | Improve this Doc View Source Dispose() Declaration public void Dispose()"
  },
  "api/Microsoft.MixedReality.WebRTC.BundlePolicy.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.BundlePolicy.html",
    "title": "Enum BundlePolicy | MixedReality-WebRTC Documentation",
    "keywords": "Enum BundlePolicy Bundle policy. See https://www.w3.org/TR/webrtc/#rtcbundlepolicy-enum . Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum BundlePolicy : int Fields Name Description Balanced Gather ICE candidates for each media type in use (audio, video, and data). If the remote endpoint is not bundle-aware, negotiate only one audio and video track on separate transports. MaxBundle Gather ICE candidates for only one track. If the remote endpoint is not bundle-aware, negotiate only one media track. MaxCompat Gather ICE candidates for each track. If the remote endpoint is not bundle-aware, negotiate all media tracks on separate transports."
  },
  "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrame.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrame.html",
    "title": "Struct Argb32VideoFrame | MixedReality-WebRTC Documentation",
    "keywords": "Struct Argb32VideoFrame Single video frame encoded in ARGB interleaved format (32 bits per pixel). The ARGB components are in the order of a little endian 32-bit integer, so 0xAARRGGBB, or (B, G, R, A) as a sequence of bytes in memory with B first and A last. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct Argb32VideoFrame Remarks The use of ref struct is an optimization to avoid heap allocation on each frame while having a nicer-to-use container to pass a frame accross methods. Fields | Improve this Doc View Source data Pointer to the data buffer containing the ARBG data for each pixel. Declaration public IntPtr data Field Value Type Description IntPtr | Improve this Doc View Source height Frame height, in pixels. Declaration public uint height Field Value Type Description UInt32 | Improve this Doc View Source stride Stride in bytes between the ARGB rows. Declaration public int stride Field Value Type Description Int32 | Improve this Doc View Source width Frame width, in pixels. Declaration public uint width Field Value Type Description UInt32"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.SdpTokenAttribute.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.SdpTokenAttribute.html",
    "title": "Class SdpTokenAttribute | MixedReality-WebRTC Documentation",
    "keywords": "Class SdpTokenAttribute Attribute for string properties representing an SDP token, which has constraints on the allowed characters it can contain, as defined in the SDP RFC. See https://tools.ietf.org/html/rfc4566#page-43 for details. Inheritance Object SdpTokenAttribute Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class SdpTokenAttribute : PropertyAttribute Constructors SdpTokenAttribute(Boolean) Declaration public SdpTokenAttribute(bool allowEmpty = true) Parameters Type Name Description Boolean allowEmpty Properties AllowEmpty Allow empty tokens. This is not valid in the RFC, but can be allowed to represent a default value generated at runtime instead of provided by the user. Declaration public bool AllowEmpty { get; } Property Value Type Description Boolean Methods Validate(String, Boolean) Validate an SDP token name against the list of allowed characters: Symbols [!#$%'*+-.^_`{|}~] and ampersand & Alphanumerical [A-Za-z0-9] Declaration public static void Validate(string name, bool allowEmpty = true) Parameters Type Name Description String name The token name to validate. Boolean allowEmpty Remarks See https://tools.ietf.org/html/rfc4566#page-43 for 'token' reference. Exceptions Type Condition ArgumentException The name contains invalid characters."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler.html",
    "title": "Class NodeDssSignaler | MixedReality-WebRTC Documentation",
    "keywords": "Class NodeDssSignaler Simple signaler for debug and testing. This is based on https://github.com/bengreenier/node-dss and SHOULD NOT BE USED FOR PRODUCTION. Inheritance Object Signaler NodeDssSignaler Inherited Members Signaler.PeerConnection Signaler.OnConnect Signaler.OnDisconnect Signaler.OnMessage Signaler.OnFailure Signaler._nativePeer Signaler.OnPeerInitialized(PeerConnection) Signaler.OnPeerUninitializing(PeerConnection) Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class NodeDssSignaler : Signaler Fields AutoLogErrors Automatically log all errors to the Unity console. Declaration public bool AutoLogErrors Field Value Type Description Boolean HttpServerAddress The https://github.com/bengreenier/node-dss HTTP service address to connect to Declaration public string HttpServerAddress Field Value Type Description String LocalPeerId Unique identifier of the local peer. Declaration public string LocalPeerId Field Value Type Description String PollTimeMs The interval (in ms) that the server is polled at Declaration public float PollTimeMs Field Value Type Description Single RemotePeerId Unique identifier of the remote peer. Declaration public string RemotePeerId Field Value Type Description String Methods OnIceCandiateReadyToSend(String, Int32, String) Callback fired when an ICE candidate message has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected override void OnIceCandiateReadyToSend(string candidate, int sdpMlineIndex, string sdpMid) Parameters Type Name Description String candidate Int32 sdpMlineIndex String sdpMid Overrides Signaler.OnIceCandiateReadyToSend(String, Int32, String) OnSdpAnswerReadyToSend(String) Callback fired when a local SDP answer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected override void OnSdpAnswerReadyToSend(string answer) Parameters Type Name Description String answer Overrides Signaler.OnSdpAnswerReadyToSend(String) OnSdpOfferReadyToSend(String) Callback fired when a local SDP offer has been generated and is ready to be sent to the remote peer by the signaling object. Declaration protected override void OnSdpOfferReadyToSend(string offer) Parameters Type Name Description String offer Overrides Signaler.OnSdpOfferReadyToSend(String) SendMessageAsync(Signaler.Message) Asynchronously send a signaling message to the remote peer. Declaration public override Task SendMessageAsync(Signaler.Message message) Parameters Type Name Description Signaler.Message message The signaling message to send to the remote peer. Returns Type Description Task A Task object completed once the message has been sent, but not necessarily delivered. Overrides Signaler.SendMessageAsync(Signaler.Message) Update() Unity Engine Update() hook Declaration protected override void Update() Overrides Signaler.Update() Remarks https://docs.unity3d.com/ScriptReference/MonoBehaviour.Update.html"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.MediaPlayer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.MediaPlayer.html",
    "title": "Class MediaPlayer | MixedReality-WebRTC Documentation",
    "keywords": "Class MediaPlayer Play video frames received from a WebRTC video track. Inheritance Object MediaPlayer Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class MediaPlayer : MonoBehaviour Remarks This component writes to the attached Material , via the attached Renderer . Fields AudioSource Declaration public AudioSource AudioSource Field Value Type Description AudioSource EnableStatistics Declaration public bool EnableStatistics Field Value Type Description Boolean FrameLoadStatHolder A textmesh onto which frame load stat data will be written Declaration public TextMesh FrameLoadStatHolder Field Value Type Description TextMesh Remarks This is how fast the frames are given from the underlying implementation FramePresentStatHolder A textmesh onto which frame present stat data will be written Declaration public TextMesh FramePresentStatHolder Field Value Type Description TextMesh Remarks This is how fast we render frames to the display FrameQueue The frame queue from which frames will be rendered. Declaration public IVideoFrameQueue FrameQueue Field Value Type Description IVideoFrameQueue FrameSkipStatHolder A textmesh into which frame skip stat dta will be written Declaration public TextMesh FrameSkipStatHolder Field Value Type Description TextMesh Remarks This is how often we skip presenting an underlying frame MaxVideoFramerate Declaration public float MaxVideoFramerate Field Value Type Description Single VideoSource Declaration public VideoSource VideoSource Field Value Type Description VideoSource"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.CaptureCameraAttribute.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.CaptureCameraAttribute.html",
    "title": "Class CaptureCameraAttribute | MixedReality-WebRTC Documentation",
    "keywords": "Class CaptureCameraAttribute Attribute for a property used by SceneVideoSource to capture the content of a framebuffer, and for which some constraints on stereoscopic rendering options need to be enforced (and errors can be reported in the Editor if they are not followed). Inheritance Object CaptureCameraAttribute Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class CaptureCameraAttribute : PropertyAttribute Methods Validate(Camera) Validate that a given instance can be used for framebuffer capture by SceneVideoSource based on the XR settings currently in effect. Declaration public static void Validate(Camera camera) Parameters Type Name Description Camera camera The camera instance to test the settings of. See Also SceneVideoSource"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStoppedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStoppedEvent.html",
    "title": "Class AudioStreamStoppedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioStreamStoppedEvent Unity event corresponding to an on-going audio stream being stopped. Inheritance Object AudioStreamStoppedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class AudioStreamStoppedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException.html",
    "title": "Class SctpNotNegotiatedException | MixedReality-WebRTC Documentation",
    "keywords": "Class SctpNotNegotiatedException Exception thrown when trying to add a data channel to a peer connection after a connection to a remote peer was established without an SCTP handshake. When using data channels, at least one data channel must be added to the peer connection before calling CreateOffer() to signal to the implementation the intent to use data channels and the need to perform a SCTP handshake during the connection. Inheritance Object SctpNotNegotiatedException Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class SctpNotNegotiatedException : Exception Constructors | Improve this Doc View Source SctpNotNegotiatedException() Declaration public SctpNotNegotiatedException() | Improve this Doc View Source SctpNotNegotiatedException(String) Declaration public SctpNotNegotiatedException(string message) Parameters Type Name Description String message | Improve this Doc View Source SctpNotNegotiatedException(String, Exception) Declaration public SctpNotNegotiatedException(string message, Exception inner) Parameters Type Name Description String message Exception inner"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration.html",
    "title": "Class PeerConnectionConfiguration | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnectionConfiguration Configuration to initialize a PeerConnection . Inheritance Object PeerConnectionConfiguration Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class PeerConnectionConfiguration : object Fields | Improve this Doc View Source BundlePolicy Bundle policy for the connection. Declaration public BundlePolicy BundlePolicy Field Value Type Description BundlePolicy | Improve this Doc View Source IceServers List of TURN and/or STUN servers to use for NAT bypass, in order of preference. Declaration public List<IceServer> IceServers Field Value Type Description List < IceServer > | Improve this Doc View Source IceTransportType ICE transport policy for the connection. Declaration public IceTransportType IceTransportType Field Value Type Description IceTransportType | Improve this Doc View Source SdpSemantic SDP semantic for the connection. Declaration public SdpSemantic SdpSemantic Field Value Type Description SdpSemantic Remarks Plan B is deprecated, do not use it."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelAddedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelAddedDelegate.html",
    "title": "Delegate PeerConnection.DataChannelAddedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate PeerConnection.DataChannelAddedDelegate Delegate for DataChannelAdded event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void DataChannelAddedDelegate(DataChannel channel); Parameters Type Name Description DataChannel channel The newly added data channel."
  },
  "api/Microsoft.MixedReality.WebRTC.MovingAverage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.MovingAverage.html",
    "title": "Class MovingAverage | MixedReality-WebRTC Documentation",
    "keywords": "Class MovingAverage Utility to manage a moving average of a time series. Inheritance Object MovingAverage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class MovingAverage : object Constructors | Improve this Doc View Source MovingAverage(Int32) Create a new moving average with a given window size. Declaration public MovingAverage(int capacity) Parameters Type Name Description Int32 capacity The capacity of the sample window. Properties | Improve this Doc View Source Average Average value of the samples. Declaration public float Average { get; } Property Value Type Description Single | Improve this Doc View Source Capacity Number of samples in the moving average window. Declaration public int Capacity { get; } Property Value Type Description Int32 Methods | Improve this Doc View Source Clear() Clear the moving average and discard all cached samples. Declaration public void Clear() | Improve this Doc View Source Push(Single) Push a new sample and recalculate the current average. Declaration public void Push(float value) Parameters Type Name Description Single value The new value to add."
  },
  "api/Microsoft.MixedReality.WebRTC.IVideoFrameQueue.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IVideoFrameQueue.html",
    "title": "Interface IVideoFrameQueue | MixedReality-WebRTC Documentation",
    "keywords": "Interface IVideoFrameQueue Interface for a queue of video frames. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public interface IVideoFrameQueue Properties | Improve this Doc View Source DequeuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video sink consumes some video frames, typically to render them. Declaration float DequeuedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source DroppedFramesPerSecond Get the number of frames dropped per seconds. This is generally an average statistics representing how many frames were enqueued by a video source but not dequeued fast enough by a video sink, meaning the video sink renders at a slower framerate than the source can produce. Declaration float DroppedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source QueuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video source produces some video frames. Declaration float QueuedFramesPerSecond { get; } Property Value Type Description Single"
  },
  "api/Microsoft.MixedReality.WebRTC.InvalidInteropNativeHandleException.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.InvalidInteropNativeHandleException.html",
    "title": "Class InvalidInteropNativeHandleException | MixedReality-WebRTC Documentation",
    "keywords": "Class InvalidInteropNativeHandleException Exception thrown when an API function expects an interop handle to a valid native object, but receives an invalid handle instead. Inheritance Object InvalidInteropNativeHandleException Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class InvalidInteropNativeHandleException : Exception Constructors | Improve this Doc View Source InvalidInteropNativeHandleException() Declaration public InvalidInteropNativeHandleException() | Improve this Doc View Source InvalidInteropNativeHandleException(String) Declaration public InvalidInteropNativeHandleException(string message) Parameters Type Name Description String message | Improve this Doc View Source InvalidInteropNativeHandleException(String, Exception) Declaration public InvalidInteropNativeHandleException(string message, Exception inner) Parameters Type Name Description String message Exception inner"
  },
  "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameRequestDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameRequestDelegate.html",
    "title": "Delegate I420AVideoFrameRequestDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate I420AVideoFrameRequestDelegate Callback invoked when the WebRTC pipeline needs an external video source to generate a new video frame for the track(s) it is connected to. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void I420AVideoFrameRequestDelegate(in FrameRequest request); Parameters Type Name Description FrameRequest request The request to fulfill with a new I420A video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.I420AVideoFrame.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420AVideoFrame.html",
    "title": "Struct I420AVideoFrame | MixedReality-WebRTC Documentation",
    "keywords": "Struct I420AVideoFrame Single video frame encoded in I420A format (triplanar YUV with optional alpha plane). See e.g. https://wiki.videolan.org/YUV/#I420 for details. The I420 format uses chroma downsampling in both directions, resulting in 12 bits per pixel. With the optional alpha plane, the size increases to 20 bits per pixel. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct I420AVideoFrame Remarks The use of ref struct is an optimization to avoid heap allocation on each frame while having a nicer-to-use container to pass a frame accross methods. The alpha plane is generically supported in this data structure, but actual support in the video tracks depend on the underlying implementation and the video codec used, and is generally not available. Fields | Improve this Doc View Source dataA Optional pointer to the alpha plane buffer, if any, or null if the frame has no alpha plane. Declaration public IntPtr dataA Field Value Type Description IntPtr | Improve this Doc View Source dataU Pointer to the U plane buffer. Declaration public IntPtr dataU Field Value Type Description IntPtr | Improve this Doc View Source dataV Pointer to the V plane buffer. Declaration public IntPtr dataV Field Value Type Description IntPtr | Improve this Doc View Source dataY Pointer to the Y plane buffer. Declaration public IntPtr dataY Field Value Type Description IntPtr | Improve this Doc View Source height Frame height, in pixels. Declaration public uint height Field Value Type Description UInt32 | Improve this Doc View Source strideA Stride in bytes between rows of the A plane, if present. Declaration public int strideA Field Value Type Description Int32 | Improve this Doc View Source strideU Stride in bytes between rows of the U plane. Declaration public int strideU Field Value Type Description Int32 | Improve this Doc View Source strideV Stride in bytes between rows of the V plane. Declaration public int strideV Field Value Type Description Int32 | Improve this Doc View Source strideY Stride in bytes between rows of the Y plane. Declaration public int strideY Field Value Type Description Int32 | Improve this Doc View Source width Frame width, in pixels. Declaration public uint width Field Value Type Description UInt32 Methods | Improve this Doc View Source CopyTo(Byte[]) Copy the frame content to a Byte [] buffer as a contiguous block of memory containing the Y, U, and V planes one after another, and the alpha plane at the end if present. Declaration public void CopyTo(byte[] buffer) Parameters Type Name Description Byte [] buffer The destination buffer to copy the frame to."
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.html",
    "title": "Enum DataChannel.ChannelState | MixedReality-WebRTC Documentation",
    "keywords": "Enum DataChannel.ChannelState Connecting state of a data channel, when adding it to a peer connection or removing it from a peer connection. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum ChannelState : int Fields Name Description Closed The data channel reached end of life and can be destroyed. It cannot be re-connected; instead a new data channel must be created. Closing The data channel is being closed, and is not available anymore for data exchange. Connecting The data channel has just been created, and negotiating is underway to establish a track between the peers. Open The data channel is open and ready to send and receive messages."
  },
  "api/Microsoft.MixedReality.WebRTC.AudioFrameDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioFrameDelegate.html",
    "title": "Delegate AudioFrameDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate AudioFrameDelegate Delegate used for events when an audio frame has been produced and is ready for consumption. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void AudioFrameDelegate(AudioFrame frame); Parameters Type Name Description AudioFrame frame The newly available audio frame."
  },
  "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameRequestDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameRequestDelegate.html",
    "title": "Delegate Argb32VideoFrameRequestDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate Argb32VideoFrameRequestDelegate Callback invoked when the WebRTC pipeline needs an external video source to generate a new video frame for the track(s) it is connected to. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void Argb32VideoFrameRequestDelegate(in FrameRequest request); Parameters Type Name Description FrameRequest request The request to fulfill with a new ARGB32 video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.RemoteVideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.RemoteVideoSource.html",
    "title": "Class RemoteVideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Class RemoteVideoSource This component represents a remote video source added as a video track to an existing WebRTC peer connection by a remote peer and received locally. The video track can optionally be displayed locally with a MediaPlayer . Inheritance Object VideoSource RemoteVideoSource Inherited Members VideoSource.FrameQueue VideoSource.VideoStreamStarted VideoSource.VideoStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class RemoteVideoSource : VideoSource Fields AutoPlayOnAdded Automatically play the remote video track when it is added. This is equivalent to manually calling Play() when the peer connection is initialized. Declaration public bool AutoPlayOnAdded Field Value Type Description Boolean See Also Play() Stop() PeerConnection Peer connection this remote video source is extracted from. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection Properties IsPlaying Is the video source currently playing? The concept of playing is described in the Play() function. Declaration public bool IsPlaying { get; } Property Value Type Description Boolean See Also Play() Stop() Methods Awake() Implementation of MonoBehaviour.Awake which registers some handlers with the peer connection to listen to its OnInitialized and OnShutdown events. Declaration protected void Awake() OnDestroy() Implementation of MonoBehaviour.OnDestroy which unregisters all listeners from the peer connection. Declaration protected void OnDestroy() Play() Manually start playback of the remote video feed by registering some listeners to the peer connection and starting to enqueue video frames as they become ready. Because the WebRTC implementation uses a push model, calling Play() does not necessarily start producing frames immediately. Instead, this starts listening for incoming frames from the remote peer. When a track is actually added by the remote peer and received locally, the VideoStreamStarted event is fired, and soon after frames will start being available for rendering in the internal frame queue. Note that this event may be fired before Play() is called, in which case frames are produced immediately. If AutoPlayOnAdded is true then this is called automatically as soon as the peer connection is initialized. Declaration public void Play() Remarks This is only valid while the peer connection is initialized, that is after the OnInitialized event was fired. See Also Stop() IsPlaying Stop() Stop playback of the remote video feed and unregister the handler listening to remote video frames. Note that this is independent of whether or not a remote track is actually present. In particular this does not fire the VideoStreamStopped , which corresponds to a track being made available to the local peer by the remote peer. Declaration public void Stop() See Also Play() IsPlaying Update() Implementation of MonoBehaviour.Update to execute from the current Unity main thread any background work enqueued from free-threaded callbacks. Declaration protected void Update()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStartedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioStreamStartedEvent.html",
    "title": "Class AudioStreamStartedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioStreamStartedEvent Unity event corresponding to a new audio stream being started. Inheritance Object AudioStreamStartedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class AudioStreamStartedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.AudioSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.AudioSource.html",
    "title": "Class AudioSource | MixedReality-WebRTC Documentation",
    "keywords": "Class AudioSource Base class for audio sources plugging into the internal peer connection API to expose a single audio stream to a renderer ( MediaPlayer or custom). Inheritance Object AudioSource LocalAudioSource RemoteAudioSource Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class AudioSource : MonoBehaviour Fields AudioStreamStarted Declaration public AudioStreamStartedEvent AudioStreamStarted Field Value Type Description AudioStreamStartedEvent AudioStreamStopped Declaration public AudioStreamStoppedEvent AudioStreamStopped Field Value Type Description AudioStreamStoppedEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.TaskExtensions.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.TaskExtensions.html",
    "title": "Class TaskExtensions | MixedReality-WebRTC Documentation",
    "keywords": "Class TaskExtensions Collection of extension methods for Task . Inheritance Object TaskExtensions Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public static class TaskExtensions : object Methods | Improve this Doc View Source AsTask(CancellationToken) A simple helper to enable \"awaiting\" a by creating a task wrapping it. Declaration public static Task AsTask(this CancellationToken cancellationToken) Parameters Type Name Description CancellationToken cancellationToken The to await. Returns Type Description Task The task that can be awaited. | Improve this Doc View Source IgnoreCancellation(Task) Prevents or from trickling up. Declaration public static Task IgnoreCancellation(this Task task) Parameters Type Name Description Task task The task to ignore exceptions for. Returns Type Description Task A wrapping task for the given task. | Improve this Doc View Source IgnoreCancellation<T>(Task<T>, T) Prevents or from trickling up. Declaration public static Task<T> IgnoreCancellation<T>(this Task<T> task, T defaultCancellationReturn = null) Parameters Type Name Description Task <T> task The task to ignore exceptions for. T defaultCancellationReturn The default value to return in case the task is cancelled. Returns Type Description Task <T> A wrapping task for the given task. Type Parameters Name Description T The result type of the Task. | Improve this Doc View Source Unless(Task, CancellationToken) The task will be awaited until the cancellation token is triggered. (await task unless cancelled). Declaration public static Task Unless(this Task task, CancellationToken cancellationToken) Parameters Type Name Description Task task The task to await. CancellationToken cancellationToken The cancellation token to stop awaiting. Returns Type Description Task The task that can be awaited unless the cancellation token is triggered. Remarks This is different from cancelling the task. The use case is to enable a calling method bow out of the await that it can't cancel, but doesn't require completion/cancellation in order to cancel it's own execution."
  },
  "api/Microsoft.MixedReality.WebRTC.SdpSemantic.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.SdpSemantic.html",
    "title": "Enum SdpSemantic | MixedReality-WebRTC Documentation",
    "keywords": "Enum SdpSemantic SDP semantic used for (re)negotiating a peer connection. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum SdpSemantic : int Fields Name Description PlanB Legacy Plan B, deprecated and soon removed. Only available for compatiblity with older implementations if needed. Do not use unless there is a problem with the Unified Plan. UnifiedPlan Unified plan, as standardized in the WebRTC 1.0 standard."
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.html",
    "title": "Class PeerConnection | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnection The WebRTC peer connection object is the entry point to using WebRTC. Inheritance Object PeerConnection Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class PeerConnection : IDisposable Constructors | Improve this Doc View Source PeerConnection() Create a new peer connection object. The object is initially created empty, and cannot be used until InitializeAsync(PeerConnectionConfiguration, CancellationToken) has completed successfully. Declaration public PeerConnection() Fields | Improve this Doc View Source PreferredAudioCodec Name of the preferred audio codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredAudioCodec Field Value Type Description String | Improve this Doc View Source PreferredAudioCodecExtraParams Advanced use only. A semicolon-separated list of \"key=value\" pairs of arguments passed as extra parameters to the preferred audio codec during SDP filtering. This enables configuring codec-specific parameters. Arguments are passed as is, and there is no check on the validity of the parameter names nor their value. This is ignored if PreferredAudioCodec is an empty string, or is not a valid codec name found in the SDP message offer. Declaration public string PreferredAudioCodecExtraParams Field Value Type Description String | Improve this Doc View Source PreferredVideoCodec Name of the preferred video codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredVideoCodec Field Value Type Description String | Improve this Doc View Source PreferredVideoCodecExtraParams Advanced use only. A semicolon-separated list of \"key=value\" pairs of arguments passed as extra parameters to the preferred video codec during SDP filtering. This enables configuring codec-specific parameters. Arguments are passed as is, and there is no check on the validity of the parameter names nor their value. This is ignored if PreferredVideoCodec is an empty string, or is not a valid codec name found in the SDP message offer. Declaration public string PreferredVideoCodecExtraParams Field Value Type Description String Properties | Improve this Doc View Source Initialized Boolean property indicating whether the peer connection has been initialized. Declaration public bool Initialized { get; } Property Value Type Description Boolean | Improve this Doc View Source IsConnected Indicates whether the peer connection is established and can exchange some track content (audio/video/data) with the remote peer. Declaration public bool IsConnected { get; } Property Value Type Description Boolean Remarks This does not indicate whether the ICE exchange is done, as it may continue after the peer connection negotiated a first session. For ICE connection status, see the IceStateChanged event. Methods | Improve this Doc View Source AddCustomLocalVideoTrack(String, ExternalVideoTrackSource) Add a local video track backed by an external video source managed by the caller. Unlike with AddLocalVideoTrackAsync(PeerConnection.LocalVideoTrackSettings) which manages a local video capture device and automatically produce frames, an external video source provides video frames directly to WebRTC when asked to do so via the provided callback. Declaration public LocalVideoTrack AddCustomLocalVideoTrack(string trackName, ExternalVideoTrackSource externalSource) Parameters Type Name Description String trackName Name of the new track. ExternalVideoTrackSource externalSource External source providing the frames for the track. Returns Type Description LocalVideoTrack | Improve this Doc View Source AddDataChannelAsync(String, Boolean, Boolean) Add a new in-band data channel whose ID will be determined by the implementation. A data channel is negotiated in-band when one peer requests its creation to the WebRTC core, and the implementation negotiates with the remote peer an appropriate ID by sending some SDP offer message. In that case once accepted the other peer will automatically create the appropriate data channel on its side with that negotiated ID, and the ID will be returned on both sides to the user for information. Compared to out-of-band messages, this requires exchanging some SDP messages, but avoids having to determine a common unused ID and having to explicitly open the data channel on both sides. Declaration public Task<DataChannel> AddDataChannelAsync(string label, bool ordered, bool reliable) Parameters Type Name Description String label The data channel name. Boolean ordered Indicates whether data channel messages are ordered (see Ordered ). Boolean reliable Indicates whether data channel messages are reliably delivered (see Reliable ). Returns Type Description Task < DataChannel > Returns a task which completes once the data channel is created. Remarks See the critical remark about SCTP handshake in AddDataChannelAsync(UInt16, String, Boolean, Boolean) . Exceptions Type Condition SctpNotNegotiatedException SCTP not negotiated. Call CreateOffer() first. | Improve this Doc View Source AddDataChannelAsync(UInt16, String, Boolean, Boolean) Add a new out-of-band data channel with the given ID. A data channel is negotiated out-of-band when the peers agree on an identifier by any mean not known to WebRTC, and both open a data channel with that ID. The WebRTC will match the incoming and outgoing pipes by this ID to allow sending and receiving through that channel. This requires some external mechanism to agree on an available identifier not otherwise taken by another channel, and also requires to ensure that both peers explicitly open that channel. Declaration public Task<DataChannel> AddDataChannelAsync(ushort id, string label, bool ordered, bool reliable) Parameters Type Name Description UInt16 id The unique data channel identifier to use. String label The data channel name. Boolean ordered Indicates whether data channel messages are ordered (see Ordered ). Boolean reliable Indicates whether data channel messages are reliably delivered (see Reliable ). Returns Type Description Task < DataChannel > Returns a task which completes once the data channel is created. Remarks Data channels use DTLS over SCTP, which ensure in particular that messages are encrypted. To that end, while establishing a connection with the remote peer, some specific SCTP handshake must occur. This handshake is only performed if at least one data channel was added to the peer connection when the connection starts its negotiation with CreateOffer() . Therefore, if the user wants to use a data channel at any point during the lifetime of this peer connection, it is critical to add at least one data channel before CreateOffer() is called. Otherwise all calls will fail with an SctpNotNegotiatedException exception. Exceptions Type Condition SctpNotNegotiatedException SCTP not negotiated. Call CreateOffer() first. | Improve this Doc View Source AddIceCandidate(String, Int32, String) Inform the WebRTC peer connection of a newly received ICE candidate. Declaration public void AddIceCandidate(string sdpMid, int sdpMlineindex, string candidate) Parameters Type Name Description String sdpMid Int32 sdpMlineindex String candidate | Improve this Doc View Source AddLocalAudioTrackAsync() Add to the current connection an audio track from a local audio capture device (microphone). Declaration public Task AddLocalAudioTrackAsync() Returns Type Description Task Asynchronous task completed once the device is capturing and the track is added. Remarks On UWP this requires the \"microphone\" capability. See https://docs.microsoft.com/en-us/windows/uwp/packaging/app-capability-declarations for more details. | Improve this Doc View Source AddLocalVideoTrackAsync(PeerConnection.LocalVideoTrackSettings) Add to the current connection a video track from a local video capture device (webcam). The video track receives its video data from an underlying hidden source associated with the track and producing video frames by capturing them from a capture device accessible from the local host machine, generally a USB webcam or built-in device camera. The underlying video source initially starts in the capturing state, and will remain live for as long as the track is added to the peer connection. It can be temporarily disabled and re-enabled (see Enabled ) while remaining added to the peer connection. Note that disabling the track does not release the device; the source retains exclusive access to it. Declaration public Task<LocalVideoTrack> AddLocalVideoTrackAsync(PeerConnection.LocalVideoTrackSettings settings = null) Parameters Type Name Description PeerConnection.LocalVideoTrackSettings settings Video capture settings for configuring the capture device associated with the underlying video track source. Returns Type Description Task < LocalVideoTrack > This returns a task which, upon successful completion, provides an instance of LocalVideoTrack representing the newly added video track. Remarks On UWP this requires the \"webcam\" capability. See https://docs.microsoft.com/en-us/windows/uwp/packaging/app-capability-declarations for more details. The video capture device may be accessed several times during the initializing process, generally once for listing and validating the capture format, and once for actually starting the video capture. Note that the capture device must support a capture format with the given constraints of profile ID or kind, capture resolution, and framerate, otherwise the call will fail. That is, there is no fallback mechanism selecting a closest match. Developers should use GetVideoCaptureFormatsAsync(String) to list the supported formats ahead of calling AddLocalVideoTrackAsync(PeerConnection.LocalVideoTrackSettings) , and can build their own fallback mechanism on top of this call if needed. Examples Create a video track called \"MyTrack\", with Mixed Reality Capture (MRC) enabled. This assumes that the platform supports MRC. Note that if MRC is not available the call will still succeed, but will return a track without MRC enabled. var settings = new LocalVideoTrackSettings { trackName = \"MyTrack\", enableMrc = true }; var videoTrack = await peerConnection.AddLocalVideoTrackAsync(settings); Create a video track from a local webcam, asking for a capture format suited for video conferencing, and a target framerate of 30 frames per second (FPS). The implementation will select an appropriate capture resolution. This assumes that the device supports video profiles, and has at least one capture format supporting 30 FPS capture associated with the VideoConferencing profile. Otherwise the call will fail. var settings = new LocalVideoTrackSettings { videoProfileKind = VideoProfileKind.VideoConferencing, framerate = 30.0 }; var videoTrack = await peerConnection.AddLocalVideoTrackAsync(settings); | Improve this Doc View Source Close() Close the peer connection and destroy the underlying native resources. Declaration public void Close() Remarks This is equivalent to Dispose() . See Also Dispose() | Improve this Doc View Source CreateAnswer() Create an SDP answer message to a previously-received offer, to accept a connection. Once the message is ready to be sent, the LocalSdpReadytoSend event is fired to allow the user to send that message to the remote peer via its selected signaling solution. Declaration public bool CreateAnswer() Returns Type Description Boolean true if the answer creation task was successfully submitted. Remarks The SDP answer message is not successfully created until the LocalSdpReadytoSend event is triggered, and may still fail even if this method returns true , for example if the peer connection is not in a valid state to create an answer. | Improve this Doc View Source CreateOffer() Create an SDP offer message as an attempt to establish a connection. Once the message is ready to be sent, the LocalSdpReadytoSend event is fired to allow the user to send that message to the remote peer via its selected signaling solution. Declaration public bool CreateOffer() Returns Type Description Boolean true if the offer creation task was successfully submitted. Remarks The SDP offer message is not successfully created until the LocalSdpReadytoSend event is triggered, and may still fail even if this method returns true , for example if the peer connection is not in a valid state to create an offer. | Improve this Doc View Source Dispose() Dispose of native resources by closing the peer connection. Declaration public void Dispose() Remarks This is equivalent to Close() . See Also Close() | Improve this Doc View Source GetVideoCaptureDevicesAsync() Get the list of video capture devices available on the local host machine. Declaration public static Task<List<VideoCaptureDevice>> GetVideoCaptureDevicesAsync() Returns Type Description Task < List < VideoCaptureDevice >> The list of available video capture devices. Remarks Assign one of the returned VideoCaptureDevice to the videoDevice field to force a local video track to use that device when creating it with AddLocalVideoTrackAsync(PeerConnection.LocalVideoTrackSettings) . | Improve this Doc View Source GetVideoCaptureFormatsAsync(String) Enumerate the video capture formats for the specified video capture device. Declaration public static Task<List<VideoCaptureFormat>> GetVideoCaptureFormatsAsync(string deviceId) Parameters Type Name Description String deviceId Unique identifier of the video capture device to enumerate the capture formats of, as retrieved from the id field of a capture device enumerated with GetVideoCaptureDevicesAsync() . Returns Type Description Task < List < VideoCaptureFormat >> The list of available video capture formats for the specified video capture device. | Improve this Doc View Source InitializeAsync(PeerConnectionConfiguration, CancellationToken) Initialize the current peer connection object asynchronously. Most other methods will fail unless this call completes successfully, as it initializes the underlying native implementation object required to create and manipulate the peer connection. Once this call asynchronously completed, the Initialized property becomes true . Declaration public Task InitializeAsync(PeerConnectionConfiguration config = null, CancellationToken token = null) Parameters Type Name Description PeerConnectionConfiguration config Configuration for initializing the peer connection. CancellationToken token Optional cancellation token for the initialize task. This is only used if the singleton task was created by this call, and not a prior call. Returns Type Description Task The singleton task used to initialize the underlying native peer connection. Remarks This method is multi-thread safe, and will always return the same task object from the first call to it until the peer connection object is deinitialized. This allows multiple callers to all execute some action following the initialization, without the need to force a single caller and to synchronize with it. | Improve this Doc View Source IsLocalAudioTrackEnabled() Check if the local audio track associated with this peer connection is enabled. Disable audio tracks are still active, but are silent. Declaration public bool IsLocalAudioTrackEnabled() Returns Type Description Boolean true if the track is enabled, or false otherwise | Improve this Doc View Source RemoveLocalAudioTrack() Remove from the current connection the local audio track added with AddLocalAudioTrackAsync() . Declaration public void RemoveLocalAudioTrack() | Improve this Doc View Source RemoveLocalVideoTrack(LocalVideoTrack) Remove from the current connection the local video track added with AddLocalAudioTrackAsync() . Declaration public void RemoveLocalVideoTrack(LocalVideoTrack track) Parameters Type Name Description LocalVideoTrack track | Improve this Doc View Source RemoveLocalVideoTracksFromSource(ExternalVideoTrackSource) Remove all the local video tracks associated with the given video track source. Declaration public void RemoveLocalVideoTracksFromSource(ExternalVideoTrackSource source) Parameters Type Name Description ExternalVideoTrackSource source The video track source. Remarks Currently there is a 1:1 mapping between tracks and sources (source sharing is not available), therefore this is equivalent to RemoveLocalVideoTrack(LocalVideoTrack) . | Improve this Doc View Source SetBitrate(Nullable<UInt32>, Nullable<UInt32>, Nullable<UInt32>) Set the bitrate allocated to all RTP streams sent by this connection. Other limitations might affect these limits and are respected (for example \"b=AS\" in SDP). Declaration public void SetBitrate(uint? minBitrateBps = null, uint? startBitrateBps = null, uint? maxBitrateBps = null) Parameters Type Name Description Nullable < UInt32 > minBitrateBps Minimum bitrate in bits per second. Nullable < UInt32 > startBitrateBps Start/current target bitrate in bits per second. Nullable < UInt32 > maxBitrateBps Maximum bitrate in bits per second. | Improve this Doc View Source SetFrameHeightRoundMode(PeerConnection.FrameHeightRoundMode) [HoloLens 1 only] Use this function to select whether resolutions where height is not multiple of 16 pixels should be cropped, padded, or left unchanged. Default is Crop to avoid severe artifacts produced by the H.264 hardware encoder on HoloLens 1 due to a bug with the encoder. This is the recommended value, and should be used unless cropping discards valuable data in the top and bottom rows for a given usage, in which case Pad can be used as a replacement but may still produce some mild artifacts. This has no effect on other platforms. Declaration public static void SetFrameHeightRoundMode(PeerConnection.FrameHeightRoundMode value) Parameters Type Name Description PeerConnection.FrameHeightRoundMode value The rounding mode for video frames. | Improve this Doc View Source SetLocalAudioTrackEnabled(Boolean) Enable or disable the local audio track associated with this peer connection. Disable audio tracks are still active, but are silent. Declaration public void SetLocalAudioTrackEnabled(bool enabled = true) Parameters Type Name Description Boolean enabled true to enable the track, or false to disable it | Improve this Doc View Source SetRemoteDescription(String, String) Pass the given SDP description received from the remote peer via signaling to the underlying WebRTC implementation, which will parse and use it. This must be called by the signaler when receiving a message. Declaration public void SetRemoteDescription(string type, string sdp) Parameters Type Name Description String type The type of SDP message (\"offer\" or \"answer\") String sdp The content of the SDP message Events | Improve this Doc View Source Argb32RemoteVideoFrameReady Event that occurs when a video frame from a remote peer has been received and is available for render. Declaration public event Argb32VideoFrameDelegate Argb32RemoteVideoFrameReady Event Type Type Description Argb32VideoFrameDelegate | Improve this Doc View Source Connected Event fired when a connection is established. Declaration public event Action Connected Event Type Type Description Action | Improve this Doc View Source DataChannelAdded Event fired when a data channel is added to the peer connection. This event is always fired, whether the data channel is created by the local peer or the remote peer, and is negotiated (out-of-band) or not (in-band). If an in-band data channel is created by the local peer, the ID field is not yet available when this event is fired, because the ID has not been agreed upon with the remote peer yet. Declaration public event PeerConnection.DataChannelAddedDelegate DataChannelAdded Event Type Type Description PeerConnection.DataChannelAddedDelegate | Improve this Doc View Source DataChannelRemoved Event fired when a data channel is removed from the peer connection. This event is always fired, whatever its creation method (negotiated or not) and original creator (local or remote peer). Declaration public event PeerConnection.DataChannelRemovedDelegate DataChannelRemoved Event Type Type Description PeerConnection.DataChannelRemovedDelegate | Improve this Doc View Source I420ARemoteVideoFrameReady Event that occurs when a video frame from a remote peer has been received and is available for render. Declaration public event I420AVideoFrameDelegate I420ARemoteVideoFrameReady Event Type Type Description I420AVideoFrameDelegate | Improve this Doc View Source IceCandidateReadytoSend Event that occurs when a local ICE candidate is ready to be transmitted. Declaration public event PeerConnection.IceCandidateReadytoSendDelegate IceCandidateReadytoSend Event Type Type Description PeerConnection.IceCandidateReadytoSendDelegate | Improve this Doc View Source IceGatheringStateChanged Event that occurs when the state of the ICE gathering changed. Declaration public event PeerConnection.IceGatheringStateChangedDelegate IceGatheringStateChanged Event Type Type Description PeerConnection.IceGatheringStateChangedDelegate | Improve this Doc View Source IceStateChanged Event that occurs when the state of the ICE connection changed. Declaration public event PeerConnection.IceStateChangedDelegate IceStateChanged Event Type Type Description PeerConnection.IceStateChangedDelegate | Improve this Doc View Source LocalAudioFrameReady Event that occurs when an audio frame from a local track has been produced locally and is available for render. Declaration public event AudioFrameDelegate LocalAudioFrameReady Event Type Type Description AudioFrameDelegate Remarks WARNING -- This is currently not implemented in the underlying WebRTC implementation, so THIS EVENT IS NEVER FIRED. | Improve this Doc View Source LocalSdpReadytoSend Event that occurs when a local SDP message is ready to be transmitted. Declaration public event PeerConnection.LocalSdpReadyToSendDelegate LocalSdpReadytoSend Event Type Type Description PeerConnection.LocalSdpReadyToSendDelegate | Improve this Doc View Source RemoteAudioFrameReady Event that occurs when an audio frame from a remote peer has been received and is available for render. Declaration public event AudioFrameDelegate RemoteAudioFrameReady Event Type Type Description AudioFrameDelegate | Improve this Doc View Source RenegotiationNeeded Event that occurs when a renegotiation of the session is needed. This generally occurs as a result of adding or removing tracks, and the user should call CreateOffer() to actually start a renegotiation. Declaration public event Action RenegotiationNeeded Event Type Type Description Action | Improve this Doc View Source TrackAdded Event that occurs when a remote track is added to the current connection. Declaration public event Action<PeerConnection.TrackKind> TrackAdded Event Type Type Description Action < PeerConnection.TrackKind > | Improve this Doc View Source TrackRemoved Event that occurs when a remote track is removed from the current connection. Declaration public event Action<PeerConnection.TrackKind> TrackRemoved Event Type Type Description Action < PeerConnection.TrackKind >"
  },
  "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoProfileKind.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.PeerConnection.VideoProfileKind.html",
    "title": "Enum PeerConnection.VideoProfileKind | MixedReality-WebRTC Documentation",
    "keywords": "Enum PeerConnection.VideoProfileKind Kind of video profile. This corresponds to the enum of the API. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum VideoProfileKind : int Fields Name Description BalancedVideoAndPhoto Balanced video profile to capture both videos and photos. HdrWithWcgPhoto Video profile for capturing photos with High Dynamic Range (HDR) and Wide Color Gamut (WCG). HdrWithWcgVideo Video profile for capturing videos with High Dynamic Range (HDR) and Wide Color Gamut (WCG). HighFrameRate Video profile containing high framerate capture formats. HighQualityPhoto Video profile for high quality photo capture. PhotoSequence Video profile for capturing a sequence of photos. Unspecified Unspecified video profile kind. Used to remove any constraint on the video profile kind. VariablePhotoSequence Video profile for capturing a variable sequence of photos. VideoConferencing Video profile for video conferencing, often of lower power consumption and lower latency by deprioritizing higher resolutions. This is the recommended profile for most WebRTC applications, if supported. VideoHdr8 Video profile for capturing videos with High Dynamic Range (HDR). VideoRecording Video profile for video recording, often of higher quality and framerate at the expense of power consumption and latency. See Also https://docs.microsoft.com/en-us/uwp/api/windows.media.capture.knownvideoprofile"
  },
  "index.html": {
    "href": "index.html",
    "title": "Index | MixedReality-WebRTC Documentation",
    "keywords": "MixedReality-WebRTC documentation User Manual Introduction Getting started Download Installation Building from sources C# tutorial (Desktop) C# tutorial (UWP) Unity tutorial C# library Feature Overview Tutorial (Desktop) Tutorial (UWP) Peer Connection Signaling Unity integration Feature Overview Tutorial Peer Connection Signaler Media Player Audio LocalAudioSource RemoteAudioSource Video LocalVideoSource RemoteVideoSource Advanced topics Building the Core dependencies from sources API reference C# library PeerConnection The PeerConnection object is the API entry point to establish a remote connection. VideoFrameQueue<T> The video frame queue bridges a video source and a video sink. DataChannel Encapsulates a single data channel for transmitting raw blobs of bytes. Unity integration PeerConnection The PeerConnection component builds on the same-named library class to expose a remote peer connection. Signaler The abstract Signaler component is the base class for signaling implementations. LocalVideoSource The LocalVideoSource component provides access to the local webcam for local rendering and remote streaming."
  },
  "manual/cs/helloworld-cs-setup-core3.html": {
    "href": "manual/cs/helloworld-cs-setup-core3.html",
    "title": "Creating a project | MixedReality-WebRTC Documentation",
    "keywords": "Creating a project In this tutorial we use .NET Core 3.0 to create a C# Desktop application. Because there is currently no simple solution to display raw video frames from a .NET Core application, like there is with UWP, we will limit this tutorial to creating a simple console application without graphics. Think of it as a recording tool to broadcast audio and/or video to a remote device. Note This tutorial assumes that the host device where the app will be running during the tutorial has access to: a webcam, or any other video capture device recognized by WebRTC a microphone, or any other audio capture device recognized by WebRTC Install .NET Core 3.0 Download the latest .NET Core 3.0 SDK (and not Runtime) from its download page and install it. Warning Visual Studio v16.3 or later is required, otherwise the console application might not run correctly inside Visual Studio. Generate the project Open a terminal and use the dotnet command to create a new project from the console template. We will name this tutorial project TestNetCoreConsole . dotnet new console --name TestNetCoreConsole This generates a folder named TestNetCoreConsole which contains the following notable files: TestNetCoreConsole.csproj : The C# project Program.cs : The C# source code for the application Note Starting Visual Studio v16.3, this project can also be generated from the Visual Studio wizard by creating a new project and selecting Console App (.NET Core) , which will default to .NET Core 3.0. Previous versions of Visual Studio default to .NET Core 2.1 or lower. Open the .NET Core project in Visual Studio 2019 Open the C# project generated earlier ( TestNetCoreConsole.csproj ), then build and run it, either by pressing F5 or selecting in the menu Debug > Start Debugging . After the project built successfully, a terminal window should appear. Note The project can alternatively be built on the command line with dotnet build , and launched with dotnet run (which implies building). Executing a dotnet run returns immediately, since the empty generated program does not currently output anything. Add a dependency to MixedReality-WebRTC In order to use the MixedReality-WebRTC project in this new TestNetCoreConsole app, we will add a dependency to its C# NuGet package hosted on nuget.org . This is by far the easiest way, although a locally-built copy of the Microsoft.MixedReality.WebRTC.dll assembly could also be alternatively used (but this is out of the scope of this tutorial). There are again multiple ways to add a reference to this NuGet package, in particular via the Visual Studio NuGet package manager for the project, or via the dotnet command line. For simplicity, we show here how to do so the dotnet way, which simply involves typing a single command from within the project folder. dotnet add TestNetCoreConsole.csproj package Microsoft.MixedReality.WebRTC This will download from nuget.org and install the Microsoft.MixedReality.WebRTC.nupkg NuGet package, which contains the same-named assembly, as well as its native dependencies (x86 and x64) for the Windows Desktop platform. Note dotnet may complain about being unable to find a stable package, if no stable package has been released yet. Remember that dotnet only install stable packages by default, you can force a specific non-stable version with the --version option. After that, TestNetCoreConsole.csproj should contain a reference to the package, with a version corresponding to the latest stable version found on nuget.org , or the one you specified with the --version option. <ItemGroup> <PackageReference Include=\"Microsoft.MixedReality.WebRTC\" Version=\"...\" /> </ItemGroup> Test the reference In order to ensure everything works fine and the Microsoft.MixedReality.WebRTC assembly can be used, we will use one of its functions to list the video capture devices, as a test. This makes uses of the static method PeerConnection.GetVideoCaptureDevicesAsync() . This is more simple than creating objects, as there is no clean-up needed after use. Edit the Program.cs file: At the top of the file, add some using statement to import the Microsoft.MixedReality.WebRTC assembly. Also import the System.Threading.Tasks module, as we will use the async / await construct and the Task object. using System.Threading.Tasks; using Microsoft.MixedReality.WebRTC; Modify the signature of the Main function to make it asynchronous, by changing its return type from void to Task and adding the async keyword. This allows using the await keyword inside its body. static async Task Main(string[] args) Change the body of the Main function to call GetVideoCaptureDevicesAsync() and display the list of devices found on the standard output. static async Task Main(string[] args) { try { // Asynchronously retrieve a list of available video capture devices (webcams). var deviceList = await PeerConnection.GetVideoCaptureDevicesAsync(); // For example, print them to the standard output foreach (var device in deviceList) { Console.WriteLine($\"Found webcam {device.name} (id: {device.id})\"); } } catch (Exception e) { Console.WriteLine(e.Message); } } Launch the application again. This time the terminal window shows a list of devices. This list depends on the actual host device (computer) where the application is running, but looks something like: Found webcam <some webcam name> (id: <some long ID>) Note that there might be multiple lines if multiple capture devices are available, which is unusual but can happen e.g. if you plug a USB webcam into a laptop which already has an integrated webcam. In general the first capture device listed will be the default one used by WebRTC, although it is possible to explicitly select a device (see PeerConnection.AddLocalVideoTrackAsync() for more details). Next : Creating a peer connection"
  },
  "manual/cs/helloworld-cs-peerconnection-uwp.html": {
    "href": "manual/cs/helloworld-cs-peerconnection-uwp.html",
    "title": "Creating a peer connection | MixedReality-WebRTC Documentation",
    "keywords": "Creating a peer connection Next, we create a PeerConnection object which encapsulates the connection to the remote peer. The PeerConnection class is marked as disposable, so must be disposed to clean-up the native resources. Failing to do so generally lead to crashes or hangs, as the internal WebRTC threads are not stopped and therefore the native DLL cannot be unloaded. Unlike in the .NET Core tutorial where we build a console application however, here with the XAML framework we cannot easily use the using var construct to rely on the C# compiler to call the Dispose() method for us, because currently the only available method is the OnLoaded() method, and it will terminate and release all its local variable once loading is finished, and before the end of the application. Instead, we need to keep a reference to the PeerConnection instance and call Dispose() explicitly when done with it. Continue editing the MainPage.xaml.cs file and append the following: At the top of the MainPage class, declare a private variable of type PeerConnection . private PeerConnection _peerConnection; Continue to append to the OnLoaded() method. First, instantiate the peer connection. _peerConnection = new PeerConnection(); The PeerConnection object is initally created in an idle state where it cannot be used until initialized with a call to InitializeAsync() . This method takes a PeerConnectionConfiguration object which allows specifying some options to configure the connection. In this tutorial, most default options are suitable, but we want to specify a STUN server to make sure that the peer connection can connect to the remote peer even if behind a NAT . var config = new PeerConnectionConfiguration { IceServers = new List<IceServer> { new IceServer{ Urls = { \"stun:stun.l.google.com:19302\" } } } }; await _peerConnection.InitializeAsync(config); In this example we use a free STUN server courtesy of Google. Note that this is fine for testing, but must not be used for production . Also, the ICE server list uses the List<> generic class, so we need to import the System.Collections.Generic module with a using directive at the top of the file. using System.Collections.Generic; Print a simple message to the debugger to confirm that the peer connection wass initialized. In a real-world application, properly notifying the user of failures is critical, but here for the sake of this tutorial we simply rely on a any exception interrupting the application before the message is printed if an error occur. Debugger.Log(0, \"\", \"Peer connection initialized successfully.\\n\"); In the App_Suspending() event handler, add some code to dispose of the peer connection. private void App_Suspending(object sender, SuspendingEventArgs e) { if (_peerConnection != null) { _peerConnection.Close(); _peerConnection.Dispose(); _peerConnection = null; } } Run the application again; the printed message should appear after some time in the Visual Studio Output window under the Debug section. It can take up to a few seconds to initialize the peer connection, depending on the device. Next : Adding local media tracks"
  },
  "manual/cs/helloworld-cs-peerconnection-core3.html": {
    "href": "manual/cs/helloworld-cs-peerconnection-core3.html",
    "title": "Creating a peer connection | MixedReality-WebRTC Documentation",
    "keywords": "Creating a peer connection Next, we create a PeerConnection object which encapsulates the connection to the remote peer. Continue editing the Program.cs file and append the following: Create the peer connection object. Note that the PeerConnection class is marked as disposable, so must be disposed to clean-up the native resources. Failing to do so generally lead to crashes or hangs, as the internal WebRTC threads are not stopped and therefore the native DLL cannot be unloaded. using var pc = new PeerConnection(); This construct with the using keyword is a shorthand that will automatically call Dispose() when that variable pc gets out of scope, in our case at the end of the Main function. The PeerConnection object is initally created in an idle state where it cannot be used until initialized with a call to InitializeAsync() . This method takes a PeerConnectionConfiguration object which allows specifying some options to configure the connection. In this tutorial, most default options are suitable, but we want to specify a STUN server to make sure that the peer connection can connect to the remote peer even if behind a NAT . var config = new PeerConnectionConfiguration { IceServers = new List<IceServer> { new IceServer{ Urls = { \"stun:stun.l.google.com:19302\" } } } }; await pc.InitializeAsync(config); In this example we use a free STUN server courtesy of Google. Note that this is fine for testing, but must not be used for production . Also, the ICE server list uses the List<> generic class, so we need to import the System.Collections.Generic module with a using directive at the top of the file. using System.Collections.Generic; Print a simple message to console to notify the user that the peer connection is initialized. This is optional, but is always good practice to inform the user after important steps completed, whether successfully or not. Console.WriteLine(\"Peer connection initialized.\"); Run the application again; the printed message should appear after some time. It generally takes up to a few seconds to initialize the peer connection, depending on the device. Next : Adding local media tracks"
  },
  "manual/cs/helloworld-cs-mediatracks-uwp.html": {
    "href": "manual/cs/helloworld-cs-mediatracks-uwp.html",
    "title": "Add local media tracks | MixedReality-WebRTC Documentation",
    "keywords": "Add local media tracks Now that the peer connection is initialized, there are two possible paths, which can be both used: Immediately adding local audio and/or video tracks to the peer connection, so that they are available right away when the connection will be established with the remote peer. Waiting for the connection to be established, and add the local media tracks after that. The first case is benefical in the sense that media tracks will be immediately negotiated during the connection establishing, without the need for an extra negotiation specific for the tracks. However it requires knowing in advance that the tracks are used. Conversely, the latter case corresponds to a scenario like late joining, where the user or the application can control when to add or remove local tracks, at the expense of requiring an extra network negotiation each time the list of tracks is changed. In this tutorial, we add the local media tracks right away for simplicity. Adding the tracks Continue editing the MainPage.xaml.cs file and append in the OnLoaded method the following: Create a new LocalVideoTrack private variable. LocalVideoTrack _localVideoTrack; Use the AddLocalVideoTrackAsync() method to add to the peer connection a local video track sending to the remote peer some video frames obtained from a local video capture device (webcam). _localVideoTrack = await _peerConnection.AddLocalVideoTrackAsync(); This method optionally takes a LocalVideoTrackSettings object to configure the video capture. In this tutorial, we leave that object out and use the default settings, which will open the first available webcam with its default resolution and framerate. This is generally acceptable, although on mobile devices like HoloLens you probably want to limit the resolution and framerate to reduce the power consumption and save on battery. Use the AddLocalAudioTrackAsync() method to add to the peer connection a local audio track sending to the remote peer some audio frames obtained from a local audio capture device (microphone). await _peerConnection.AddLocalAudioTrackAsync(); Unlike for the video track, the audio track currently does not offer any configuration option, and will always use the first available audio capture device. At this point, if you run the application again, there is no visible difference, except some extra delay to open the audio and video devices; this delay varies greatly depending on the number of capture devices on the host machine, but is generally within a few seconds too, sometimes much less. Additionally, if the webcam or microphone have a LED indicating recording, it should turn ON when the capture device starts recording. But the captured audio and video are not visible. This is because the audio and video tracks are capturing frames from the webcam and microphone, to be sent later to the remote peer once connected to it, but there is no local rendering by default. Importing the VideoBridge utility In order to display the local webcam feed as a feedback for the user, we need to collect the video frames captured by WebRTC via the webcam, and display them locally in the app using whichever technology we choose. In this tutorial for simplicity we use the MediaPlayerElement XAML control, which is based on the Media Foundation framework. And to keep things simple, we also use the TestAppUwp.Video.VideoBridge helper class from the TestAppUWP sample application provided by MixedReality-WebRTC, which bridges a raw source of frames (WebRTC) with the MediaPlayerElement , taking care of the interoperability details for us. The TestAppUwp.Video.VideoBridge helper class makes use of the StreamSamplePool class, which also needs to be imported. For the sake of simplicity in this tutorial we simply copy those two files. Download the VideoBridge.cs and StreamSamplePool.cs from the MixedReality-WebRTC repository, or copy them from a local clone of the repository, and paste them into the current tutorial project. Add a reference to the App1.csproj project by right-clicking on the project in the Solution Explorer panel and selecting Add > Existing Item... (or using Shift+Alt+A), pointing the add dialog to the two newly copied files VideoBridge.cs and StreamSamplePool.cs . Editing the local video UI Double-click on the MainPage.xaml (or right-click > Open ) to bring up the XAML visual editor of the MainPage page. The page is currently blank, similar to what is displayed when launching the application. We will add a video player to it. The XAML visual editor allows editing the XAML user interface both via the visual top panel and via the XAML code in the bottom panel. Which approach is best is generally up to personal preference, although for discoverability it is easier to drag and drop controls from the Toolbox panel. In this tutorial however we make use of the MediaPlayerElement control which is not available from the toolbox. Edit the MainPage.xaml code from the XAML panel of the editor, and inside the <Grid> element add a <MediaPlayerElement> node: <Grid> <MediaPlayerElement x:Name=\"localVideoPlayerElement\" /> </Grid> This will create a media player which covers the entire surface of the application window. Bridging the track and its rendering The key link between a raw source of video frames and the Media Foundation pipeline is the MediaStreamSource class, which wraps an external video source to deliver raw frames directly to the media playback pipeline. Get back to the associated MainPage.xaml.cs and continue editing: At the top of the file, import the following extra modules: using TestAppUWP.Video; using Windows.Media.Core; using Windows.Media.Playback; using Windows.Media.MediaProperties; Create a new utility method CreateI420VideoStreamSource() which builds a MediaStreamSource instance to encaspulate a given video stream encoded in I420 format, which is the encoding in which WebRTC provides its raw video frames. This method will be reused later for the remote video too. private MediaStreamSource CreateI420VideoStreamSource( uint width, uint height, int framerate) { if (width == 0) { throw new ArgumentException(\"Invalid zero width for video.\", \"width\"); } if (height == 0) { throw new ArgumentException(\"Invalid zero height for video.\", \"height\"); } // Note: IYUV and I420 have same memory layout (though different FOURCC) // https://docs.microsoft.com/en-us/windows/desktop/medfound/video-subtype-guids var videoProperties = VideoEncodingProperties.CreateUncompressed( MediaEncodingSubtypes.Iyuv, width, height); var videoStreamDesc = new VideoStreamDescriptor(videoProperties); videoStreamDesc.EncodingProperties.FrameRate.Numerator = (uint)framerate; videoStreamDesc.EncodingProperties.FrameRate.Denominator = 1; // Bitrate in bits per second : framerate * frame pixel size * I420=12bpp videoStreamDesc.EncodingProperties.Bitrate = ((uint)framerate * width * height * 12); var videoStreamSource = new MediaStreamSource(videoStreamDesc); videoStreamSource.BufferTime = TimeSpan.Zero; videoStreamSource.SampleRequested += OnMediaStreamSourceRequested; videoStreamSource.IsLive = true; // Enables optimizations for live sources videoStreamSource.CanSeek = false; // Cannot seek live WebRTC video stream return videoStreamSource; } The CreateI420VideoStreamSource() method references the SampleRequested event, which is invoked by the Media Foundation playback pipeline when it needs a new frame. We use the VideoBridge helper class to serve those frames. At the top of the MainPage class, define two new variables: a MediaStreamSource for wrapping the local video stream and exposing it to the Media Foundation playback pipeline, and a VideoBridge for managing the delivery of the video frames. The video bridge is initialized with a queue capacity of 3 frames, which is generally enough for local video as it is not affected by network latency. private MediaStreamSource _localVideoSource; private VideoBridge _localVideoBridge = new VideoBridge(3); Implement the OnMediaStreamSourceRequested() callback using the video bridge. As we plan to reuse that callback for the remote video, the code finds the suitable video bridge based on the source which invoked the event. private void OnMediaStreamSourceRequested(MediaStreamSource sender, MediaStreamSourceSampleRequestedEventArgs args) { VideoBridge videoBridge; if (sender == _localVideoSource) videoBridge = _localVideoBridge; else return; videoBridge.TryServeVideoFrame(args); } In the OnLoaded() method where the local video track was created, subscribe to the I420AVideoFrameReady event. _localVideoTrack = await _peerConnection.AddLocalVideoTrackAsync(); _localVideoTrack.I420AVideoFrameReady += Peer_LocalI420AFrameReady; Implement the event handler by enqueueing the newly captured video frames into the bridge, which will later deliver them when the Media Foundation playback pipeline requests them. private void Peer_LocalI420AFrameReady(I420AVideoFrame frame) { _localVideoBridge.HandleIncomingVideoFrame(frame); } Starting the media playback The last part is to actually start the playback pipeline when video frames start to be received from WebRTC. This is done lazily for two reasons: to avoid starving the Media Foundation playback pipeline if the WebRTC local video track takes some time to start delivering frames, which it generally does compared to the expectation of the playback pipeline. to get access to the frame resolution, which is not otherwise available from WebRTC. Unfortunately at this time the capture framerate is not available, so we assume a framerate of 30 frames per second (FPS). At the top of the MainPage class, add a boolean field to indicate whether the local video is playing. This is protected by a lock, because the I420AVideoFrameReady and the SampleRequested events can be fired in parallel from multiple threads. private bool _localVideoPlaying = false; private object _localVideoLock = new object(); Modify the Peer_LocalI420AFrameReady() event handler to start the media player when the first WebRTC frame arrives. private void Peer_LocalI420FrameReady(I420AVideoFrame frame) { lock (_localVideoLock) { if (!_localVideoPlaying) { _localVideoPlaying = true; // Capture the resolution into local variable useable from the lambda below uint width = frame.width; uint height = frame.height; // Defer UI-related work to the main UI thread RunOnMainThread(() => { // Bridge the local video track with the local media player UI int framerate = 30; // for lack of an actual value _localVideoSource = CreateI420VideoStreamSource( width, height, framerate); var localVideoPlayer = new MediaPlayer(); localVideoPlayer.Source = MediaSource.CreateFromMediaStreamSource( _localVideoSource); localVideoPlayerElement.SetMediaPlayer(localVideoPlayer); localVideoPlayer.Play(); }); } } _localVideoBridge.HandleIncomingVideoFrame(frame); } Some of the work cannot be carried during the execution of this event handler, which is invoked from an unspecified worker thread, because access to XAML UI elements must be done exclusively on the main UI thread. Therefore we use a helper method which schedule this work for execution on that thread. Note The use of _localVideoSource from the OnMediaStreamSourceRequested() event handler is not protected by the _localVideoLock lock. This is because the event cannot be fired until well after the _localVideoSource has been assigned a new value, so there is no race condition concern here. And since _localVideoSource is not further modified, we avoid acquiring that lock in the OnMediaStreamSourceRequested() to reduce the chances of contention. The lock is actually not needed at all at this point, since _localVideoPlaying is also only modified in the current Peer_LocalI420FrameReady() event handler. But a typical application will provide some UI like a button to start and stop the local video, and therefore needs to synchronize access to _localVideoPlaying and _localVideoSource , at which point OnMediaStreamSourceRequested() will also need to acquire this lock. Implement the RunOnMainThread() helper using the Dispatcher of the current window. private void RunOnMainThread(Windows.UI.Core.DispatchedHandler handler) { if (Dispatcher.HasThreadAccess) { handler.Invoke(); } else { // Note: use a discard \"_\" to silence CS4014 warning _ = Dispatcher.RunAsync(Windows.UI.Core.CoreDispatcherPriority.Normal, handler); } } In the App_Suspending() event handler, clear the media player of the localVideoPlayerElement control so that it repaint itself and avoids keeping the last video frame when the video is turned off. localVideoPlayerElement.SetMediaPlayer(null); At this point the MediaPlayerElement control and the WebRTC local video track are connected together. Launch the application again; the local webcam starts capturing video frame, which are displayed in the main window of the application. Next : A custom signaling solution"
  },
  "manual/cs/cs-peerconnection.html": {
    "href": "manual/cs/cs-peerconnection.html",
    "title": "C# PeerConnection class | MixedReality-WebRTC Documentation",
    "keywords": "C# PeerConnection class The PeerConnection class is the entry point to using WebRTC. It encapsulates a connection between a local peer on the local physical device, and a remote peer on the same or, more generally, another physical device. Note The Unity integration also has a PeerConnection component, which is based on this class. Initialization A PeerConnection instance can be created with the default constructor, but initially most of its properties, methods, and events, cannot be used until it is initialized with a call to InitializeAsync() . Once intialized, the Initialized property returns true and it is safe to use the peer connection. InitializeAsync() takes a PeerConnectionConfiguration and a CancellationToken . The former allows configuring the peer connection about to be established, while the later allows cancelling that task while it is being processed. PeerConnectionConfiguration contains several fields, but the most important are: IceServers contains an optional collection of STUN and/or TURN servers used by the Interactive Connectivity Establishment (ICE) to establish a connection with the remote peer through routing devices (NATs). Without these, only direct connections to the remote peer can be established, which can be enough if the application knows in advance the network topology surrounding the two peers, but is generally recommended otherwise to increase the chances of establishing a connection over the Internet. SdpSemantic describes the semantic used by the Session Description Protocol (SDP) while trying to establish a connection. This is a compatibility feature, which allows connecting with older peers supporting only the deprecated Plan B semantic. New code should always use the default Unified Plan , which is the only one accepted by the WebRTC 1.0 standard. The other fields are for advanced use, and can be left with their default values. Once the connection is initialized, two categories of actions can be performed, in any order: Attempt to establish a connection with the remote peer. Add some media tracks and/or data channels. Media (audio and/or video) tracks and data channels added before the connection is established will be available immediately on connection. Other tracks and channels can be added later during the lifetime of the peer connection, while it remains open. Events A PeerConnection exposes several categories of events: Signaling events are related to establishing a connection to the remote peer. Track and channel events provide informational notifications about adding and removing media tracks and data channels. Frame events are high frequency per-frame callbacks invoked when and audio or video frame is available for local rendering. In general it is recommended to subscribe to these events before starting to establish a connection with the remote peer. Signaling events Event Status* Description Connected Recommended Invoked when the connection with the remote peer is established. This indicates that tracks and channel are ready for use, although ICE can continue its discovery in the background. LocalSdpReadytoSend Mandatory Invoked when the local peer finished crafting an SDP message, to request the user to send that message to the remote peer via its chosen signaling solution. IceCandidateReadytoSend Mandatory Invoked when the local peer finished crafting an ICE candidate message, to request the user to send that message to the remote peer via its chosen signaling solution. IceStateChanged Optional Invoked when the state of the local ICE connection changed. RenegotiationNeeded Recommended Invoked when the peer connection detected that the current session is obsolete, and needs to be renegotiated. This is generally the result of some media tracks or data channels being added or removed, and must be handled to make those changes available to the remote peer. However, it is perfectly acceptable to ignore some of those events, if several changes are expected in a short period of time, to avoid triggering multiple unnecessary renegotiations. However a renegotiation eventually needs to happen for the newly added tracks and channel to become open. * Status indicates the recommended subscription status for a working peer connection: Mandatory means those events must be handled, otherwise the connection cannot be established. Recommended means those events are typically handled, although not mandatory. Optional means those events are informational only, and it is entirely optional to subscribe to them. Track and channel events Event Status* Description TrackAdded Optional Invoked when a media track (audio or video) was added on the remote peer, and received on the local peer after a renegotiation. This only concerns remotely-created tracks. Tracks whose creation was initied locally with e.g. AddLocalVideoTrackAsync() do not generate a TrackAdded event. TrackRemoved Optional Invoked when a media track (audio or video) was removed on the remote peer, and a remove message was received on the local peer after a renegotiation. This only concerns remotely-deleted tracks. Tracks removed locally with e.g. RemoveLocalVideoTrack(LocalVideoTrack) do not generate a TrackRemoved event. DataChannelAdded Optional Invoked when a data channel was added to the local peer connection. This is always fired, irrelevant of how the data channel was initially created (in-band or out-of-band). DataChannelRemoved Optional Invoked when a data channel was removed from the local peer connection. This is always fired, irrelevant of how the data channel was initially created (in-band or out-of-band). * Status - See above. Frame events Event Status* Description LocalVideoTrack.I420AVideoFrameReady Optional (Note that this event was moved to LocalVideoTrack) Invoked when a video frame, encoded in I420 format, has been captured by a local video capture device and is ready to be rendered locally. This is optional, as some application will chose to only transmit local video frames to the remote peer without rendering them, whereas others like a video chat application will often provide a feedback to the user showing the video from a local webcam. I420ARemoteVideoFrameReady Optional Invoked when a video frame, encoded in I420 format, has been received from the remote peer and is ready to be rendered locally. This is optional, as the local peer does not have control on video tracks added by the remote peer, although most applications will genrally want to render those frames (or do any other kind of procesing on them). LocalVideoTrack.Argb32VideoFrameReady Optional (Note that this event was moved to LocalVideoTrack) Variant of I420AVideoFrameReady where the frame is encoded in raw ARGB 32-bits-per-pixel format. This generally requires an extra conversion from I420, performed internally. Argb32RemoteVideoFrameReady Optional Variant of I420ARemoteVideoFrameReady where the frame is encoded in raw ARGB 32-bits-per-pixel format. This generally requires an extra conversion from I420, performed internally. * Status - See above. Note It is generally recommended to use the I420 callbacks instead of the ARGB ones. Even in situations where the processing ( e.g. local rendering) requires ARGB frames, I420 frames are smaller thanks to the chroma downsampling, so faster to upload to GPU. And GPU-based I420-to-ARGB conversion via a custom pixel shader (fragment shader) is more efficient than the CPU conversion provided by the ARGB callbacks, even with the use of SIMD. See YUVFeedShaderUnlit.shader in the Unity integration for an example of such conversion shader."
  },
  "manual/unity-signaler.html": {
    "href": "manual/unity-signaler.html",
    "title": "Unity Signaler component | MixedReality-WebRTC Documentation",
    "keywords": "Unity Signaler component The Signaler Unity component is an abstract base class for implementing a custom component for a given signaling solution. The PeerConnection Unity component takes a reference to an intance of a Signaler Unity component to delegate handling its signaling message. Property Description PeerConnection A back reference to the PeerConnection Unity component that this signaler is attached to. This property is updated automatically after the peer connection is initialized. See PeerConnection.Signaler . OnConnect Event fired when the peer connection is established. Derived classes must invoke this event when appropriate. OnDisconnect Event fired when the peer connection is closed. Derived classes must invoke this event when appropriate. OnMessage Event fired when a signaling message is received. Derived classes must invoke this event when appropriate to deliver incoming messages to the PeerConnection . OnFailure Event fired when an error occurs inside the signaler. Derived classes may invoke this event when appropriate. Implementing a custom signaler A custom signaling solution needs to derive from the abstract base Signaler class so it can be used by the PeerConnection Unity component. Derived classes implementing a particular signaling solution must: invoke the OnConnect and OnDisconnect events to notify both the user and the peer connection of the state of signaling. invoke the OnMessage event and implement the SendMessageAsync method to respectively deliver incoming messages to the local peer and send outgoing message to the remote peer. Aditionally it is recommended that implementations also invoke the OnFailure event so that the user can be notified."
  },
  "manual/unity-remotevideosource.html": {
    "href": "manual/unity-remotevideosource.html",
    "title": "Unity RemoteVideoSource component | MixedReality-WebRTC Documentation",
    "keywords": "Unity RemoteVideoSource component The RemoteVideoSource Unity component represents a single video track received from the remote peer through an established peer connection. Property Description Video track PeerConnection Reference to the PeerConnection instance which contains the remote video track. AutoPlayOnAdded Automatically start playback of the video track when added. This corresponds to registering a video frame callback with the PeerConnection instance pointed by the RemoteVideoSource.PeerConnection property. Events VideoStreamStarted Event invoked when the remote video stream starts, after the track has been added to the peer connection. VideoStreamStopped Event invoked when the remote video stream stops, before the track is removed from the peer connection."
  },
  "manual/unity-remoteaudiosource.html": {
    "href": "manual/unity-remoteaudiosource.html",
    "title": "Unity RemoteAudioSource component | MixedReality-WebRTC Documentation",
    "keywords": "Unity RemoteAudioSource component The RemoteAudioSource Unity component represents a single audio track received from the remote peer through an established peer connection. Important FIXME: This component is not currently functional. The remote audio data is currently sent directly to the local audio out device by the internal WebRTC implementation without any configuration possible. See issue #92 for details. Property Description Audio track PeerConnection Reference to the PeerConnection instance which contains the remote audio track. AutoPlayOnAdded Automatically start playback of the audio track when added. This corresponds to registering an audio frame callback with the PeerConnection instance pointed by the RemoteAudioSource.PeerConnection property. AudioTrackAdded FIXME: This event is not currently fired. AudioTrackRemoved FIXME: This event is not currently fired. Events AudioStreamStarted FIXME: This event is not currently fired. AudioStreamStopped FIXME: This event is not currently fired."
  },
  "manual/unity-integration.html": {
    "href": "manual/unity-integration.html",
    "title": "Unity integration overview | MixedReality-WebRTC Documentation",
    "keywords": "Unity integration overview The Unity integration offers a simple way to add real-time communication to an existing Unity application. MixedReality-WebRTC provides a collection of Unity componenents ( MonoBehaviour -derived classes) which encapsulate objects from the underlying C# library , and allow in-editor configuration as well as establishing a connection to a remote peer both in standalone and in Play mode . The PeerConnection component is the entry point for configuring and establishing a peer-to-peer connection. The peer connection component makes use of a Signaler to handle the SDP messages dispatching until the direct peer-to-peer connection can be established. Audio and video tracks from a local audio(microphone) and video (webcam) capture device are handled by the LocalAudioSource and LocalVideoSource components, respectively. For remote tracks, similarly the RemoteAudioSource and RemoteVideoSource respectively handle configuring a remote audio and video track streamed from the remote peer. Rendering of both local and remote media tracks is handled by the MediaPlayer component, which connects to a video source and renders it using a custom shader into a Unity Texture2D object which is later applied on a mesh to be rendered in the scene. Warning Currently the remote audio stream is sent directly to the local audio out device, without any interaction with the MediaPlayer component, while the local audio stream is never rendered (heard) locally. This is due to the lack of audio callbacks, not yet implemented on the wrapped PeerConnection object from the underlying C# library."
  },
  "manual/unity-components.html": {
    "href": "manual/unity-components.html",
    "title": "Unity components | MixedReality-WebRTC Documentation",
    "keywords": "Unity components The Unity components provide some idiomatic wrapping over the C# MixedReality-WebRTC library. Component Description PeerConnection Encapsulates a single peer-to-peer connection to a remote peer Signaler Abstract base class of components which manage the signaling messages to allow the peer connection to establish NodeDssSignaler Simple testing / debugging Signaler implementation component based on node-dss VideoSource Component providing a hook to the local and remote video tracks of a peer connection VideoTrackPlayer Component bridging a Unity MeshRenderer with a video track from a VideoSource component Components by feature area Connection The most important component is Microsoft.MixedReality.WebRTC.Unity.PeerConnection which encapsulate the connection to a single remote peer. The peer connection works in coordination with a Signaler component, for example Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler , which handles the message transport for the session establishment. Video The entry point for video tracks is the VideoSource component, which is associated with a given PeerConnection and handles its video-related signals. The video source in turn makes use of one or two VideoTrackPlayer to render the content of the local and remote video tracks if needed, although this is optional. Video frames are provided by the underlying WebRTC.PeerConnection to the VideoSource via its frame events. The video source then fills a VideoFrameQueue for each video track player associated with it. The player use that shared queue to read back the frame and display it using a custom shader. Audio Todo... List of components Microsoft.MixedReality.WebRTC.Unity.PeerConnection This component abstracts a WebRTC peer connection and encapsulates the lower level Microsoft.MixedReality.WebRTC.PeerConnection object from the C# library. This is the main entry point for establishing a connection. It contains the list of ICE servers ( Interactive Connectivity Establishment ) used to punch through NAT, as well as some key events like OnInitialized and OnShutdown which mark the beginning and end of the connection from the point of view of the user. Microsoft.MixedReality.WebRTC.Unity.Signaler This abstract base component is used by the peer connection to establish a connection with a remote peer. The peer connection needs one concrete implementation derived from this class to be specified in its Signaler property. Microsoft.MixedReality.WebRTC.Unity.NodeDssSignaler THIS SHOULD NOT BE USED FOR PRODUCTION. This components is used for debugging and testing as a concrete implementation of a Signaler component. It is based on the node-dss protocol and NodeJS service. It is very simple and helps developers starting, but lacks many features. Microsoft.MixedReality.WebRTC.Unity.VideoSource This component controls sending the local video track through the peer connection, and handles receiving a remote video track. For the local video, it controls whether or not a track is added to the WebRTC stream, and optionally provides a VideoTrackPlayer with frames to render that local video. For the remote video, it controls whether or not to handle the received feed and send it to a VideoTrackPlayer for rendering. Note that the local peer cannot control whether or not the remote peer sends a remote track; it can only ignore it if not interested (and ideally should probably tell the remote peer that it should stop sending it, although this is application specific logic). Microsoft.MixedReality.WebRTC.Unity.VideoTrackPlayer This components bridges a raw video frame feed from a VideoSource to a Unity MeshRenderer for display. The component can limit the framerate of the video playback, and optionally display some statistics about it. The associated MeshRenderer on the same GameObject typically uses a YUVFeedMaterial to display the YUV-encoded feed uploaded to the main texture of that material by the video track player component."
  },
  "manual/introduction.html": {
    "href": "manual/introduction.html",
    "title": "Introduction | MixedReality-WebRTC Documentation",
    "keywords": "Introduction The MixedReality-WebRTC project is a collection of components to help mixed reality app developers to integrate peer-to-peer audio, video, and data real-time communication into their application and improve their collaborative experience. These components are based on the WebRTC protocol for Real-Time Communication (RTC), which is supported by most modern web browsers. Project scope The MixedReality-WebRTC project focuses on features which actively contribute to enhance collaborative experiences in mixed reality apps. Although the WebRTC technology can be used for many different video and audio streaming application -- and in fact the MixedReality-WebRTC project itself can be used in any application, even non-mixed-reality ones -- the project scope limits the features the maintaining team will spend its development resources on. This does not mean that other features are not welcome; as an open-source project, we welcome any contribution. However we will dedicate more of our own resources to those contributions within the project scope."
  },
  "manual/installation.html": {
    "href": "manual/installation.html",
    "title": "Installation | MixedReality-WebRTC Documentation",
    "keywords": "Installation C# library The C# library is consumed as a NuGet package by adding a dependency to that package in your C# project. In Visual Studio 2019: Right-click on the C# project > Manage NuGet Packages... to open the NuGet package manager window. In the search bar, type \"Microsoft.MixedReality.WebRTC\". You may need to check the Include prerelease option. Select the NuGet package to install: For a C# Desktop project, choose Microsoft.MixedReality.WebRTC . For a C# UWP project, choose Microsoft.MixedReality.WebRTC.UWP . In the right panel, choose a version and click the Install button. This will add a dependency to the currently selected C# project. If multiple projects are using the MixedReality-WebRTC library, this process must be repeated for each project. C++ library The C++ library is consumed as a NuGet package by adding a dependency to that package in your C++ project. The C++ library is often referred to as the native library. In Visual Studio 2019: Right-click on the C++ project > Manage NuGet Packages... to open the NuGet package manager window. Select the Browse tab. In the search bar, type \"Microsoft.MixedReality.WebRTC.Native\". You may need to check the Include prerelease option. Select the NuGet package to install: For a C++ Desktop project, choose Microsoft.MixedReality.WebRTC.Native.Desktop . For a C++ UWP project, choose Microsoft.MixedReality.WebRTC.Native.UWP . In the right panel, choose a version and click the Install button. This will add a dependency to the currently selected C++ project. If multiple projects are using the MixedReality-WebRTC library, this process must be repeated for each project. Unity integration The Unity integration currently does not provide any automated installation process. Instead, users need to copy the relevant files into their Unity project. Copying the sources Close Unity . Do not proceed further while the target project is open inside Unity. Clone the MixedReality-WebRTC project from GitHub using git clone https://github.com/microsoft/MixedReality-WebRTC.git . There is no need in this case to do a recursive clone, since we are not building anything. Alternatively, you can download a ZIP archive of the source code from any of the releases on the release page . Copy from libs/Microsoft.MixedReality.WebRTC.Unity/Assets/ the following files into the Assets/ folder of the Unity project you want to import the MixedReality-WebRTC library in: The Microsoft.MixedReality.WebRTC.Unity folder, which contains the main Unity scripts for the integration. The Microsoft.MixedReality.WebRTC.Unity.Editor folder, which contains some helper scripts for the Editor integration. The Plugins folder, which contains all the variants of the underlying native library used by the C# library, which in turns is used by the Unity integration. At this point, do not open Unity yet . The native library DLLs are not checked in the repository, but the .meta files which configure them are. If Unity detects that those .meta files do not have a corresponding .dll file it will delete the .meta file, and later recreate some with the wrong configuration . This will result in errors at runtime. Instead, we need first to copy the native library DLLs into the Plugins/ folder. Note If you opened Unity already by mistake, you can close it and revert its changes with git reset , or restore the .meta files from the ZIP archive. For more information about the .meta files and the per-platform configuration of the native library, see Importing MixedReality-WebRTC in the Unity tutorial. Note The repository contains some .meta files for the debug symbols databases ( .pdb ). The PDB are only necessary for debugging, thus the .pdb.meta files can be deleted. The .dll.meta files however need to be carefully saveguarded as explained above. Manually download the C# NuGet packages Download the NuGet package for the C# library from nuget.org by navigating to the package page using the links below, and selecting the Download package link in the right navigation panel. Microsoft.MixedReality.WebRTC Microsoft.MixedReality.WebRTC.UWP Once downloaded, rename them by changing the .nuget extension to .zip , and extract the archives in some temporary folder of your choice. Copying the native DLLs Copy all the Microsoft.MixedReality.WebRTC.Native.dll variants as indicated in the table below. Source Folder Destination Folder From Microsoft.MixedReality.WebRTC - runtimes/win10-x86/native Assets/Plugins/Win32/x86 - runtimes/win10-x64/native Assets/Plugins/Win32/x86_64 From Microsoft.MixedReality.WebRTC.UWP - runtimes/win10-x86/native Assets/Plugins/WSA/x86 - runtimes/win10-x64/native Assets/Plugins/WSA/x86_64 - runtimes/win10-arm/native Assets/Plugins/WSA/ARM Copying the C# library The Unity integration also references the C# library. Copy the Microsoft.MixedReality.WebRTC.dll assembly from either the Desktop package Microsoft.MixedReality.WebRTC.nupkg or the UWP package Microsoft.MixedReality.WebRTC.UWP.nupkg ; both packages have the same copy of the C# assembly, and only differ by the associated native DLLs. Source: <package_root>/lib/netstandard2.0/Microsoft.MixedReality.WebRTC.dll Destination: <unity_project>/Assets/Plugins/Win32/x86_64/ In theory the managed assembly could be copied anywhere in the Assets/Plugins/ folder of the target Unity project. Practice however shows that Unity tends to stop its DLL search in the first folder that contains at least one DLL, instead of continuing to recurse into the parent folders. For this reason, we arbitrarily choose the Assets/Plugins/Win32/x86_64/ as the destination for the C# assembly. Summary The above steps should result in the following hierarchy on disk: MyAwesomeUnityProject/Assets/Plugins/ - Win32.meta + Win32/ - x86.meta + x86/ - Microsoft.MixedReality.WebRTC.Native.dll - Microsoft.MixedReality.WebRTC.Native.dll.meta - x86_64.meta + x86_64/ - Microsoft.MixedReality.WebRTC.Native.dll - Microsoft.MixedReality.WebRTC.Native.dll.meta - Microsoft.MixedReality.WebRTC.dll - Microsoft.MixedReality.WebRTC.dll.meta - WSA.meta + WSA/ - x86.meta + x86/ - Microsoft.MixedReality.WebRTC.Native.dll - Microsoft.MixedReality.WebRTC.Native.dll.meta - x86_64.meta + x86_64/ - Microsoft.MixedReality.WebRTC.Native.dll - Microsoft.MixedReality.WebRTC.Native.dll.meta - ARM.meta + ARM/ - Microsoft.MixedReality.WebRTC.Native.dll - Microsoft.MixedReality.WebRTC.Native.dll.meta"
  },
  "manual/helloworld-unity-remotevideo.html": {
    "href": "manual/helloworld-unity-remotevideo.html",
    "title": "Adding remote video | MixedReality-WebRTC Documentation",
    "keywords": "Adding remote video Unlike local video, the remote video track is controlled by the remote peer who will decide if and when to add the track to the connection. On the receiver side, the only thing to decide is whether or not to handle that track. The RemoteVideoSource Unity component is used to expose a remote video track. And similarly to the LocalVideoSource , it can be added to a MediaPlayer to render the content of the video feed. Adding a remote video source Like we did for the local video feed, we create a new game object with a RemoteVideoSource component: In the Hierarchy window, select Create > Create Empty . Rename it to something memorable like \"RemoteMediaPlayer\". Go to the Transform component and increase the scale to (5,5,1) . In the Inspector window, press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > RemoteVideoSource . Like the local video source, this component needs to know which peer connection to use. Once again, use the asset selection window to assign our peer connection to the Peer Connection property. We note immediately that the remote video source is more simple than the local video source. Aside from the PeerConnection property we already assigned, the only other property is the Auto Play On Added boolean. This property instructs the component to immediately start playing back the video feed when the remote track is added to the connection and received locally. If this property is false , then the user needs to manually call Play() to start listening to incoming remote video frames. Adding a remote media player This is again similar to the local video source: the remote video source only exposes a video frame queue which gets populated using the video frames coming from the remote peer, but the component does not do any rendering by itself. Instead we can use again a MediaPlayer to render those frames. So let's create a new game object for it: Add a Mesh Filter component, a Mesh Renderer component, and a Media Player component. In the Mesh Filter component, set the Mesh property to the built-in Unity Quad mesh. In the Mesh Renderer component, expand the Materials array and set the first material Element 0 to the YUVFeedMaterial material located in the Assets/Microsoft.MixedReality.WebRTC.Unity/Materials folder. In the Media Player component, set the Video Source property to the remote video source component previously created. The Inspector window should now look like this: At this point we need to fix a small issue : both the local and remote media player objects are at the same location. Change the Position of the local one to move it slightly to the left at (-3,0,0) . Change the Position of the remote one to move it slightly to the right at (3,0,0) . After that we should be able to see in the Game window our two pink squares representing the local and remote videos:"
  },
  "manual/helloworld-unity-peerconnection.html": {
    "href": "manual/helloworld-unity-peerconnection.html",
    "title": "Creating a peer connection | MixedReality-WebRTC Documentation",
    "keywords": "Creating a peer connection From this point we start building the scene. Because the MixedReality-WebRTC components are installed, and because we work now almost exclusively inside Unity, for brievety we will use the term component to designate a Unity component, that is a class deriving from MonoBehaviour . Create a new GameObject with a PeerConnection component: In the Hierarchy window , select Create > Create Empty to add a new GameObject to the scene. In the Inspector window , select Add Component > MixedReality-WebRTC > PeerConnection to add a PeerConnection component to that new object. At the top of the Inspector window, rename the newly-created game object to something memorable like \"MyPeerConnection\". You can also rename this object in the Hierarchy window directly (for example by pressing F2 when selected). The PeerConnection component provided by the Unity integration of MixedReality-WebRTC has various settings to configure its behaviour. For the moment you can leave the default values. The component has one required property however, the PeerConnection.Signaler property, which it uses to establish a connection. This property must point to another component deriving from the base Microsoft.MixedReality.WebRTC.Unity.Signaler component. For the moment it points to nothing, so Unity shows \"None\"."
  },
  "manual/helloworld-unity-localvideo.html": {
    "href": "manual/helloworld-unity-localvideo.html",
    "title": "Adding local video | MixedReality-WebRTC Documentation",
    "keywords": "Adding local video There are two different aspects covered by the concept of local video : Capturing some video feed from a local camera to send it to the remote peer Displaying locally the captured video feed Both are optional, although the second one alone simply corresponds to capturing a displaying a local webcam and doesn't require WebRTC. So we generally want the first one in a scenario where an application needs WebRTC. The second one is application-dependent, and even within one given application can be toggled ON and OFF by the user. Both cases however are covered by the LocalVideoSource component. This components serves as a bridge between a local video capture device (camera), the peer connection, and an optional video player to render the video feed locally. Adding a local video source For clarity we will create a new game object and add a LocalVideoSource component. It may sound superfluous at the moment to create a new game object, as we could add the local video source to the same game object already owning the peer connection component, but this will prove more clear and easy to manipulate later. In the Hierarchy window, select Create > Create Empty . In the Inspector window, rename the newly-created game object to something memorable like \"LocalMediaPlayer\". Press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > LocalVideoSource . This component needs to know which peer connection to use. Once again, use the asset selection window to assign our peer connection to the Peer Connection property. The local video source component contains several interesting properties: The Auto Start Capture property instructs the component to open the video capture device (webcam) automatically as soon as possible. This enables starting local video playback even before the peer connection is established. The Enable Mixed Reality Capture property tells the component it should attempt to open the video capture device with MRC enabled, if supported. The Auto Add Track property allows automatically adding a video track to the peer connection and start sending the video feed to the remote peer once the connection is established. If not checked, the user has to manually call some method to add that track. These are good defaults values to start, and we will leave them as is. Adding a media player We said before that the LocalVideoSource component covers both sending the video feed to the remote peer and displaying it locally. This is partially incorrect. The local video source plugs into the peer connection and the video capture device, and exposes some C# event to access the video frames produced by that video device. But it does not do any rendering itself. In order to render the video frames of the local video capture device, MixedReality-WebRTC offers a simple MediaPlayer component which uses a Unity Texture2D object and renders the video frames to it. This texture is then applied to the material of a Renderer component to be displayed in Unity on a mesh. Let's add a MediaPlayer component on our game object: In the Inspector window, press the Add Component button at the bottom of the window, and select MixedReality-WebRTC > MediaPlayer This time however Unity will not create the component, and instead display a somewhat complex error message: What the message means is that the MediaPlayer component requires a Renderer component on the same game object, and Unity lists all possible implementation of a renderer (all classes deriving from Renderer ). Although all renderers might work, in our case the most simple is to add a MeshRenderer component. If you are familiar with Unity, you also know that the renderer needs a source mesh in the form of a MeshFilter component. So for each component, in the Inspector window, press the Add Component button at the bottom of the window, and select successively and in order: Mesh > MeshFilter Mesh > MeshRenderer MixedReality-WebRTC > MediaPlayer After that, set the component properties as follow: In the Mesh Filter component, set the Mesh property to the built-in Unity Quad mesh. This is a simple square mesh on which the texture containing the video feed will be applied. The built-in Quad mesh size is quite small for rendering a video, so go to the Transform component and increase the scale to (5,5,1) . In the Mesh Renderer component, expand the Materials array and set the first material Element 0 to the YUVFeedMaterial material located in the Assets/Microsoft.MixedReality.WebRTC.Unity/Materials folder. This instructs Unity to use that special material and its associated shader to render the video texture on the quad mesh. More on that later. In the Media Player component, set the Video Source property to the local video source component previously added to the same game object. This instructs the media player to connect to the local video source for retrieving the video frames that it will copy to the video texture for rendering. This should result in a setup looking like this: And the Game view should display a pink square, which materializes the quad mesh: A word on the YUVFeedMaterial material here. The video frames coming from the local video source are encoded using the I420 format. Unity on the other hand, and more specifically the GPU it abstracts, generally don't support directly rendering I420-encoded textures. So the YUVFeedMaterial material is using a custom shader called YUVFeedShader (Unlit) to load the I420-encoded video frame from the video texture, and convert it to ARGB on the fly before rendering the quad. This GPU-based conversion is very efficient and avoids any software processing on the CPU before uploading the video texture to the GPU. This is how LocalVideoSource is able to directly copy the I420-encoded video frames coming from the WebRTC core implementation into a texture without further processing, and MediaPlayer is able to render them on a quad mesh. Test the local video At this point the local audio source and the media player are configured to open the local video capture device (webcam) of the local machine the Unity Editor is running on, and display the video feed to that quad mesh in the scene. Press the Play button in the Unity Editor. After a few seconds (depending on the device) the video should appear over the quad mesh."
  },
  "manual/helloworld-unity-importwebrtc.html": {
    "href": "manual/helloworld-unity-importwebrtc.html",
    "title": "Importing MixedReality-WebRTC | MixedReality-WebRTC Documentation",
    "keywords": "Importing MixedReality-WebRTC In order to use the Unity integration, the following components are required: C++ library : Microsoft.MixedReality.WebRTC.Native.dll (one variant per platform/architecture) C# library : Microsoft.MixedReality.WebRTC.dll (single universal module for all platforms/architectures) Unity integration scripts and assets There is currently no pre-packaged distribution method for Unity, so users have to manually copy the relevant files into their Unity project. Copying the libraries The libraries can be copied either from a local build of the solution, of from the NuGet packages. The latter does not require any building, but is generally only available for the latest stable release, so will be missing any newer development available on the master branch. The former allows accessing the latest features, but may contain some API breaking changes since the latest stable release, so may require some code changes when upgrading an existing project using an earlier API version. NuGet packages The C++ and C# libraries of MixedReality-WebRTC are available precompiled via NuGet packages. See the GitHub Releases page for the latest packages. The packages can be downloaded from nuget.org, and once downloaded can be extracted by simply changing their extension from .nupkg to .zip and using any standard ZIP archive extraction method. Once extracted, the DLLs can be copied as detailed on the Unity integration section of the installation page . Local solution build If the C++ and C# libraries are compiled from sources as explained in the Building page, they are available in one of the sub-folders under the <root>/bin/ folder of the MixedReality-WebRTC project. Note that the copy is automatically done by a build script when compiling via the provided Visual Studio solution . The steps below are only required if the libraries are compiled in another way. Otherwise you can skip to the next step to Configure the import settings . The C# library Microsoft.MixedReality.WebRTC.dll is a .NET Standard 2.0 library. This means it is compatible with all CPU architectures. This is often referred to as \"AnyCPU\", and the C# library is therefore available from bin\\AnyCPU\\Debug or bin\\AnyCPU\\Release depending on the build configuration which was compiled. In doubt you can use the Release configuration, which provides better performance. This module needs to be copied somewhere into the Assets\\Plugins\\ folder of the Unity project (if that folder doesn't exist you can create it). On Windows this can be done via the command line with xcopy , assuming that the MixedReality-WebRTC project is located in D:\\mr-webrtc : cd /D D:\\testproj xcopy D:/mr-webrtc/bin/AnyCPU/Release/Microsoft.MixedReality.WebRTC.dll Assets/Plugins/ For the C++ library Microsoft.MixedReality.WebRTC.Native.dll things are a bit more complex. The C++ code is compiled for a particular platform and architecture, in addition of the Debug or Release build config, and the correct variant needs to be used. On Windows, the Unity Editor needs a 64-bit Desktop variant; it is available from the bin\\Win32\\x64\\Release folder, and should be copied to the Assets\\Plugins\\Win32\\x86_64\\ folder. cd /D D:\\testproj xcopy D:/mr-webrtc/bin/Win32/x64/Release/Microsoft.MixedReality.WebRTC.Native.dll Assets/Plugins/Win32/x86_64/ Configuring the import settings When building the Unity application for a given platform, another variant may be required. In order for the C# library to be truly platform-independent, the name of all C++ library variants is the same. This allows the C# code to reference the C++ library with the same DllImport attribute path . But this also means that Unity needs to know which copy is associated with which build variant, to be able to deploy the correct one. This is done by configuring the platform associated with a DLL in the import settings in the Unity inspector: By selecting: Any Platform except WSAPlayer , the DLL will be used by Unity on all platforms except when deploying for UWP. WSAPlayer is the name Unity uses for its UWP standalone player. CPU equal to x86_64 , Unity will only deploy that DLL when deploying on a 64-bit Intel architecture. This way, multiple variants of the same-named Microsoft.MixedReality.WebRTC.Native.dll can co-exist in different sub-folders of Assets/Plugins/ and Unity will deploy and use the correct variant on each platform. For Windows Desktop , the C++ library variants are: Path Any Platform Exclude Platforms CPU OS Example use Assets/Plugins/Win32/x86 yes -WSAPlayer x86 Windows 32-bit Windows Desktop application Assets/Plugins/Win32/x86_64 yes -WSAPlayer x86_64 Windows 64-bit Windows Desktop application, including the Unity Editor on Windows For Windows UWP , the C++ library variants are: Path Any Platform Include Platforms SDK CPU Example use Assets/Plugins/UWP/x86 no +WSAPlayer UWP X86 Microsoft HoloLens Assets/Plugins/UWP/x86_64 no +WSAPlayer UWP X64 64-bit UWP Desktop app on Windows Assets/Plugins/UWP/ARM no +WSAPlayer UWP ARM HoloLens 2 (compatibility) Assets/Plugins/UWP/ARM64 no +WSAPlayer UWP ARM64* HoloLens 2 *ARM64 is only available on Unity 2019.1+ If all variants are installed, the resulting hierarchy should look like this: Assets +- Plugins +- Win32 | +- x86 | | +- Microsoft.MixedReality.WebRTC.Native.dll | +- x86_64 | +- Microsoft.MixedReality.WebRTC.Native.dll +- UWP +- x86 | +- Microsoft.MixedReality.WebRTC.Native.dll +- x86_64 | +- Microsoft.MixedReality.WebRTC.Native.dll +- ARM | +- Microsoft.MixedReality.WebRTC.Native.dll +- ARM64 +- Microsoft.MixedReality.WebRTC.Native.dll Importing the Unity integration In order to import the Unity integration into your new Unity project, simply copy the libs\\Microsoft.MixedReality.WebRTC.Unity\\Assets\\Microsoft.MixedReality.WebRTC.Unity and libs\\Microsoft.MixedReality.WebRTC.Unity\\Assets\\Microsoft.MixedReality.WebRTC.Unity.Editor folders into the Assets folder of your project. The former provides the integration itself, while the later contains some helpers for the Unity Editor. Those helpers are only needed in the Editor, and not when the application is deployed at runtime. After Unity finished processing the new files, the Project window should look like this:"
  },
  "manual/helloworld-unity-createproject.html": {
    "href": "manual/helloworld-unity-createproject.html",
    "title": "Creating a new Unity project | MixedReality-WebRTC Documentation",
    "keywords": "Creating a new Unity project Go to the Unity archive page and download the latest 2018.3.x version or 2019.1.x version. Other versions might work, but are not officially supported. Create a new Unity project by selecting the 3D Template and choosing a project name and location. Unity will create several folders at that location, the most important of which is the Assets folder. This contains all source files for the project, and this is where the MixedReality-WebRTC Unity integration will be installed."
  },
  "manual/helloworld-unity-connection.html": {
    "href": "manual/helloworld-unity-connection.html",
    "title": "Establishing a connection | MixedReality-WebRTC Documentation",
    "keywords": "Establishing a connection Now that we have a both local and remote video players, we can attempt to establish a connection with a remote peer. There are a few requirements for this: We need 2 instances of the application running at the same time. Unfortunately the Unity Editor cannot be opened twice with the same project. As a workaround, we can build and deploy the app on a device, even locally on the developer machine. Alternatively, we can use a second computer running another instance of the Unity Editor with an exact copy of this Unity project. The later is easier because we can still modify the project. The NodeDssSignaler component needs to be configured to know which remote peer to expect. This is due to the fact that this is a simple, easy, and not production-ready solution which does not offer any functionality to discover and select a remote peer. Instead it uses strings to identify the two peers. We can chose any two different strings. Warning When deploying to multiple devices, remember to change the IP address of the node-dss server to the IP address of the host machine in the NodeDssSignaler component, instead of the default 127.0.0.1 . Configuring the NodeDssSignaler The NodeDssSignaler has a Remote Peer Id property which contains the string identifying the remote peer to connect with. This should be filled with the identifier of the remote peer. The easiest way to obtain this identifier is to press Play and wait for the local signaler to start polling our node-dss server. If the server was started with the DEBUG=*dss environment variable set, it will output for each web request a message containing the identifier of the peer. Download and install Node.js from the official website . Clone the node-dss repository : git clone https://github.com/bengreenier/node-dss.git Configure and run it: cd node-dss set DEBUG=dss* npm install npm start The node-dss server should start in a new shell window and wait for incoming connections. At this point we can press Play in the Unity Editor to start polling the node-dss server, and retrieve from the shell terminal its identifier string: This string needs to be pasted into the Remote Peer Id property of the remote peer on the other machine. Repeat the process on the remote machine and paste the result on the Remote Peer Id property of the local machine. Warning This step is critical, and there is no safeguard. If any of the two signalers doesn't have the correct value for the identifier of the remote peer then the peer connection will not be established. Starting the WebRTC connection Now that both peers are connected to the node-dss signaling server and can exchange some SDP messages, it is time to start an actual WebRTC connection. The VideoChatDemo sample contains an example of creating a button and using the NodeDssSignalerUI.cs script to do that, but the task essentially boils down to one of the two peers, and one only, calling PeerConnection.CreateOffer() . Once CreateOffer() is called, the local peer will asynchronously generate a new SDP offer message, and invoke the LocalSdpReadyToSend event to let the signaler send it. At this point the message should appear in the node-dss logs, which will buffer it until the remote peer polls for new data. After delivering the message, the remote peer will process it, and generally create an SDP answer message, which will travel back to the local peer via the node-dss server. At this point if everything went right the WebRTC connection is established, and the audio and video tracks should start sending and receiving data, and the data channels become open to do the same with the application's data."
  },
  "manual/gettingstarted.html": {
    "href": "manual/gettingstarted.html",
    "title": "Getting started | MixedReality-WebRTC Documentation",
    "keywords": "Getting started The MixedReality-WebRTC project is comprised of several components: A C++ library to integrate into a native C++ application A C# library to integrate into a C# application A Unity integration to help integrate into an existing Unity application Not all components are required for all use cases, but each component builds upon the previous one. This means that for use in a C++ application only the C++ library needs to be installed. But the Unity integration will require installing also the C# and C++ libraries. Note A note on terminology: in this documentation the term component refers to one of the libraries mentioned above. This has no relation with a Unity component , which is a C# class deriving from MonoBehaviour . In this chapter we discuss: Download : Downloading precompiled binary packages of MixedReality-WebRTC. Installation : How to install the various libraries for use in your poject. Building from sources : Building the MixedReality-WebRTC from sources. Hello, C# world! (Desktop) : Your first C# .NET Core 3.0 application based on the C# library. Hello, C# world! (UWP) (TODO) : Your first C# UWP application based on the C# library. Hello, Unity world! : Your first Unity application based on the Unity integration."
  },
  "manual/download.html": {
    "href": "manual/download.html",
    "title": "Downloading MixedReality-WebRTC | MixedReality-WebRTC Documentation",
    "keywords": "Downloading MixedReality-WebRTC MixedReality-WebRTC is primarily distributed as NuGet packages hosted on nuget.org . Those packages are signed by Microsoft. Generally you do not want to download those packages directly, but instead add a reference inside a Visual Studio project and let Visual Studio do the installation. See Installation for details. Alternatively, the libraries can be compiled from source if wanted. See Building from sources for details on this process. C# library The C# library is distributed as two separate packages for Windows Desktop and Windows UWP: Windows Desktop package Microsoft.MixedReality.WebRTC Windows UWP package Microsoft.MixedReality.WebRTC.UWP Note As per existing C# NuGet packages convention, and unlike the C++ library below, the Desktop package has no suffix, and the UWP package adds a .UWP suffix. The C# library packages contain the C# assembly Microsoft.MixedReality.WebRTC as well as the per-architecture native DLLs. Therefore those packages are standalone, and there is no need to also reference the C++ library packages in your project. C++ library The C++ library is distributed as two separate packages for Windows Desktop and Windows UWP: Windows Desktop package Microsoft.MixedReality.WebRTC.Native.Desktop Windows UWP package Microsoft.MixedReality.WebRTC.Native.UWP The C++ packages contain the shared library Microsoft.MixedReality.WebRTC.Native.dll as well as its import library ( .lib ). Note Unlike for the C# library above, the C++ library packages are named explicitly according to the target platform, adding either a .Desktop or .UWP suffix to the package name. Unity integration The Unity integration is not currently distributed in any particular packaged way. Instead, users can check out the GitHub repository and copy the relevant parts of the Unity sample project from libs/Microsoft.MixedReality.WebRTC.Unity/ . See Installation for details."
  },
  "manual/building-core.html": {
    "href": "manual/building-core.html",
    "title": "Building the Core dependencies from sources | MixedReality-WebRTC Documentation",
    "keywords": "Building the Core dependencies from sources This document describes how to build the entire project from sources, including the so-called Core dependencies: The low-level WebRTC C++ implementation webrtc.lib from Google. The UWP WinRT wrapper Org.WebRtc.winmd from the Microsoft WebRTC UWP team. The dependencies require some heavy setup and take time to compile, therefore it is strongly recommended to use the prebuilt binaries shipped as NuGet packages instead of trying to build those from source. Windows Desktop These packages contain webrtc.lib built for Windows Desktop ( Win32 ) for a given architecture and build config. Microsoft.MixedReality.WebRTC.Native.Core.Desktop.x86.Debug Microsoft.MixedReality.WebRTC.Native.Core.Desktop.x86.Release Microsoft.MixedReality.WebRTC.Native.Core.Desktop.x64.Debug Microsoft.MixedReality.WebRTC.Native.Core.Desktop.x64.Release Windows UWP These packages contain webrtc.lib built for Windows UWP for a given architecture and build config. Microsoft.MixedReality.WebRTC.Native.Core.UWP.x86.Debug Microsoft.MixedReality.WebRTC.Native.Core.UWP.x86.Release Microsoft.MixedReality.WebRTC.Native.Core.UWP.x64.Debug Microsoft.MixedReality.WebRTC.Native.Core.UWP.x64.Release Microsoft.MixedReality.WebRTC.Native.Core.UWP.ARM.Debug Microsoft.MixedReality.WebRTC.Native.Core.UWP.ARM.Release In addition, the Microsoft.MixedReality.WebRTC.Native.Core.UWP package (which should have been named .WinRT for consistency) references all the architecture-dependent ones for UWP for convenience, and also contains the platform-independent generated C++/WinRT headers necessary to consume the libraries. WinRT binding These packages contain the WinRT binding ( Org.WebRtc.dll , Org.WebRtc.winmd , Org.WebRtc.WrapperGlue.lib ) built for Windows UWP for a given architecture and build config. Microsoft.MixedReality.WebRTC.Native.Core.WinRT.x86.Debug Microsoft.MixedReality.WebRTC.Native.Core.WinRT.x86.Release Microsoft.MixedReality.WebRTC.Native.Core.WinRT.x64.Debug Microsoft.MixedReality.WebRTC.Native.Core.WinRT.x64.Release Microsoft.MixedReality.WebRTC.Native.Core.WinRT.ARM.Debug Microsoft.MixedReality.WebRTC.Native.Core.WinRT.ARM.Release In general most users will want to follow the steps in the readme instead of the ones below if there is no need to modify the input dependencies. Prerequisites General Python 2.7 must be installed as the default interpreter. That is, python --version must return a Python version equal to 2.7. It is strongly recommended to get a patch version >= 15, that is Python 2.7.15+ , as some users reported to the WebRTC UWP team some spurious failures with earlier versions. Python 3.x does not work and should not be the default interpreter. A recent version of Perl is needed for some builds. On Windows you can install for example Strawberry Perl , or any other equivalent distribution you want. Core WebRTC Core WebRTC refers to the C++ implementation of WebRTC maintained by Google and used by this project, whose source repository is https://webrtc.googlesource.com/src . Visual Studio 2017 is required to compile the core WebRTC implementation from Google. Having the MSVC v141 toolchain installed inside another version of Visual Studio is unfortunately not enough (see this issue ), the actual IDE needs to be installed for the detection script to work. Selecting the C++ Workload alone is enough. If compiling for ARM or ARM64 architecture though, check the Visual C++ compilers and libraries for ARM(64) optional individual component. The Windows SDK 10.0.17134 (also called 1803, or April 2018) is required to compile the Google WebRTC core implementation ( archive download ). As mentioned on the README of WebRTC UWP, the Debugging Tools for Windows are required: When installing the SDK, include the feature Debugging Tools for Windows which is required by the preparation scripts. Note that the SDK installed as part of Visual Studio does not include this feature. If the SDK is already installed, this optional feature can be added with Add or Remove Programs > Windows Software Development Kit - Windows 10.0.x > Modify > Select Change then Next button > Check Debugging Tools for Windows . Core WebRTC UWP wrappers The UWP wrappers refer to the set of wrappers and other UWP-specific additional code made available by the WebRTC UWP team (Microsoft) on top of the core implementation, to allow access to the core WebRTC API. The Windows SDK 10.0.17763 (also called 1809, or October 2018) is required to compile the UWP wrappers provided by the WebRTC UWP team ( archive download ), with the Debugging Tools for Windows as above. The UWP wrappers also require the v141 platform toolset for UWP, either from the Universal Windows Platform development workload in Visual Studio 2017, or from the optional component C++ (v141) Universal Windows Platform tools in Visual Studio 2019 . The UWP wrappers use C++/WinRT, so the C++/WinRT Visual Studio extension must be installed from the marketplace. MixedReality-WebRTC C++ library The MSVC v142 - VS 2019 C++ x64/x86 build tools toolchain is required to build the C++17 library of MixedReality-WebRTC. This is installed by default with the Desktop development with C++ workload on Visual Studio 2019. Note - Currently due to CI limitations some projects are downgraded to VS 2017, but will be reverted to VS 2019 eventually (see #14). Unity integration The Unity integration has been tested on Unity 2018.3.x and 2019.1.x . Versions earlier than 2018.3.x may work but are not officially supported. Build steps Check out the repository and its dependencies git clone --recursive https://github.com/microsoft/MixedReality-WebRTC.git Note that this may take some time (> 5 minutes) due to the large number of submodules in the WebRTC UWP SDK repository this repository depends on. Build the WebRTC UWP SDK libraries Using the build script In order to simplify building, a PowerShell build script is available. The prerequisites still need to be installed manually before running it. To use the script, simply run for example: cd tools/build/ build.ps1 -BuildConfig Debug -BuildArch x64 -BuildPlatform Win32 Note - Currently the build script assumes it runs from tools/build/ only. It will fail if invoked from another directory. Valid parameter values are: BuildConfig : Debug | Release BuildArch : x86 | x64 | ARM | ARM64 BuildPlatform : Win32 | UWP Note - ARM and ARM64 are only valid for the UWP platform. Note - ARM64 is not yet available (see #13 ). The manual steps are details below and can be skipped if running the build script. Manually The WebRTC UWP project has specific requirements . In particular it needs Python 2.7.15+ installed as default , that is calling python from a shell without specifying a path launches that Python 2.7.15+ version. Note - Currently the Azure hosted agents with VS 2017 have Python 2.7.14 installed, but this is discouraged by the WebRTC UWP team as some spurious build errors might occur. The new VS 2019 build agents have Python 2.7.16 installed. Note - Currently the libyuv external dependency is incorrectly compiled with Clang instead of MSVC on ARM builds. This was an attempt to benefit from inline assembly, but this produces link errors (see this issue ). Until this is fixed, a patch is available under tools\\patches\\libyuv_win_msvc_157.patch which is applied by build.ps1 but needs to be applied manually if build.ps1 is not used. More generally all patches under tools\\patches need to be manually applied . For Windows 10 Desktop support (also called \"Win32\"): Open the WebRtc.Win32.sln Visual Studio solution located in external\\webrtc-uwp-sdk\\webrtc\\windows\\solution\\ In the menu bar, select the relevant solution platform and solution configuration. For the Unity editor, the x64 binaries are required. Build the WebRtc.Win32.Native.Builder project alone , which generates some files needed by some of the other projects in the solution, by right-clicking on that project > Build . The other projects are samples and are not needed. For UWP support: Open the WebRtc.Universal.sln Visual Studio solution located in external\\webrtc-uwp-sdk\\webrtc\\windows\\solution\\ In the menu bar, select the relevant solution platform and solution configuration. For HoloLens, the x86 binaries are required. For HoloLens 2, the ARM binaries are required (ARM64 is not supported yet, see #13 ). Build first the WebRtc.UWP.Native.Builder project alone , which generates some files needed by some of the other projects in the solution, by right-clicking on that project > Build Next build the Org.WebRtc and Org.WebRtc.WrapperGlue projects. The other projects samples and are not needed. Build the MixedReality-WebRTC libraries Open the Microsoft.MixedReality.WebRTC.sln Visual Studio solution located at the root of the repository. Build the solution with F7 or Build > Build Solution On successful build, the binaries will be generated in a sub-directory under bin/ , and the relevant DLLs will be copied by a post-build script to libs\\Microsoft.MixedReality.WebRTC.Unity\\Assets\\Plugins\\ for Unity to consume them. The Microsoft.MixedReality.WebRTC.sln Visual Studio solution contains several projects: The native C++ library, which can be compiled: for Windows Desktop with the Microsoft.MixedReality.WebRTC.Native.Win32 project for UWP with the Microsoft.MixedReality.WebRTC.Native.UWP project The C# library project Microsoft.MixedReality.WebRTC A C# unit tests project Microsoft.MixedReality.WebRTC.Tests A UWP C# sample app project Microsoft.MixedReality.WebRTC.TestAppUWP based on WPF and XAML Optionally test the installation Test the install by e.g. opening the Unity project at libs\\Microsoft.MixedReality.WebRTC.Unity , loading the Assets\\Microsoft.MixedReality.WebRTC\\Unity.Examples\\SimpleVideoChat scene and pressing Play . After a few seconds (depending on the machine) the left media player should display the video feed from the local webcam. The Unity console should also display a message about the WebRTC library being initialized successfully. See the Hello, Unity World! tutorial for more details."
  },
  "manual/cs/helloworld-cs-uwp.html": {
    "href": "manual/cs/helloworld-cs-uwp.html",
    "title": "Hello, C# world! (UWP) | MixedReality-WebRTC Documentation",
    "keywords": "Hello, C# world! (UWP) In this tutorial we will create a simple UWP application based on the MixedReality-WebRTC C# library , with a XAML-based UI. Creating a project Creating a peer connection Add local media tracks A custom signaling solution Establishing a WebRTC connection"
  },
  "manual/cs/helloworld-cs-signaling-uwp.html": {
    "href": "manual/cs/helloworld-cs-signaling-uwp.html",
    "title": "A custom signaling solution | MixedReality-WebRTC Documentation",
    "keywords": "A custom signaling solution Signaling is the process of communicating with a remote endpoint with the intent of establishing a peer-to-peer connection. The WebRTC standard does not enforce any specific protocol or solution for WebRTC signaling; instead it simply states that some opaque messages must be transported between the remote peers by whatever mean the developer choses, its signaling solution . The .NET Core Desktop tutorial introduces the NamedPipeSignaler , a simple solution for local discovery based on named pipes. Unfortunately named pipes are not available on UWP, so this solution cannot be used. Instead, we rely in this tutorial on the existing NodeDssSignaler already used by the TestAppUWP sample app and the Unity integration. This requires a little bit more setup, described in details in the Unity tutorial. Note The NodeDssSignaler found in the TestAppUWP and the one found in the Unity integration use the same protocol and are compatible, but the code is different, the latter being based on Unity's MonoBehaviour component class. Here we use the former, which is written in pure C# and is independent of Unity. Install As for the VideoBridge helper class, the easiest way to consume the NodeDssSignaler class in the App1 sample app for UWP is to copy the examples/TestAppUwp/NodeDssSignaler.cs file alongside the App1.csproj project of the current tutorial, and add a reference to it in the project using right-click > Add > Existing Item... (or Shift+Alt+A). The NodeDssSignaler class makes use of the Newtonsoft.Json package for JSON data serialization and deserialization. This module is available as a NuGet package. In the Solution Explorer panel, right click on the References item and select Manage NuGet Packages... . In the Browse panel, search for the Newtonsoft.Json package, select the latest stable version, and click the Install button. Setup Continue editing the MainPage.xaml.cs file. Import the TestAppUwp module. At the top of the MainPage.xaml.cs file add: using TestAppUwp; At the top of the MainPage class, create a NodeDssSignaler field. private NodeDssSignaler _signaler; In the OnLoaded() method, add some subscriptions to the signaling events. _peerConnection.LocalSdpReadytoSend += Peer_LocalSdpReadytoSend; _peerConnection.IceCandidateReadytoSend += Peer_IceCandidateReadytoSend; The LocalSdpReadytoSend event is triggered after a call to CreateOffer or CreateAnswer when WebRTC prepared the corresponding SDP message and signals the application the message is ready to be sent. The IceCandidateReadytoSend event similarly corresponds to ICE candidate messages generated by WebRTC, which the application needs to deliver to the remote peer. Implement the event handlers, which simply format the SDP message for the singaler. private void Peer_LocalSdpReadytoSend(string type, string sdp) { var msg = new NodeDssSignaler.Message { MessageType = NodeDssSignaler.Message.WireMessageTypeFromString(type), Data = sdp, IceDataSeparator = \"|\" }; _signaler.SendMessageAsync(msg); } private void Peer_IceCandidateReadytoSend( string candidate, int sdpMlineindex, string sdpMid) { var msg = new NodeDssSignaler.Message { MessageType = NodeDssSignaler.Message.WireMessageType.Ice, Data = $\"{candidate}|{sdpMlineindex}|{sdpMid}\", IceDataSeparator = \"|\" }; _signaler.SendMessageAsync(msg); } The NodeDssSignaler uses a simple JSON-based message encoding with a single string of data per message. For ICE messages, the 3 components of the message are joined together in that string with a given separator passed alongside the message, and split back into individual components on the receiving peer. Continue appending to the OnLoaded() method to initialize and start the signaler. // Initialize the signaler _signaler = new NodeDssSignaler() { HttpServerAddress = \"http://127.0.0.1:3000/\", LocalPeerId = \"App1\", RemotePeerId = \"<input the remote peer ID here>\", }; _signaler.OnMessage += (NodeDssSignaler.Message msg) => { switch (msg.MessageType) { case NodeDssSignaler.Message.WireMessageType.Offer: _peerConnection.SetRemoteDescription(\"offer\", msg.Data); _peerConnection.CreateAnswer(); break; case NodeDssSignaler.Message.WireMessageType.Answer: _peerConnection.SetRemoteDescription(\"answer\", msg.Data); break; case NodeDssSignaler.Message.WireMessageType.Ice: var parts = msg.Data.Split(new string[] { msg.IceDataSeparator }, StringSplitOptions.RemoveEmptyEntries); // Note the inverted arguments for historical reasons. // 'candidate' is last in AddIceCandidate(), but first in the message. string sdpMid = parts[2]; int sdpMlineindex = int.Parse(parts[1]); string candidate = parts[0]; _peerConnection.AddIceCandidate(sdpMid, sdpMlineindex, candidate); break; } }; _signaler.StartPollingAsync(); Warning Take care to set the value of the RemotePeerId field of the NodeDssSignaler to the remote peer's ID, otherwise the signaling will not work. Similarly, this tutorial arbitrarily uses App1 as the peer ID for the application; this value needs to be set as the remote peer's ID when configuring the remote peer signaler. In the App_Suspending() event handler, stop the signaler and clean-up the resources. if (_signaler != null) { _signaler.StopPollingAsync(); _signaler = null; } At this point the sample app is ready to establish a connection. Next : Establishing a WebRTC connection"
  },
  "manual/cs/helloworld-cs-signaling-core3.html": {
    "href": "manual/cs/helloworld-cs-signaling-core3.html",
    "title": "A custom signaling solution | MixedReality-WebRTC Documentation",
    "keywords": "A custom signaling solution Signaling is the process of communicating with a remote endpoint with the intent of establishing a peer-to-peer connection. The WebRTC standard does not enforce any specific protocol or solution for WebRTC signaling; instead it simply states that some opaque messages must be transported between the remote peers by whatever mean the developer choses, its signaling solution . NamedPipeSignaler In this chapter we use the NamedPipeSignaler found in examples/NamedPipeSignaler . This is a simple signaling solution based as the name implies on named pipes, which allows local peer discovery and connection out of the box on a local host without any configuration. This is not a production-ready solution, but for this tutorial it has the benefit of being very simple. Install The easiest way to consume the NamedPipeSignaler class in the TestNetCoreConsole sample app is to copy the examples/NamedPipeSignaler/NamedPipeSignaler.cs file alongside the TestNetCoreConsole.csproj project. This avoids the need for any reference setup in the project, or any other kind of configuration. Pipe creation There is no need to understand how the NamedPipeSignaler class works for this tutorial. But for the sake of curiosity, this is how the connection is established (you can skip to the Setup the signaler section below if not interested): Try to create a pipe server. If that succeeds, then this peer is the first peer and will act as server . If that fails, then another peer already created that pipe server, so this peer will act as client . If acting as server: Wait for the remote peer to connect its client pipe to this server. Create a reverse pipe client and connect to the reverse pipe server of the remote peer. If acting as client: Connect to the pipe server created by the other peer. Create a reverse pipe server, and wait for the server to connect back with its reverse pipe client. At this point, both peer have a client pipe for sending data and a server pipe for receiving data, and can communicate. Start a background task to read incoming messages from the remote peer, and wait. Setup the signaler Continue editing the Program.cs file and append the following: Create a signaler associated with the existing peer connection. var signaler = new NamedPipeSignaler.NamedPipeSignaler(pc, \"testpipe\"); Connect handlers to the signaler's messages, and forward them to the peer connection. signaler.SdpMessageReceived += (string type, string sdp) => { pc.SetRemoteDescription(type, sdp); if (type == \"offer\") { pc.CreateAnswer(); } }; signaler.IceCandidateReceived += (string candidate, int sdpMlineindex, string sdpMid) => { pc.AddIceCandidate(sdpMid, sdpMlineindex, candidate); }; Start the signaler and connect it to the remote peer's signaler. await signaler.StartAsync(); This last call will block until the two signalers are connected with each other. At this point the signaler is functional. However as pointed above it will wait for a second instance of the TestNetCoreConsole app to connect. But currently this cannot work, because both instances will attempt to capture the webcam and microphone, and one of them will fail to do so. Optional audio and video capture In order to test the signaler with 2 instances of TestNetCoreConsole , we need one of those to not attempt to open the audio and video capture devices. For this, we had some command-line arguments to control the audio and video capture. Continue editing the Program.cs file: At the top of the Main function, check if the audio and video capture arguments are present on the command-line arguments provided by the user. We name those arguments -v / --video to enable video capture, and -a / --audio to enable audio capture. bool needVideo = Array.Exists(args, arg => (arg == \"-v\") || (arg == \"--video\")); bool needAudio = Array.Exists(args, arg => (arg == \"-a\") || (arg == \"--audio\")); Wrap the calls to AddLocal(Audio|Video)TrackAsync into if blocks using the boolean just defined. We also print some console message, so that the user can confirm whether the flags were indeed taken into account. This is useful to avoid mistakes since we will be running 2 instances of the app, one with the flags and one without. // Record video from local webcam, and send to remote peer if (needVideo) { Console.WriteLine(\"Opening local webcam...\"); await pc.AddLocalVideoTrackAsync(); } // Record audio from local microphone, and send to peer if (needAudio) { Console.WriteLine(\"Opening local microphone...\"); await pc.AddLocalAudioTrackAsync(); } Establishing a signaler connection At this point the sample app is ready to establish a signaler connection. That is, 2 instances of the TestNetCoreConsole app can be launched, and their NamedPipeSignaler instances will connect together. Note however that we are not done yet with the peer connections, so the WebRTC peer-to-peer connection itself will not be established yet. Start 2 instances of the sample app: one with the audio/video flags, the capturer one without any flag, the receiver Terminal #1 (capturer) dotnet run TestNetCoreConsole -- --audio --video Terminal #2 (receiver) dotnet run TestNetCoreConsole The two terminals should print some messages and eventually indicate that the connection was successful: Signaler connection established. Next : Establishing a WebRTC connection"
  },
  "manual/cs/helloworld-cs-setup-uwp.html": {
    "href": "manual/cs/helloworld-cs-setup-uwp.html",
    "title": "Creating a project | MixedReality-WebRTC Documentation",
    "keywords": "Creating a project In this tutorial we create a C# UWP application with a simple XAML-based UI to render video. Note At this time there is no solution to render raw video frames from a .NET Core 3.0 application that is simple and short enough to be used in a turorial. Instead, we use the MediaPlayerElement XAML control from UWP which provides the necessary API. Its WPF equivalent MediaElement unfortunately does not currently allow specifying a custom video source other than an URI-based source like a file on disk. Note This tutorial assumes that the host device where the app will be running during the tutorial has access to: a webcam, or any other video capture device recognized by WebRTC a microphone, or any other audio capture device recognized by WebRTC Generate the project Open Visual Studio 2019 and select Create a new project . Filter projects by Language = C# and Platform = UWP to find the Blank App (Universal Window) project template. When prompted to select a platform version, chose a version of at least Windows 10, version 1803 (10.0; Build 17134) or higher, as older versions are not officially supported by MixedReality-WebRTC. For HoloLens 2 development, it is strongly recommended to select at least Windows 10, version 1903 (10.0; Build 18362) to be able to use the latest OS APIs, although those are not used in this tutorial. Visual Studio 2019 generates a C# project ( .csproj ) and solution ( .sln ). In this tutorial we use the default name App1 , therefore we get the following hierarchy: - App1/ - Assets/ - App.xaml - App.xaml.cs - App1.csproj - MainPage.xaml - MainPage.xaml.cs - App1.sln At this point the project is already in a working state, although not yet functional. Press F5 or select in the menu Debug > Start Debugging to build and launch the application. A blank window titles \"App1\" should appear. Note : At the top of the window, the XAML debug bar allows access to debugging feature for the UI. This bar only appears while debugging, not when running the app outside Visual Studio 2019. It can be ignored. Add a dependency to MixedReality-WebRTC In order to use the MixedReality-WebRTC project in this new App1 application, we will add a dependency to its C# NuGet package hosted on nuget.org . This is by far the easiest way, although a locally-built copy of the Microsoft.MixedReality.WebRTC.dll assembly could also be alternatively used (but this is out of the scope of this tutorial). In the Solution Explorer , right-click on the App1 (Universal Windows) C# project and select the Manage NuGet Packages... menu entry. This opens a new tab NuGet: App1 which allows configuring the NuGet dependencies for this project alone. The Installed tab contains the list of NuGet dependencies already installed, and should contain the Microsoft.NETCore.UniversalWindowsPlatform which was already installed by Visual Studio when creating the project. Select the Browse tab and, after making sure that the Package source is set to nuget.org , select the Microsoft.MixedReality.WebRTC.UWP NuGet package and click Install . Note : If you cannot find the package, make sure that Include prerelease is checked, which disables filtering out preview packages (those packages with a version containing a suffix like \"-preview\" after the X.Y.Z version number). This will download from nuget.org and install the Microsoft.MixedReality.WebRTC.UWP.nupkg NuGet package, which contains the Microsoft.MixedReality.WebRTC.dll assembly, as well as its native dependencies (x86, x64, ARM) for the UWP platform. After that, the App1 project should contain a reference to the package. Test the reference In order to ensure everything works fine and the Microsoft.MixedReality.WebRTC assembly can be used, we will use one of its functions to list the video capture devices, as a test. This makes uses of the static method PeerConnection.GetVideoCaptureDevicesAsync() . This is more simple than creating objects, as there is no clean-up needed after use. First, because this sample application is a UWP application, it needs to declare some capabilities to access the microphone and webcam on the host device. In the Solution Explorer of Visual Studio, double-click on the Package.appxmanifest to open the AppX manifest of the app and select the Capabilities tab. Check Microphone and Webcam , and confirm that Internet (Client) is already checked. Next, edit MainPage.xaml.cs : At the top of the file, add some using statement to import the Microsoft.MixedReality.WebRTC assembly. Also import the System.Diagnostics module, as we will be using the Debugger class to print debug information to the Visual Studio output window. Finally, import the Windows.Media.Capture module to be able to request access to the microphone and webcam, and the Windows.ApplicationModel module to handle resource clean-up. using Microsoft.MixedReality.WebRTC; using System.Diagnostics; using Windows.Media.Capture; using Windows.ApplicationModel; In the MainPage constructor, register a handler for the Loaded event, which will be fired once the XAML user interface finished loading. For now it is not required to wait on the UI to call Microsoft.MixedReality.WebRTC methods. But later when accessing the UI to interact with its controls, either to get user inputs or display results, this will be required. So as a best practice we start doing so right away instead of invoking some code directly in the MainPage constructor. Also register a handler for the Application.Suspending event to clean-up resources on exit. public MainPage() { this.InitializeComponent(); this.Loaded += OnLoaded; Application.Current.Suspending += App_Suspending; } Create the event handler OnLoaded() and use it to request access from the user to the microphone and camera, and enumerate the video capture devices. The MediaCapture.InitializeAsync() call will prompt the user with a dialog to authorize access to the microphone and webcam. The latter be must authorized before calling PeerConnection.GetVideoCaptureDevicesAsync() , while the former will be needed in the following of the tutorial for calls like PeerConnection.AddLocalAudioTrackAsync() . private async void OnLoaded(object sender, RoutedEventArgs e) { // Request access to microphone and camera var settings = new MediaCaptureInitializationSettings(); settings.StreamingCaptureMode = StreamingCaptureMode.AudioAndVideo; var capture = new MediaCapture(); await capture.InitializeAsync(settings); // Retrieve a list of available video capture devices (webcams). List<VideoCaptureDevice> deviceList = await PeerConnection.GetVideoCaptureDevicesAsync(); // Get the device list and, for example, print them to the debugger console foreach (var device in deviceList) { // This message will show up in the Output window of Visual Studio Debugger.Log(0, \"\", $\"Webcam {device.name} (id: {device.id})\\n\"); } } Create the event handler App_Suspending() . For now there is nothing to do from it. private void App_Suspending(object sender, SuspendingEventArgs e) { } Launch the app again. The main window is still empty, but the Output window of Visual Studio 2019 ( View > Output , or Alt + 2 ) should show a list of devices. This list depends on the actual host device running the app, but looks something like: Webcam <some device name> (id: <some device ID>) Note that there might be multiple lines if multiple capture devices are available. In general the first one listed will be the default used by WebRTC, although it is possible to explicitly select a device (see PeerConnection.AddLocalVideoTrackAsync ). If this is the first time that MediaCapture.InitializeAsync() is requesting access to the webcam and microhpone, Windows displays a prompt asking the user for confirmation. You must click Yes , otherwise access to the microphone and webcam will be denied, and WebRTC will not be able to use them. This is part of the standard UWP capability mechanism for security and privacy. If you clicked No by mistake, the prompt will not appear again and access will be silently denied on next runs. To change this access setting again, go to the Windows Settings > Privacy > Microphone , find the App1 application and toggle its access On . Do the same for the webcam from the Settings > Privacy > Camera page. Next : Creating a peer connection"
  },
  "manual/cs/helloworld-cs-mediatracks-core3.html": {
    "href": "manual/cs/helloworld-cs-mediatracks-core3.html",
    "title": "Add local media tracks | MixedReality-WebRTC Documentation",
    "keywords": "Add local media tracks Now that the peer connection is initialized, there are two possible paths, which can be both used: Immediately adding local audio and/or video tracks to the peer connection, so that they are available right away when the connection will be established with the remote peer. Waiting for the connection to be established, and add the local media tracks after that. The first case is benefical in the sense that media tracks will be immediately negotiated during the connection establishing, without the need for an extra negotiation specific for the tracks. However it requires knowing in advance that the tracks are used. Conversely, the latter case corresponds to a scenario like late joining, where the user or the application can control when to add or remove local tracks, at the expense of requiring an extra network negotiation each time the list of tracks is changed. In this tutorial, we add the local media tracks right away for simplicity. Continue editing the Program.cs file and append the following: Use the AddLocalVideoTrackAsync() method to add to the peer connection a local video track sending to the remote peer some video frames obtained from a local video capture device (webcam). await pc.AddLocalVideoTrackAsync(); This method optionally takes a LocalVideoTrackSettings object to configure the video capture. In this tutorial, we leave that object out and use the default settings, which will open the first available webcam with its default resolution and framerate. This is generally acceptable, although on mobile devices like HoloLens you probably want to limit the resolution and framerate to reduce the power consumption and save on battery. Use the AddLocalAudioTrackAsync() method to add to the peer connection a local audio track sending to the remote peer some audio frames obtained from a local audio capture device (microphone). await pc.AddLocalAudioTrackAsync(); Unlike for the video track, the audio track currently does not offer any configuration option, and will always use the first available audio capture device. Run the application again. This time, there is no visible difference in the terminal window, except some extra delay to open the audio and video devices; this delay varies greatly depending on the number of capture devices on the host machine, but is generally within a few seconds too. Additionally, if the webcam or microphone have a LED indicating recording, it should briefly turn ON when the capture device starts recording, and immediately stop when the program reaches its end and the peer connection and its tracks are automatically shut down. Next : A custom signaling solution"
  },
  "manual/cs/helloworld-cs-core3.html": {
    "href": "manual/cs/helloworld-cs-core3.html",
    "title": "Hello, C# world! (Desktop) | MixedReality-WebRTC Documentation",
    "keywords": "Hello, C# world! (Desktop) In this tutorial we will create a simple .NET Core 3.0 application based on the MixedReality-WebRTC C# library . Creating a project Creating a peer connection Add local media tracks A custom signaling solution Establishing a WebRTC connection"
  },
  "manual/cs/helloworld-cs-connection-uwp.html": {
    "href": "manual/cs/helloworld-cs-connection-uwp.html",
    "title": "Establishing a WebRTC connection | MixedReality-WebRTC Documentation",
    "keywords": "Establishing a WebRTC connection Now that the signaling solution is in place, the final step is to establish a peer connection. Continue editing the OnLoaded() method and append after the InitializeAsync() call: For debugging purpose, and to understand what is going on with the connection, connect the Connected and IceStateChanged events to handlers printing messages to the debugger. These messages are visible in the Output window of Visual Studio. _peerConnection.Connected += () => { Debugger.Log(0, \"\", \"PeerConnection: connected.\\n\"); }; _peerConnection.IceStateChanged += (IceConnectionState newState) => { Debugger.Log(0, \"\", $\"ICE state: {newState}\\n\"); }; The Connected event is invoked when the peer connection is established, that is when an offer/answer pair is successfully exchanged. The IceStateChanged is invoked each time the ICE status changes. Note that the Connected event can be invoked before the ICE status reaches its IceConnectionState.Connected state. In order to render the remote video, we also subscribe to the I420ARemoteVideoFrameReady event. _peerConnection.I420ARemoteVideoFrameReady += Peer_RemoteI420AFrameReady; That event handler is similar to the one for the local video, using another video bridge. Create a new set of fields for the remote video: private object _remoteVideoLock = new object(); private bool _remoteVideoPlaying = false; private MediaStreamSource _remoteVideoSource; private VideoBridge _remoteVideoBridge = new VideoBridge(5); This time we increase the video buffer queue to 5 frames, as the remote video is more prone to delays due to network latency. Modify the OnMediaStreamSourceRequested() event handler to dispatch either to the local or to the remote bridge: if (sender == _localVideoSource) videoBridge = _localVideoBridge; else if (sender == _remoteVideoSource) videoBridge = _remoteVideoBridge; else return; Implement the handler with the newly created members: private void Peer_RemoteI420AFrameReady(I420AVideoFrame frame) { lock (_remoteVideoLock) { if (!_remoteVideoPlaying) { _remoteVideoPlaying = true; uint width = frame.width; uint height = frame.height; RunOnMainThread(() => { // Bridge the remote video track with the remote media player UI int framerate = 30; // for lack of an actual value _remoteVideoSource = CreateI420VideoStreamSource(width, height, framerate); var remoteVideoPlayer = new MediaPlayer(); remoteVideoPlayer.Source = MediaSource.CreateFromMediaStreamSource( _remoteVideoSource); remoteVideoPlayerElement.SetMediaPlayer(remoteVideoPlayer); remoteVideoPlayer.Play(); }); } } _remoteVideoBridge.HandleIncomingVideoFrame(frame); } In the App_Suspending() event handler, add a line to also clear the media player of the remote element. remoteVideoPlayerElement.SetMediaPlayer(null); Finally, we need to change the UI to add the new remoteVideoPlayerElement XAML control displaying the remote video track. Open MainPage.xaml in the visual editor and edit it: Add a new <MediaPlayerElement> tag for the remote video. <MediaPlayerElement x:Name=\"remoteVideoPlayerElement\" /> Warning Be sure to put the remote tag first, before the local one, so that the remote video is rendered first in the background, and the local one second on top of it. Otherwise the local video will be hidden by the remote one. Change the tag for the local video to reduce its size and position it in the lower right corner of the window, like is typical for local video preview in a video chat application. <MediaPlayerElement x:Name=\"localVideoPlayerElement\" Width=\"320\" Height=\"240\" HorizontalAlignment=\"Right\" VerticalAlignment=\"Bottom\" Margin=\"0,0,20,20\" /> Here we fix the size to 320x240 pixels, align the control to the lower right corner of the window, and add a 20px margin. At this point, the sample application is functional, although there is no mechanism to initiate a call. You can either add a button or similar to call CreateOffer() , or test this sample with the TestAppUWP available in the MixedReality-WebRTC repository in examples/TestAppUWP , which implements a \"Create offer\" button. Be sure to set the correct local and remote peer ID on both peers and have a node-dss server running before hitting the \"Create offer\" button, otherwise signaling will not work."
  },
  "manual/cs/helloworld-cs-connection-core3.html": {
    "href": "manual/cs/helloworld-cs-connection-core3.html",
    "title": "Establishing a WebRTC connection | MixedReality-WebRTC Documentation",
    "keywords": "Establishing a WebRTC connection Now that the signaling solution is in place, the final step is to establish a peer connection. Continue editing the Program.cs file and append the following: For debugging purpose, and to understand what is going on with the connection, connect the Connected and IceStateChanged events to handlers printing messages to console. pc.Connected += () => { Console.WriteLine(\"PeerConnection: connected.\"); }; pc.IceStateChanged += (IceConnectionState newState) => { Console.WriteLine($\"ICE state: {newState}\"); }; The Connected event is invoked when the peer connection is established. The IceStateChanged is invoked each time the ICE status changes. Note that the Connected event can be invoked before the ICE status reaches its IceConnectionState.Connected state. In order to verify that the remote video is received, we also subscribe to the I420ARemoteVideoFrameReady event. Since this event is invoked frequently, we only print a message every 60 frames. int numFrames = 0; pc.I420ARemoteVideoFrameReady += (I420AVideoFrame frame) => { ++numFrames; if (numFrames % 60 == 0) { Console.WriteLine($\"Received video frames: {numFrames}\"); } }; To establish a WebRTC connection, one peer has to call CreateOffer() , but not both. Since the signaler implementation NamedPipeSignaler already provides a way to distinguish between the two peers, we use that information to select which peer will automatically initiate the connection. if (signaler.IsClient) { Console.WriteLine(\"Connecting to remote peer...\"); pc.CreateOffer(); } else { Console.WriteLine(\"Waiting for offer from remote peer...\"); } In this state, the application is working but will terminate immediately once the peer connection is established. To prevent that, simply wait until the user press a key, and then close the signaler. Console.WriteLine(\"Press a key to terminate the application...\"); Console.ReadKey(true); signaler.Stop(); Console.WriteLine(\"Program termined.\"); Run the two instances of the application again. This time both terminals print a large quantity of messages related to SDP and ICE message exchanges, and eventually establish a WebRTC peer connection. If launched with the audio or video capture flags, the capturer instance records those media and send them via the network to the other instance, which invokes the remote frame callback and print a message every 60 frames. After that, you can press any key to stop each instance. The signaler and peer connection will close and the program will terminate."
  },
  "manual/cs/cs.html": {
    "href": "manual/cs/cs.html",
    "title": "C# library overview | MixedReality-WebRTC Documentation",
    "keywords": "C# library overview The C# library Microsoft.MixedReality.WebRTC provides a wrapper over the C++ native library of MixedReality-WebRTC, offering a more C# oriented API with familiar constructs such as Task and the async / await keywords. There are currently two different tutorials: The .NET Core 3.0 tutorial for the Windows Desktop platform builds a console application which records audio and video. The UWP tutorial for the Windows Universal Platform builds a graphical application with a XAML-based UI."
  },
  "manual/cs/cs-signaling.html": {
    "href": "manual/cs/cs-signaling.html",
    "title": "C# signaling | MixedReality-WebRTC Documentation",
    "keywords": "C# signaling The C# library does not have a dedicated class for signaling. Instead, the PeerConnection class provides some events and methods to build upon in order to build a signaling solution. A custom signaling solution needs to handle sending locally-prepared messages to the remote peer, for example via a separate TCP/IP connection, and dispatching messages received from the remote peer down to the local peer connection. Neither the WebRTC standard nor the MixedReality-WebRTC library specify the way those messages must be transmitted. Local to remote Local messages are generated under several circumstances by the local peer. The signaling solution must listen to the LocalSdpReadytoSend and IceCandidateReadytoSend events, and dispatch those messages to the remote peer by whatever way it choses. peerConnection.LocalSdpReadyToSend += (string type, string sdp) => { MyCustomSignaling_SendSdp(type, sdp); } peerConnection.IceCandidateReadytoSend += (string candidate, int sdpMlineindex, string sdpMid) => { MyCustomSignaling_SendIce(candidate, sdpMlineindex, sdpMid); } Remote to local Upon receiving the above messages, the signaling solution must: For SDP messages originating from a LocalSdpReadytoSend event invoked on the remote peer, call the PeerConnection.SetRemoteDescription() method to inform the local peer connection of the newly received session description. public void OnSdpMessage(string type, string sdp) { peerConnection.SetRemoteDescription(type, sdp); // Optionally if (type == \"offer\") { peerConnection.CreateAnswer(); } } For ICE messages originating from an IceCandidateReadytoSend event invoked on the remote peer, call the PeerConnection.AddIceCandidate() method to inform the local peer connection of the newly received ICE candidate. public void OnIceMessage(string candidate, int sdpMlineindex, string sdpMid) { // Note the args order here! peerConnection.AddIceCandidate(sdpMid, sdpMlineindex, candidate); }"
  },
  "manual/unity-peerconnection.html": {
    "href": "manual/unity-peerconnection.html",
    "title": "Unity PeerConnection component | MixedReality-WebRTC Documentation",
    "keywords": "Unity PeerConnection component The PeerConnection Unity component encapsulates a single peer connection between the local application and another remote Unity peer application. Note The C# library also has a PeerConnection class, which this components build upon. Property Description Behavior settings Auto Initialize On Start Automatically initialize the peer connection when the MonoBehaviour.Start() . If not set, the user need to call InitializeAsync() manually before using the component. Auto Log Errors To Unity Console Add an event listener to the OnError event which calls Debug.LogError() to display the error message in the Unity console. ICE servers Ice Servers A list of ConfigurableIceServer elements representing the list of ICE servers to use to establish the peer connection. The list can be empty, in which case only a local connection will be possible. Ice Username Optional user name for TURN servers authentication. Ice Credential Optional password for TURN servers authentication. Peer connection events On Initialized Event fired once the InitializeAsync() method returned successfully, to indicate that the peer connection component is ready for use. On Shutdown Event fired when Uninitialize() is called, usually automatically during MonoBehaviour.OnDestroy() On Error Event fired when an error occur in the peer connection."
  },
  "manual/unity-mediaplayer.html": {
    "href": "manual/unity-mediaplayer.html",
    "title": "Unity MediaPlayer component | MixedReality-WebRTC Documentation",
    "keywords": "Unity MediaPlayer component The MediaPlayer Unity component is a utility component mixing an audio source and a video source for local rendering. Important FIXME: This component is not currently fully functional. The remote audio data is currently sent directly to the local audio out device by the internal WebRTC implementation without any configuration possible. And the local audio is never rendered. So the MediaPlayer.AudioSource property is unused. The video however capability works as intended. See issue #92 for details. Property Description Source AudioSource Reference to the AudioSource instance that the media player outputs the audio of. VideoSource Reference to the VideoSource instance that the media player renders the video of. MaxVideoFramerate Maximum number of frames per second rendered. Extra frames coming from the video source are discarded. Statistics EnableStatistics Enable the collecting of video statistics by the media player. This adds some minor overhead. FrameLoadStatHolder Reference to a TextMesh instance whose text is set to the number of incoming video frames per second pulled from the video source into the media player's internal queue. FramePresentStatHolder Reference to a TextMesh instance whose text is set to the number of video frames per second dequeued from the media player's internal queue and rendered to the texture(s) of the Renderer component associated with this media player. FrameSkipStatHolder Reference to a TextMesh instance whose text is set to the number of video frames per second dropped due to the media player's internal queue being full. This corresponds to frames being enqueued faster than they are dequeued, which happens when the source is overflowing the sink, and the sink cannot render all frames."
  },
  "manual/unity-localvideosource.html": {
    "href": "manual/unity-localvideosource.html",
    "title": "Unity LocalVideoSource component | MixedReality-WebRTC Documentation",
    "keywords": "Unity LocalVideoSource component The LocalVideoSource Unity component represents a single video track obtaining its frames from a local video capture device. The component controls both the capture device and the track it feeds. Property Description Video track PeerConnection Reference to the PeerConnection instance a video track is added to. AutoAddTrack Automatically start video capture and add a video track to the PeerConnection instance pointed by the LocalVideoSource.PeerConnection property once the peer connection is initialized. See PeerConnection.OnInitialized . Local video capture AutoStartCapture Automatically start video capture once the PeerConnection instance pointed by the LocalVideoSource.PeerConnection property is initialized. See PeerConnection.OnInitialized . PreferredVideoCodec Optional choice of a preferred video codec to use for local video capture and SDP offering. This is implemented by filtering out all other video codecs when sending an SDP offer message if the original offer was already containing that codec. EnableMixedRealityCapture On platforms supporting Mixed Reality Capture (MRC) like HoloLens 1st generation and 2nd generation, instruct the video capture module to enable this feature and produce a video stream containing the holograms rendered over the raw webcam feed. This has no effect if the local device does not support MRC."
  },
  "manual/unity-localaudiosource.html": {
    "href": "manual/unity-localaudiosource.html",
    "title": "Unity LocalAudioSource component | MixedReality-WebRTC Documentation",
    "keywords": "Unity LocalAudioSource component The LocalAudioSource Unity component represents a single audio track obtaining its data from a local audio capture device. The component controls both the capture device and the track it feeds. Property Description Audio track PeerConnection Reference to the PeerConnection instance an audio track is added to. AutoAddTrack Automatically start audio capture and add an audio track to the PeerConnection instance pointed by the LocalAudioSource.PeerConnection property once the peer connection is initialized. See PeerConnection.OnInitialized . Local audio capture AutoStartCapture Automatically start audio capture once the PeerConnection instance pointed by the LocalAudioSource.PeerConnection property is initialized. See PeerConnection.OnInitialized .. PreferredAudioCodec Optional choice of a preferred audio codec to use for local audio capture and SDP offering. This is implemented by filtering out all other audio codecs when sending an SDP offer message if the original offer was already containing that codec. Events AudioStreamStarted Event invoked after the local audio stream started. AudioStreamStopped Event invoked before the local audio stream is stopped."
  },
  "manual/helloworld-unity.html": {
    "href": "manual/helloworld-unity.html",
    "title": "Hello, Unity world! | MixedReality-WebRTC Documentation",
    "keywords": "Hello, Unity world! In this tutorial we will create a simple Unity application based on the MixedReality-WebRTC Unity integration . Creating a new Unity project Importing MixedReality-WebRTC Creating a peer connection Creating a signaler Adding local video Adding remote video Establishing a connection"
  },
  "manual/helloworld-unity-signaler.html": {
    "href": "manual/helloworld-unity-signaler.html",
    "title": "Creating a signaler | MixedReality-WebRTC Documentation",
    "keywords": "Creating a signaler The WebRTC standard specifies how a peer-to-peer connection can be established using the Session Description Protocol (SDP) , but does not enforce a particular signaling solution to discover and select a remote peer, and to send to and receive from it the SDP messages necessary to establish that connection. MixedReality-WebRTC offers a built-in solution in the form of the NodeDssSignaler component, but also allows any other custom implementation to be used. For this tutorial, we will use the NodeDssSignaler component for simplicity. Caution NodeDssSignaler is very simple and useful for getting started quickly and debugging, but it is worth noting that this is not a production-quality solution and should not be used in production . In particular, this component offers no security whatsoever , all communications happening in clear text over HTTP, and no authentication , as all it requires to connect with a remote peer is to know its identifier. Do not be fooled by the fact that WebRTC supports encryption, as it would be very easy for an attacker to bypass it by compromising the signaler. Remember that any security solution is no better that its weakest link, and NodeDssSignaler is that link. It must be replaced with a secure solution when moving to production. The NodeDssSignaler component uses a Node.js server with a simple polling model where both peers are constantly checking if a message is available. The server implementation is called node-dss and is freely available on GitHub. Install and run node-dss The node-dss repository has instructions on how to install and run the server , which essentially boil down to installing Node.js, downloading the code, and running: set DEBUG=dss* npm install npm start This opens a console window which will output all requests received from all peers. Leave that console window open and the node-dss server running for the following, and go back to the Unity editor. Creating a NodeDssSignaler The NodeDssSignaler component can be added to the existing GameObject , or to a new one. There is no fundamental difference, and this is mostly a matter of taste. For this tutorial we will create a separate game object to separate it from the peer connection in the Hierarchy window. Create a new GameObject with a NodeDssSignaler component: In the Hierarchy window , select Create > Create empty to add a new GameObject to the scene. In the Inspector window , select Add Component > MixedReality-WebRTC > NodeDssSignaler to add a NodeDssSignaler component to that new object. Optionally rename the game object to something easy to remember like \"MySignaler\" to easily find it in the Hierarchy window. By default the NodeDssSignaler component is configured to connect to a node-dss server running locally on the developper machine at http://127.0.0.1:3000/ and poll the server every 500 milliseconds to query for available messages. Connecting the signaler Now that a signaling solution is available, the last step is to assign the PeerConnection.Signaler property of our peer connection to that implementation: In the Hierarchy window, select the game object with the peer connection component In the Inspector window, find the Signaler property and click on the circle to its right to bring the asset selection window From the Scene tab, select the game object with the NodeDssSignaler component on it. The signaler object should now appear in the Inspector window of the peer connection game object. At that point the peer connection is fully configured and ready to be used. However audio and video tracks are not added automatically, so there is little use for that peer connection. Next we will look at connecting a local webcam and microphone to provide some video and audio track to send through to the peer connection."
  },
  "manual/building.html": {
    "href": "manual/building.html",
    "title": "Building from sources | MixedReality-WebRTC Documentation",
    "keywords": "Building from sources The MixedReality-WebRTC libraries are built from the Microsoft.MixedReality.WebRTC.sln Visual Studio solution located at the root of the repository. If you want to start right away with MixedReality-WebRTC, the recommended approach is to consume the precompiled binaries distributed as NuGet packages instead. See Download for details. Prerequisites Environment and tooling The NuGet packages for the input dependencies (see Core input dependencies below) require in total approximately 10 GB of disk space . Those dependencies contain unstripped .lib files much larger than the final compiled DLL libraries, for both the Debug and Release build configurations. Due to Windows paths length limit, it is recommended to clone the source repository close to the root of the filesystem, e.g. C:\\mr-webrtc\\ or similar, as the recursive external dependencies create a deep hierarchy which may otherwise produce paths beyond the OS limit and result in build failure. The solution uses Visual Studio 2019 with the following features: The MSVC v141 - VS 2017 C++ x64/x86 build tools toolchain from Visual Studio 2017 is required to build the C++17 library of MixedReality-WebRTC. This will eventually be replaced with the Visual Studio 2019 compiler (v142 toolchain) once the project is upgraded to a Google milestone supporting Visual Studio 2019 (see details on issue #14 ) For ARM support, the MSVC v141 - VS 2017 C++ ARM build tools toolchain is also required. The C# library requires a .NET Standard 2.0 compiler, like the Roslyn compiler available as part of Visual Studio when installing the .NET desktop development workload. The UWP libraries and projects require UWP support from the compiler, available as part of Visual Studio when installing the Universal Windows Platform development workload. The Unity integration is officially supported for Unity version 2018.4.x (LTS). However, although the 2018.4 LTS version is currently the only officially supported version, we do our best to keep things working on 2019+ versions too. Versions earlier than 2018.4.x may work but are not tested at all, and no support will be provided for those. Core input dependencies The so-called Core input dependencies are constituted of: webrtc.lib : A static library containing the Google implementation of the WebRTC standard. Org.WebRtc.winmd : A set of WinRT wrappers for accessing the WebRTC API from UWP. Those dependencies require many extra prerequisites. They are also complex and time-consuming to build. Therefore to save time and avoid headache the MixedReality-WebRTC solution consumes those dependencies as precompiled NuGet packages available from nuget.org . Those NuGet packages are compiled from the WebRTC UWP project maintained by Microsoft. So there is no extra setup for those . Cloning the repository The official repository containing the source code of MixedReality-WebRTC is hosted on GitHub . The latest developments are done on the master branch, while the latest stable release is a release/* branch. Clone the choosen branch of the repository and its dependencies recursively, preferably close to the root of the filesystem (see prerequisites): git clone --recursive https://github.com/microsoft/MixedReality-WebRTC.git -b <branch_name> C:\\mr-webrtc Note that this may take some time (> 5 minutes) due to the large number of submodules in the WebRTC UWP SDK repository this repository depends on. Building the libraries Open the Microsoft.MixedReality.WebRTC.sln Visual Studio solution located at the root of the freshly cloned repository. Build the solution with F7 or Build > Build Solution On successful build, the binaries will be generated in a sub-directory under bin/ , and the relevant DLLs will be copied by a post-build script to libs\\Microsoft.MixedReality.WebRTC.Unity\\Assets\\Plugins\\ for the Unity integration to consume them. Important Be sure to build the solution before opening any Unity integration project. As part of the build, the libraries are copied to the Plugins directory of the Unity integration project. There are already some associated .meta files, which have been committed to the repository, to inform Unity of the platform of each DLL. If the Unity project is opened first, before the DLLs are present, Unity will assume those .meta files are stale and will delete them, and then later will recreate some with a different default config once the DLLs are copied. This leads to errors about modules with duplicate names. See the Importing MixedReality-WebRTC chapter of the \"Hello, Unity World!\" tutorial for more details. Testing the build Test the newly built libraries by e.g. using the VideoChatDemo Unity integration sample: Open the Unity project at libs\\Microsoft.MixedReality.WebRTC.Unity . Load the Assets\\Microsoft.MixedReality.WebRTC\\Unity.Examples\\VideoChatDemo scene. At the top center of the Unity editor window, press the Play button. After a few seconds (depending on the host device) the left media player should display the video feed from the local webcam. The Unity console window should also display a message about the WebRTC library being initialized successfully. Note that this does not test any remote connection , but simply ensures that Unity can use the newly built C++ and C# libraries. See the Hello, Unity World! tutorial for more details. Installing into an existing C# project The C# library requires the C++ library, which contains the core WebRTC implementation. The setup is summarized in the following table: Source DLLs How to add bin\\netstandard2.0\\Release\\Microsoft.MixedReality.WebRTC.dll Include in \"References\" of your VS project bin\\<platform>\\<arch>\\Release\\Microsoft.MixedReality.WebRTC.Native.dll Add as \"Content\" to the project, so that the Deploy step copies the DLL to the AppX folder alongside the application executable. See the TestAppUWP project for an example, noting how it uses the $(Platform) and $(Configuration) Visual Studio variables to automatically copy the right DLL corresponding to the currently selected project configuration. where: <platform> is either Win32 for a Desktop app, or UWP for a UWP app. <arch> is one of [ x86 , x64 , ARM ]. Note that ARM is only available on UWP."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.RemoteAudioSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.RemoteAudioSource.html",
    "title": "Class RemoteAudioSource | MixedReality-WebRTC Documentation",
    "keywords": "Class RemoteAudioSource This component represents a remote audio source added as an audio track to an existing WebRTC peer connection by a remote peer and received locally. The audio track can optionally be displayed locally with a MediaPlayer . Inheritance Object AudioSource RemoteAudioSource Inherited Members AudioSource.AudioStreamStarted AudioSource.AudioStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class RemoteAudioSource : AudioSource Fields AutoPlayOnAdded Automatically play the remote audio track when it is added. Declaration public bool AutoPlayOnAdded Field Value Type Description Boolean PeerConnection Peer connection this remote audio source is extracted from. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection Properties IsPlaying Is the audio source currently playing? The concept of playing is described in the Play() function. Declaration public bool IsPlaying { get; } Property Value Type Description Boolean See Also Play() Stop() Methods Awake() Implementation of MonoBehaviour.Awake which registers some handlers with the peer connection to listen to its OnInitialized and OnShutdown events. Declaration protected void Awake() OnDestroy() Implementation of MonoBehaviour.OnDestroy which unregisters all listeners from the peer connection. Declaration protected void OnDestroy() Play() Manually start playback of the remote audio feed by registering some listeners to the peer connection and starting to enqueue audio frames as they become ready. If AutoPlayOnAdded is true then this is called automatically as soon as the peer connection is initialized. Declaration public void Play() Remarks This is only valid while the peer connection is initialized, that is after the OnInitialized event was fired. See Also Stop() IsPlaying Stop() Stop playback of the remote audio feed and unregister the handler listening to remote video frames. Note that this is independent of whether or not a remote track is actually present. In particular this does not fire the AudioStreamStopped , which corresponds to a track being made available to the local peer by the remote peer. Declaration public void Stop() See Also Play() IsPlaying Update() Implementation of MonoBehaviour.Update to execute from the current Unity main thread any background work enqueued from free-threaded callbacks. Declaration protected void Update()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.PeerConnection.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.PeerConnection.html",
    "title": "Class PeerConnection | MixedReality-WebRTC Documentation",
    "keywords": "Class PeerConnection High-level wrapper for Unity WebRTC functionalities. This is the API entry point for establishing a connection with a remote peer. Inheritance Object PeerConnection Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class PeerConnection : MonoBehaviour Fields AutoInitializeOnStart Flag to initialize the peer connection on MonoBehaviour.Start() . Declaration public bool AutoInitializeOnStart Field Value Type Description Boolean AutoLogErrorsToUnityConsole Flag to log all errors to the Unity console automatically. Declaration public bool AutoLogErrorsToUnityConsole Field Value Type Description Boolean IceCredential Optional credential for the ICE servers. Declaration public string IceCredential Field Value Type Description String IceServers Set of ICE servers the WebRTC library will use to try to establish a connection. Declaration public List<ConfigurableIceServer> IceServers Field Value Type Description List < ConfigurableIceServer > IceUsername Optional username for the ICE servers. Declaration public string IceUsername Field Value Type Description String OnError Event that occurs when a WebRTC error occurs Declaration public WebRTCErrorEvent OnError Field Value Type Description WebRTCErrorEvent OnInitialized Event fired after the peer connection is initialized and ready for use. Declaration public UnityEvent OnInitialized Field Value Type Description UnityEvent OnShutdown Event fired after the peer connection is shut down and cannot be used anymore. Declaration public UnityEvent OnShutdown Field Value Type Description UnityEvent Signaler Signaler to use to establish the connection. Declaration public Signaler Signaler Field Value Type Description Signaler Properties Peer Retrieves the underlying peer connection object once initialized. Declaration public PeerConnection Peer { get; } Property Value Type Description PeerConnection Remarks If OnInitialized has not fired, this will be null . Methods GetVideoCaptureDevicesAsync() Enumerate the video capture devices available as a WebRTC local video feed source. Declaration public static Task<List<VideoCaptureDevice>> GetVideoCaptureDevicesAsync() Returns Type Description Task < List < VideoCaptureDevice >> The list of local video capture devices available to WebRTC. InitializeAsync(CancellationToken) Initialize the underlying WebRTC libraries Declaration public Task InitializeAsync(CancellationToken token = default(CancellationToken)) Parameters Type Name Description CancellationToken token Returns Type Description Task Remarks This function is asynchronous, to monitor it's status bind a handler to OnInitialized and OnError Uninitialize() Uninitialize the underlying WebRTC library, effectively cleaning up the allocated peer connection. Declaration public void Uninitialize() Remarks Peer will be null afterward."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.LocalAudioSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.LocalAudioSource.html",
    "title": "Class LocalAudioSource | MixedReality-WebRTC Documentation",
    "keywords": "Class LocalAudioSource This component represents a local audio source added as an audio track to an existing WebRTC peer connection and sent to the remote peer. The audio track can optionally be rendered locally with a MediaPlayer . Inheritance Object AudioSource LocalAudioSource Inherited Members AudioSource.AudioStreamStarted AudioSource.AudioStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class LocalAudioSource : AudioSource Fields AutoAddTrack Automatically register as an audio track when the peer connection is ready. Declaration public bool AutoAddTrack Field Value Type Description Boolean AutoStartCapture Automatically start local audio capture when this component is enabled. Declaration public bool AutoStartCapture Field Value Type Description Boolean PeerConnection Peer connection this local audio source will add an audio track to. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection PreferredAudioCodec Name of the preferred audio codec, or empty to let WebRTC decide. See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names. Declaration public string PreferredAudioCodec Field Value Type Description String Methods Awake() Declaration protected void Awake() OnDestroy() Declaration protected void OnDestroy() OnDisable() Declaration protected void OnDisable() OnEnable() Declaration protected void OnEnable()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.IceType.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.IceType.html",
    "title": "Enum IceType | MixedReality-WebRTC Documentation",
    "keywords": "Enum IceType Enumeration of the different types of ICE servers. Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public enum IceType Fields Name Description None Indicates there is no ICE information Stun Indicates ICE information is of type STUN Turn Indicates ICE information is of type TURN"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.html",
    "title": "Namespace Microsoft.MixedReality.WebRTC.Unity.Editor | MixedReality-WebRTC Documentation",
    "keywords": "Namespace Microsoft.MixedReality.WebRTC.Unity.Editor Classes CaptureCameraDrawer Property drawer for CaptureCameraAttribute , to report an error to the user if the associated property instance cannot be used for framebuffer capture by SceneVideoSource . LocalVideoSourceEditor Inspector editor for LocalVideoSource . Allows displaying some error message when Mixed Reality Capture is enabled but XR is not, the later corresponding to a non-exclusive app (2D slate) where MRC is not available. SdpTokenDrawer Property drawer for SdpTokenAttribute , to validate the associated string property content and display an error message box if invalid characters are found."
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Editor.SdpTokenDrawer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Editor.SdpTokenDrawer.html",
    "title": "Class SdpTokenDrawer | MixedReality-WebRTC Documentation",
    "keywords": "Class SdpTokenDrawer Property drawer for SdpTokenAttribute , to validate the associated string property content and display an error message box if invalid characters are found. Inheritance Object SdpTokenDrawer Namespace : Microsoft.MixedReality.WebRTC.Unity.Editor Assembly : cs.temp.dll.dll Syntax public class SdpTokenDrawer : PropertyDrawer Methods GetPropertyHeight(SerializedProperty, GUIContent) Declaration public override float GetPropertyHeight(SerializedProperty property, GUIContent label) Parameters Type Name Description SerializedProperty property GUIContent label Returns Type Description Single OnGUI(Rect, SerializedProperty, GUIContent) Declaration public override void OnGUI(Rect position, SerializedProperty property, GUIContent label) Parameters Type Name Description Rect position SerializedProperty property GUIContent label"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.CustomVideoSource-1.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.CustomVideoSource-1.html",
    "title": "Class CustomVideoSource<T> | MixedReality-WebRTC Documentation",
    "keywords": "Class CustomVideoSource<T> Abstract base component for a custom video source delivering raw video frames directly to the WebRTC implementation. Inheritance Object VideoSource CustomVideoSource<T> SceneVideoSource Inherited Members VideoSource.FrameQueue VideoSource.VideoStreamStarted VideoSource.VideoStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class CustomVideoSource<T> : VideoSource where T : class, IVideoFrameStorage, new() Type Parameters Name Description T Fields _frameQueue Frame queue holding the pending frames enqueued by the video source itself, which a video renderer needs to read and display. Declaration protected VideoFrameQueue<T> _frameQueue Field Value Type Description VideoFrameQueue <T> AutoAddTrackOnStart Automatically add the video track to the peer connection when the Unity component starts. Declaration public bool AutoAddTrackOnStart Field Value Type Description Boolean PeerConnection Peer connection this local video source will add a video track to. Declaration public PeerConnection PeerConnection Field Value Type Description PeerConnection TrackName Name of the track. Declaration public string TrackName Field Value Type Description String Remarks This must comply with the 'msid' attribute rules as defined in https://tools.ietf.org/html/draft-ietf-mmusic-msid-05#section-2 , which in particular constraints the set of allows characters to those allowed for a 'token' element as specified in https://tools.ietf.org/html/rfc4566#page-43 : Symbols [!#$%'*+-.^_`{|}~] and ampersand & Alphanumerical [A-Za-z0-9] Properties Source Video track source providing video frames to the local video track. Declaration public ExternalVideoTrackSource Source { get; } Property Value Type Description ExternalVideoTrackSource Track Video track encapsulated by this component. Declaration public LocalVideoTrack Track { get; } Property Value Type Description LocalVideoTrack Methods Awake() Declaration protected void Awake() OnDestroy() Declaration protected void OnDestroy() OnDisable() Declaration protected void OnDisable() OnEnable() Declaration protected void OnEnable() OnFrameRequested(FrameRequest) Declaration protected abstract void OnFrameRequested(in FrameRequest request) Parameters Type Name Description FrameRequest request StartTrack() Add a new track to the peer connection and start the video track playback. Declaration public void StartTrack() StopTrack() Stop the video track playback and remove the track from the peer connection. Declaration public void StopTrack()"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.ConfigurableIceServer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.ConfigurableIceServer.html",
    "title": "Struct ConfigurableIceServer | MixedReality-WebRTC Documentation",
    "keywords": "Struct ConfigurableIceServer Represents an Ice server in a simple way that allows configuration from the unity inspector Inherited Members ValueType.Equals(Object) ValueType.GetHashCode() Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetType() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public struct ConfigurableIceServer Fields Type The type of the server Declaration public IceType Type Field Value Type Description IceType Uri The unqualified uri of the server Declaration public string Uri Field Value Type Description String Remarks You should not prefix this with \"stun:\" or \"turn:\" Methods ToString() Convert the server to the representation the underlying libraries use Declaration public override string ToString() Returns Type Description String stringified server information Overrides ValueType.ToString()"
  },
  "api/Microsoft.MixedReality.WebRTC.IceTransportType.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IceTransportType.html",
    "title": "Enum IceTransportType | MixedReality-WebRTC Documentation",
    "keywords": "Enum IceTransportType Type of ICE candidates offered to the remote peer. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public enum IceTransportType : int Fields Name Description All Offer all types of ICE candidates. NoHost None No ICE candidate offered. Relay Only advertize relay-type candidates, like TURN servers, to avoid leaking the IP address of the client."
  },
  "api/Microsoft.MixedReality.WebRTC.IceServer.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.IceServer.html",
    "title": "Class IceServer | MixedReality-WebRTC Documentation",
    "keywords": "Class IceServer ICE server configuration (STUN and/or TURN). Inheritance Object IceServer Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class IceServer : object Fields | Improve this Doc View Source TurnPassword Optional TURN server credentials. Declaration public string TurnPassword Field Value Type Description String | Improve this Doc View Source TurnUserName Optional TURN server username. Declaration public string TurnUserName Field Value Type Description String | Improve this Doc View Source Urls List of TURN and/or STUN server URLs to use for NAT bypass, in order of preference. The scheme is defined in the core WebRTC implementation, and is in short: stunURI = stunScheme \":\" stun-host [ \":\" stun-port ] stunScheme = \"stun\" / \"stuns\" turnURI = turnScheme \":\" turn-host [ \":\" turn-port ] [ \"?transport=\" transport ] turnScheme = \"turn\" / \"turns\" Declaration public List<string> Urls Field Value Type Description List < String > Methods | Improve this Doc View Source ToString() Format the ICE server data according to the encoded marshalling of the C++ API. Declaration public override string ToString() Returns Type Description String The encoded string of ICE servers."
  },
  "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.html",
    "title": "Class I420AVideoFrameStorage | MixedReality-WebRTC Documentation",
    "keywords": "Class I420AVideoFrameStorage Storage for a video frame encoded in I420+Alpha format. Inheritance Object I420AVideoFrameStorage Implements IVideoFrameStorage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class I420AVideoFrameStorage : object, IVideoFrameStorage Properties | Improve this Doc View Source Buffer Raw byte buffer containing the frame data. Declaration public byte[] Buffer { get; } Property Value Type Description Byte [] | Improve this Doc View Source Capacity Total capacity of the storage, in bytes. This can be assigned to resize the storage. Declaration public ulong Capacity { get; set; } Property Value Type Description UInt64 Remarks Reading this property is equivalent to reading the property of Buffer . | Improve this Doc View Source Height Frame height, in pixels. Declaration public uint Height { get; set; } Property Value Type Description UInt32 | Improve this Doc View Source Width Frame width, in pixels. Declaration public uint Width { get; set; } Property Value Type Description UInt32 Implements IVideoFrameStorage"
  },
  "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.I420AVideoFrameDelegate.html",
    "title": "Delegate I420AVideoFrameDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate I420AVideoFrameDelegate Delegate used for events when an I420-encoded video frame has been produced and is ready for consumption. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void I420AVideoFrameDelegate(I420AVideoFrame frame); Parameters Type Name Description I420AVideoFrame frame The newly available I420-encoded video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.FrameRequest.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.FrameRequest.html",
    "title": "Struct FrameRequest | MixedReality-WebRTC Documentation",
    "keywords": "Struct FrameRequest Request sent to an external video source via its registered callback to generate a new video frame for the track(s) connected to it. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct FrameRequest Fields | Improve this Doc View Source RequestId Unique request identifier, for error checking. Declaration public uint RequestId Field Value Type Description UInt32 | Improve this Doc View Source Source Video track source this request is associated with. Declaration public ExternalVideoTrackSource Source Field Value Type Description ExternalVideoTrackSource | Improve this Doc View Source TimestampMs Frame timestamp, in milliseconds. This corresponds to the time when the request was made to the native video track source. Declaration public long TimestampMs Field Value Type Description Int64 Methods | Improve this Doc View Source CompleteRequest(Argb32VideoFrame) Complete the current request by providing a video frame for it. This must be used if the video track source was created with CreateFromArgb32Callback(Argb32VideoFrameRequestDelegate) . Declaration public void CompleteRequest(in Argb32VideoFrame frame) Parameters Type Name Description Argb32VideoFrame frame The video frame used to complete the request. | Improve this Doc View Source CompleteRequest(I420AVideoFrame) Complete the current request by providing a video frame for it. This must be used if the video track source was created with CreateFromI420ACallback(I420AVideoFrameRequestDelegate) . Declaration public void CompleteRequest(in I420AVideoFrame frame) Parameters Type Name Description I420AVideoFrame frame The video frame used to complete the request."
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannel.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannel.html",
    "title": "Class DataChannel | MixedReality-WebRTC Documentation",
    "keywords": "Class DataChannel Encapsulates a data channel of a peer connection. A data channel is a pipe allowing to send and receive arbitrary data to the remote peer. Data channels are based on DTLS-SRTP, and are therefore secure (encrypted). Exact security guarantees are provided by the underlying WebRTC core implementation and the WebRTC standard itself. https://tools.ietf.org/wg/rtcweb/ An instance of DataChannel is created by calling AddDataChannelAsync(String, Boolean, Boolean) or one of its variants. DataChannel cannot be instantiated directly. Inheritance Object DataChannel Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class DataChannel : IDisposable Properties | Improve this Doc View Source ID Declaration public int ID { get; } Property Value Type Description Int32 The unique identifier of the data channel in the current connection. | Improve this Doc View Source Label Declaration public string Label { get; } Property Value Type Description String The data channel name in the current connection. | Improve this Doc View Source Ordered Indicates whether the data channel messages are ordered or not. Ordered messages are delivered in the order they are sent, at the cost of delaying later messages delivery to the application (via MessageReceived ) when internally arriving out of order. Declaration public bool Ordered { get; } Property Value Type Description Boolean true if messages are ordered. See Also Reliable | Improve this Doc View Source PeerConnection Declaration public PeerConnection PeerConnection { get; } Property Value Type Description PeerConnection The PeerConnection object this data channel was created from. | Improve this Doc View Source Reliable Indicates whether the data channel messages are reliably delivered. Reliable messages are guaranteed to be delivered as long as the connection is not dropped. Unreliable messages may be silently dropped for whatever reason, and the implementation will not try to detect this nor resend them. Declaration public bool Reliable { get; } Property Value Type Description Boolean true if messages are reliable. See Also Ordered | Improve this Doc View Source State The channel connection state represents the connection status when creating or closing the data channel. Changes to this state are notified via the StateChanged event. Declaration public DataChannel.ChannelState State { get; } Property Value Type Description DataChannel.ChannelState The channel connection state. See Also StateChanged Methods | Improve this Doc View Source Dispose() Remove the data track from the peer connection and destroy it. Declaration public void Dispose() | Improve this Doc View Source Finalize() Finalizer to ensure the data track is removed from the peer connection and the managed resources are cleaned-up. Declaration protected void Finalize() | Improve this Doc View Source SendMessage(Byte[]) Send a message through the data channel. If the message cannot be sent, for example because of congestion control, it is buffered internally. If this buffer gets full, an exception is thrown and this call is aborted. The internal buffering is monitored via the BufferingChanged event. Declaration public void SendMessage(byte[] message) Parameters Type Name Description Byte [] message The message to send to the remote peer. See Also InitializeAsync(PeerConnectionConfiguration, CancellationToken) Initialized BufferingChanged Events | Improve this Doc View Source BufferingChanged Event fired when the data channel buffering changes. Monitor this to ensure calls to SendMessage(Byte[]) do not fail. Internally the data channel contains a buffer of messages to send that could not be sent immediately, for example due to congestion control. Once this buffer is full, any further call to SendMessage(Byte[]) will fail until some mesages are processed and removed to make space. Declaration public event DataChannel.BufferingChangedDelegate BufferingChanged Event Type Type Description DataChannel.BufferingChangedDelegate See Also SendMessage(Byte[]) | Improve this Doc View Source MessageReceived Event fires when a message is received through the data channel. Declaration public event Action<byte[]> MessageReceived Event Type Type Description Action < Byte []> See Also SendMessage(Byte[]) | Improve this Doc View Source StateChanged Event fired when the data channel state changes, as reported by State . Declaration public event Action StateChanged Event Type Type Description Action See Also State"
  },
  "api/Microsoft.MixedReality.WebRTC.DataChannel.BufferingChangedDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.DataChannel.BufferingChangedDelegate.html",
    "title": "Delegate DataChannel.BufferingChangedDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate DataChannel.BufferingChangedDelegate Delegate for the BufferingChanged event. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void BufferingChangedDelegate(ulong previous, ulong current, ulong limit); Parameters Type Name Description UInt64 previous Previous buffering size, in bytes. UInt64 current New buffering size, in bytes. UInt64 limit Maximum buffering size, in bytes."
  },
  "api/Microsoft.MixedReality.WebRTC.AudioFrame.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.AudioFrame.html",
    "title": "Struct AudioFrame | MixedReality-WebRTC Documentation",
    "keywords": "Struct AudioFrame Single raw uncompressed audio frame. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct AudioFrame Remarks The use of ref struct is an optimization to avoid heap allocation on each frame while having a nicer-to-use container to pass a frame accross methods. Fields | Improve this Doc View Source audioData Buffer of audio samples for all channels. Declaration public IntPtr audioData Field Value Type Description IntPtr | Improve this Doc View Source bitsPerSample Number of bits per sample, generally 8 or 16. Declaration public uint bitsPerSample Field Value Type Description UInt32 | Improve this Doc View Source channelCount Number of audio channels. Declaration public uint channelCount Field Value Type Description UInt32 | Improve this Doc View Source sampleCount Number of consecutive samples in the audio data buffer. WebRTC generally delivers frames in 10ms chunks, so for e.g. a 16 kHz sample rate the sample count would be 1000. Declaration public uint sampleCount Field Value Type Description UInt32 | Improve this Doc View Source sampleRate Sample rate, in Hz. Generally in the range 8-48 kHz. Declaration public uint sampleRate Field Value Type Description UInt32"
  },
  "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.html",
    "title": "Class Argb32VideoFrameStorage | MixedReality-WebRTC Documentation",
    "keywords": "Class Argb32VideoFrameStorage Storage for a video frame encoded in ARGB format. Inheritance Object Argb32VideoFrameStorage Implements IVideoFrameStorage Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class Argb32VideoFrameStorage : object, IVideoFrameStorage Properties | Improve this Doc View Source Buffer Raw byte buffer containing the frame data. Declaration public byte[] Buffer { get; } Property Value Type Description Byte [] | Improve this Doc View Source Capacity Total capacity of the storage, in bytes. This can be assigned to resize the storage. Declaration public ulong Capacity { get; set; } Property Value Type Description UInt64 Remarks Reading this property is equivalent to reading the property of Buffer . | Improve this Doc View Source Height Frame height, in pixels. Declaration public uint Height { get; set; } Property Value Type Description UInt32 | Improve this Doc View Source Width Frame width, in pixels. Declaration public uint Width { get; set; } Property Value Type Description UInt32 Implements IVideoFrameStorage"
  },
  "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameDelegate.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Argb32VideoFrameDelegate.html",
    "title": "Delegate Argb32VideoFrameDelegate | MixedReality-WebRTC Documentation",
    "keywords": "Delegate Argb32VideoFrameDelegate Delegate used for events when an ARGB-encoded video frame has been produced and is ready for consumption. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public delegate void Argb32VideoFrameDelegate(Argb32VideoFrame frame); Parameters Type Name Description Argb32VideoFrame frame The newly available ARGB-encoded video frame."
  },
  "api/Microsoft.MixedReality.WebRTC.VideoCaptureFormat.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoCaptureFormat.html",
    "title": "Struct VideoCaptureFormat | MixedReality-WebRTC Documentation",
    "keywords": "Struct VideoCaptureFormat Capture format for a video track. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct VideoCaptureFormat Fields | Improve this Doc View Source fourcc FOURCC identifier of the video encoding. Declaration public uint fourcc Field Value Type Description UInt32 | Improve this Doc View Source framerate Capture framerate, in frames per second. Declaration public double framerate Field Value Type Description Double | Improve this Doc View Source height Frame height, in pixels. Declaration public uint height Field Value Type Description UInt32 | Improve this Doc View Source width Frame width, in pixels. Declaration public uint width Field Value Type Description UInt32"
  },
  "api/Microsoft.MixedReality.WebRTC.VideoCaptureDevice.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoCaptureDevice.html",
    "title": "Struct VideoCaptureDevice | MixedReality-WebRTC Documentation",
    "keywords": "Struct VideoCaptureDevice Identifier for a video capture device. Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public struct VideoCaptureDevice Fields | Improve this Doc View Source id Unique device identifier. Declaration public string id Field Value Type Description String | Improve this Doc View Source name Friendly device name. Declaration public string name Field Value Type Description String"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.Message.WireMessageType.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.Signaler.Message.WireMessageType.html",
    "title": "Enum Signaler.Message.WireMessageType | MixedReality-WebRTC Documentation",
    "keywords": "Enum Signaler.Message.WireMessageType Possible message types as-serialized on the wire Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public enum WireMessageType Fields Name Description Answer A SDP answer message Ice A trickle-ice or ice message Offer A SDP offer message Unknown An unrecognized message"
  },
  "api/Microsoft.MixedReality.WebRTC.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.html",
    "title": "Namespace Microsoft.MixedReality.WebRTC | MixedReality-WebRTC Documentation",
    "keywords": "Namespace Microsoft.MixedReality.WebRTC Classes Argb32VideoFrameStorage Storage for a video frame encoded in ARGB format. DataChannel Encapsulates a data channel of a peer connection. A data channel is a pipe allowing to send and receive arbitrary data to the remote peer. Data channels are based on DTLS-SRTP, and are therefore secure (encrypted). Exact security guarantees are provided by the underlying WebRTC core implementation and the WebRTC standard itself. https://tools.ietf.org/wg/rtcweb/ An instance of DataChannel is created by calling AddDataChannelAsync(String, Boolean, Boolean) or one of its variants. DataChannel cannot be instantiated directly. ExternalVideoTrackSource Video source for WebRTC video tracks based on a custom source of video frames managed by the user and external to the WebRTC implementation. This class is used to inject into the WebRTC engine a video track whose frames are produced by a user-managed source the WebRTC engine knows nothing about, like programmatically generated frames, including frames not strictly of video origin like a 3D rendered scene, or frames coming from a specific capture device not supported natively by WebRTC. This class serves as an adapter for such video frame sources. I420AVideoFrameStorage Storage for a video frame encoded in I420+Alpha format. IceServer ICE server configuration (STUN and/or TURN). InvalidInteropNativeHandleException Exception thrown when an API function expects an interop handle to a valid native object, but receives an invalid handle instead. LocalVideoTrack Video track sending to the remote peer video frames originating from a local track source. MovingAverage Utility to manage a moving average of a time series. PeerConnection The WebRTC peer connection object is the entry point to using WebRTC. PeerConnection.LocalVideoTrackSettings Settings for adding a local video track. PeerConnectionConfiguration Configuration to initialize a PeerConnection . SctpNotNegotiatedException Exception thrown when trying to add a data channel to a peer connection after a connection to a remote peer was established without an SCTP handshake. When using data channels, at least one data channel must be added to the peer connection before calling CreateOffer() to signal to the implementation the intent to use data channels and the need to perform a SCTP handshake during the connection. TaskExtensions Collection of extension methods for Task . VideoFrameQueue<T> Small queue of video frames received from a source and pending delivery to a sink. Used as temporary buffer between the WebRTC callback (push model) and the video player rendering (pull model). This also handles dropping frames when the source is faster than the sink, by limiting the maximum queue length. Structs Argb32VideoFrame Single video frame encoded in ARGB interleaved format (32 bits per pixel). The ARGB components are in the order of a little endian 32-bit integer, so 0xAARRGGBB, or (B, G, R, A) as a sequence of bytes in memory with B first and A last. AudioFrame Single raw uncompressed audio frame. FrameRequest Request sent to an external video source via its registered callback to generate a new video frame for the track(s) connected to it. I420AVideoFrame Single video frame encoded in I420A format (triplanar YUV with optional alpha plane). See e.g. https://wiki.videolan.org/YUV/#I420 for details. The I420 format uses chroma downsampling in both directions, resulting in 12 bits per pixel. With the optional alpha plane, the size increases to 20 bits per pixel. VideoCaptureDevice Identifier for a video capture device. VideoCaptureFormat Capture format for a video track. Interfaces IVideoFrameQueue Interface for a queue of video frames. IVideoFrameStorage Interface for a storage of a single video frame. Enums BundlePolicy Bundle policy. See https://www.w3.org/TR/webrtc/#rtcbundlepolicy-enum . DataChannel.ChannelState Connecting state of a data channel, when adding it to a peer connection or removing it from a peer connection. IceConnectionState State of an ICE connection. IceGatheringState State of an ICE gathering process. IceTransportType Type of ICE candidates offered to the remote peer. PeerConnection.FrameHeightRoundMode Frame height round mode. PeerConnection.TrackKind Kind of WebRTC track. PeerConnection.VideoProfileKind Kind of video profile. This corresponds to the enum of the API. SdpSemantic SDP semantic used for (re)negotiating a peer connection. Delegates Argb32VideoFrameDelegate Delegate used for events when an ARGB-encoded video frame has been produced and is ready for consumption. Argb32VideoFrameRequestDelegate Callback invoked when the WebRTC pipeline needs an external video source to generate a new video frame for the track(s) it is connected to. AudioFrameDelegate Delegate used for events when an audio frame has been produced and is ready for consumption. DataChannel.BufferingChangedDelegate Delegate for the BufferingChanged event. I420AVideoFrameDelegate Delegate used for events when an I420-encoded video frame has been produced and is ready for consumption. I420AVideoFrameRequestDelegate Callback invoked when the WebRTC pipeline needs an external video source to generate a new video frame for the track(s) it is connected to. PeerConnection.DataChannelAddedDelegate Delegate for DataChannelAdded event. PeerConnection.DataChannelRemovedDelegate Delegate for DataChannelRemoved event. PeerConnection.IceCandidateReadytoSendDelegate Delegate for the IceCandidateReadytoSend event. PeerConnection.IceGatheringStateChangedDelegate Delegate for the IceGatheringStateChanged event. PeerConnection.IceStateChangedDelegate Delegate for the IceStateChanged event. PeerConnection.LocalSdpReadyToSendDelegate Delegate for LocalSdpReadytoSend event."
  },
  "api/Microsoft.MixedReality.WebRTC.VideoFrameQueue-1.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.VideoFrameQueue-1.html",
    "title": "Class VideoFrameQueue<T> | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoFrameQueue<T> Small queue of video frames received from a source and pending delivery to a sink. Used as temporary buffer between the WebRTC callback (push model) and the video player rendering (pull model). This also handles dropping frames when the source is faster than the sink, by limiting the maximum queue length. Inheritance Object VideoFrameQueue<T> Implements IVideoFrameQueue Namespace : Microsoft.MixedReality.WebRTC Assembly : Microsoft.MixedReality.WebRTC.dll Syntax public class VideoFrameQueue<T> : object, IVideoFrameQueue where T : class, IVideoFrameStorage, new() Type Parameters Name Description T The type of video frame storage Constructors | Improve this Doc View Source VideoFrameQueue(Int32) Create a new queue with a maximum frame length. Declaration public VideoFrameQueue(int maxQueueLength) Parameters Type Name Description Int32 maxQueueLength Maxmimum number of frames to enqueue before starting to drop incoming frames Properties | Improve this Doc View Source DequeuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video sink consumes some video frames, typically to render them. Declaration public float DequeuedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source DroppedFramesPerSecond Get the number of frames dropped per seconds. This is generally an average statistics representing how many frames were enqueued by a video source but not dequeued fast enough by a video sink, meaning the video sink renders at a slower framerate than the source can produce. Declaration public float DroppedFramesPerSecond { get; } Property Value Type Description Single | Improve this Doc View Source QueuedFramesPerSecond Get the number of frames enqueued per seconds. This is generally an average statistics representing how fast a video source produces some video frames. Declaration public float QueuedFramesPerSecond { get; } Property Value Type Description Single Methods | Improve this Doc View Source Clear() Clear the queue and drop all frames currently pending. Declaration public void Clear() | Improve this Doc View Source Enqueue(Argb32VideoFrame) Try to enqueue a new video frame encoded in raw ARGB format. If the internal queue reached its maximum capacity, do nothing and drop the frame. Declaration public bool Enqueue(Argb32VideoFrame frame) Parameters Type Name Description Argb32VideoFrame frame The video frame to enqueue Returns Type Description Boolean Return true if the frame was enqueued successfully, or false if it was dropped Remarks This should only be used if the queue has storage for a compatible video frame encoding. | Improve this Doc View Source Enqueue(I420AVideoFrame) Enqueue a new video frame encoded in I420+Alpha format. If the internal queue reached its maximum capacity, do nothing and drop the frame. Declaration public bool Enqueue(I420AVideoFrame frame) Parameters Type Name Description I420AVideoFrame frame The video frame to enqueue Returns Type Description Boolean Return true if the frame was enqueued successfully, or false if it was dropped Remarks This should only be used if the queue has storage for a compatible video frame encoding. | Improve this Doc View Source RecycleStorage(T) Recycle a frame storage, putting it back into the internal pool for later reuse. This prevents deallocation and reallocation of a frame, and decreases pressure on the garbage collector. Declaration public void RecycleStorage(T frame) Parameters Type Name Description T frame The unused frame storage to recycle for a later new frame | Improve this Doc View Source TrackLateFrame() Track statistics for a late frame, which short-circuits the queue and is delivered as soon as it is received. Declaration public void TrackLateFrame() | Improve this Doc View Source TryDequeue(out T) Try to dequeue a video frame, usually to be consumed by a video sink (video player). Declaration public bool TryDequeue(out T frame) Parameters Type Name Description T frame On success, returns the dequeued frame. Returns Type Description Boolean Return true on success or false if the queue is empty. Implements IVideoFrameQueue"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStoppedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStoppedEvent.html",
    "title": "Class VideoStreamStoppedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoStreamStoppedEvent Unity event corresponding to an on-going video stream being stopped. Inheritance Object VideoStreamStoppedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class VideoStreamStoppedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStartedEvent.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoStreamStartedEvent.html",
    "title": "Class VideoStreamStartedEvent | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoStreamStartedEvent Unity event corresponding to a new video stream being started. Inheritance Object VideoStreamStartedEvent Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public class VideoStreamStartedEvent : UnityEvent"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoSource.html",
    "title": "Class VideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Class VideoSource Base class for video sources plugging into the internal peer connection API to expose a single video stream to a renderer ( MediaPlayer or custom). Inheritance Object VideoSource CustomVideoSource<T> LocalVideoSource RemoteVideoSource Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public abstract class VideoSource : MonoBehaviour Fields VideoStreamStarted Event invoked from the main Unity thread when the video stream starts. This means that video frames are available and the renderer should start polling. Declaration public VideoStreamStartedEvent VideoStreamStarted Field Value Type Description VideoStreamStartedEvent VideoStreamStopped Event invoked from the main Unity thread when the video stream stops. This means that the video frame queue is not populated anymore, though some frames may still be present in it that may be rendered. Declaration public VideoStreamStoppedEvent VideoStreamStopped Field Value Type Description VideoStreamStoppedEvent Properties FrameQueue Frame queue holding the pending frames enqueued by the video source itself, which a video renderer needs to read and display. Declaration public IVideoFrameQueue FrameQueue { get; protected set; } Property Value Type Description IVideoFrameQueue"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.VideoCaptureConstraints.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.VideoCaptureConstraints.html",
    "title": "Struct VideoCaptureConstraints | MixedReality-WebRTC Documentation",
    "keywords": "Struct VideoCaptureConstraints Additional optional constraints applied to the resolution and framerate when selecting a video capture format. Inherited Members ValueType.Equals(Object) ValueType.GetHashCode() ValueType.ToString() Object.Equals(Object, Object) Object.ReferenceEquals(Object, Object) Object.GetType() Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax [Serializable] public struct VideoCaptureConstraints Fields framerate Desired framerate, in frame-per-second, or zero for unconstrained. Note: the comparison is exact, and floating point imprecision may prevent finding a matching format. Use with caution. Declaration public double framerate Field Value Type Description Double height Desired resolution height, in pixels, or zero for unconstrained. Declaration public int height Field Value Type Description Int32 width Desired resolution width, in pixels, or zero for unconstrained. Declaration public int width Field Value Type Description Int32"
  },
  "api/Microsoft.MixedReality.WebRTC.Unity.SceneVideoSource.html": {
    "href": "api/Microsoft.MixedReality.WebRTC.Unity.SceneVideoSource.html",
    "title": "Class SceneVideoSource | MixedReality-WebRTC Documentation",
    "keywords": "Class SceneVideoSource Custom video source capturing the Unity scene content as rendered by a given camera, and sending it as a video track through the selected peer connection. Inheritance Object VideoSource CustomVideoSource < Argb32VideoFrameStorage > SceneVideoSource Inherited Members CustomVideoSource<Argb32VideoFrameStorage>.PeerConnection CustomVideoSource<Argb32VideoFrameStorage>.TrackName CustomVideoSource<Argb32VideoFrameStorage>.AutoAddTrackOnStart CustomVideoSource<Argb32VideoFrameStorage>.Source CustomVideoSource<Argb32VideoFrameStorage>.Track CustomVideoSource<Argb32VideoFrameStorage>._frameQueue CustomVideoSource<Argb32VideoFrameStorage>.StartTrack() CustomVideoSource<Argb32VideoFrameStorage>.StopTrack() CustomVideoSource<Argb32VideoFrameStorage>.Awake() CustomVideoSource<Argb32VideoFrameStorage>.OnDestroy() VideoSource.FrameQueue VideoSource.VideoStreamStarted VideoSource.VideoStreamStopped Namespace : Microsoft.MixedReality.WebRTC.Unity Assembly : cs.temp.dll.dll Syntax public class SceneVideoSource : CustomVideoSource<Argb32VideoFrameStorage> Fields CameraEvent Camera event indicating the point in time during the Unity frame rendering when the camera rendering is to be captured. This defaults to , which is a reasonable default to capture the entire scene rendering, but can be customized to achieve other effects like capturing only a part of the scene. Declaration public CameraEvent CameraEvent Field Value Type Description CameraEvent SourceCamera Camera used to capture the scene content, whose rendering is used as video content for the track. Declaration public Camera SourceCamera Field Value Type Description Camera Remarks If the project uses Multi-Pass stereoscopic rendering, then this camera needs to render to a single eye to produce a single video frame. Generally this means that this needs to be a separate Unity camera from the one used for XR rendering, which is generally rendering to both eyes. If the project uses Single-Pass Instanced stereoscopic rendering, then Unity 2019.1+ is required to make this component work, due to the fact earlier versions of Unity are missing some command buffer API calls to be able to efficiently access the camera backbuffer in this mode. For Unity 2018.3 users who cannot upgrade, use Single-Pass (non-instanced) instead. Methods OnDisable() Declaration protected void OnDisable() OnEnable() Declaration protected void OnEnable() OnFrameRequested(FrameRequest) Callback invoked by the base class when the WebRTC track requires a new frame. Declaration protected override void OnFrameRequested(in FrameRequest request) Parameters Type Name Description FrameRequest request The frame request to serve. Overrides Microsoft.MixedReality.WebRTC.Unity.CustomVideoSource<Argb32VideoFrameStorage>.OnFrameRequested(FrameRequest)"
  }
}